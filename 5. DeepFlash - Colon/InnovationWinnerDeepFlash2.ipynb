{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OcsetTMwKXqC"
   },
   "source": [
    "# HuBMAP - deepflash2 submission\n",
    "\n",
    "> Submission kernel for model trained with efficient region based sampling. \n",
    "\n",
    "***\n",
    "\n",
    "\n",
    "## Highlights\n",
    "\n",
    "- *Super fast submission*: ~30 min for a 5-model ensemble on the public test set\n",
    "- Using overlapping tiles and gaussian weighting similar to [nnuet](https://www.nature.com/articles/s41592-020-01008-z), which removes almost all prediction artifacts\n",
    "\n",
    "## Related Kernels\n",
    "- Training: https://www.kaggle.com/matjes/hubmap-deepflash2-train\n",
    "- Sampling: https://www.kaggle.com/matjes/hubmap-efficient-sampling-ii-deepflash2\n",
    "\n",
    "To make this kernel fast and reliable, special thanks go to @leighplt ([kernel](https://www.kaggle.com/leighplt/pytorch-fcn-resnet50)) and @iafoss ([kernel](https://www.kaggle.com/iafoss/hubmap-pytorch-fast-ai-starter-sub))!\n",
    "\n",
    "## Overview\n",
    "\n",
    "1. Installation and package loading\n",
    "2. Functions and classes for prediction\n",
    "3. Configuration\n",
    "4. Prediction\n",
    "5. Submission\n",
    "\n",
    "### Installation and package loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".tooltip {\n",
       "  position: relative;\n",
       "  display: inline-block;\n",
       "}\n",
       "\n",
       ".tooltip .tooltiptext {\n",
       "  visibility: hidden;\n",
       "  width: max-content;\n",
       "  max-width: 280px;\n",
       "  background-color: #00bcd4;\n",
       "  text-align: left;\n",
       "  color: white;\n",
       "  border-radius: 4px;\n",
       "  padding: 4px 4px;\n",
       "  border: solid 0px black;\n",
       "  line-height: 1em;\n",
       "\n",
       "  /* Position the tooltip */\n",
       "  position: absolute;\n",
       "  z-index: 1;\n",
       "}\n",
       "\n",
       ".tooltip:hover .tooltiptext {\n",
       "  visibility: visible;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Install deepflash2 and dependencies\n",
    "import sys\n",
    "import cv2, torch, gc, rasterio\n",
    "import torch.nn.functional as F\n",
    "import deepflash2.tta as tta\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd, numpy as np\n",
    "import segmentation_models_pytorch as smp\n",
    "from pathlib import Path\n",
    "from rasterio.windows import Window\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from scipy .ndimage.filters import gaussian_filter\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions and classes for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/bguberfain/memory-aware-rle-encoding\n",
    "#with transposed mask\n",
    "def rle_encode_less_memory(img):\n",
    "    #the image should be transposed\n",
    "    pixels = img.T.flatten()\n",
    "    \n",
    "    # This simplified method requires first and last pixel to be zero\n",
    "    pixels[0] = 0\n",
    "    pixels[-1] = 0\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 2\n",
    "    runs[1::2] -= runs[::2]\n",
    "    \n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "def load_model_weights(model, file, strict=True):\n",
    "    state = torch.load(file, map_location='cpu')\n",
    "    stats = state['stats']\n",
    "    model_state = state['model']\n",
    "    model.load_state_dict(model_state, strict=strict)\n",
    "    return model, stats\n",
    "\n",
    "# from https://github.com/MIC-DKFZ/nnUNet/blob/2fade8f32607220f8598544f0d5b5e5fa73768e5/nnunet/network_architecture/neural_network.py#L250\n",
    "def _get_gaussian(patch_size, sigma_scale=1. / 8) -> np.ndarray:\n",
    "    tmp = np.zeros(patch_size)\n",
    "    center_coords = [i // 2 for i in patch_size]\n",
    "    sigmas = [i * sigma_scale for i in patch_size]\n",
    "    tmp[tuple(center_coords)] = 1\n",
    "    gaussian_importance_map = gaussian_filter(tmp, sigmas, 0, mode='constant', cval=0)\n",
    "    gaussian_importance_map = gaussian_importance_map / np.max(gaussian_importance_map) * 1\n",
    "    gaussian_importance_map = gaussian_importance_map.astype(np.float32)\n",
    "\n",
    "    # gaussian_importance_map cannot be 0, otherwise we may end up with nans!\n",
    "    gaussian_importance_map[gaussian_importance_map == 0] = np.min(\n",
    "        gaussian_importance_map[gaussian_importance_map != 0])\n",
    "\n",
    "    return gaussian_importance_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# Some code adapted from https://www.kaggle.com/iafoss/hubmap-pytorch-fast-ai-starter-sub\n",
    "class HubmapDataset(Dataset):\n",
    "    'HubmapDataset class that does not load the full tiff files.'\n",
    "    def __init__(self, file, stats, scale=3, shift=.8, output_shape=(512,512), s_th = 40):\n",
    "        \n",
    "        self.mean, self.std = stats\n",
    "        self.scale = scale\n",
    "        self.shift = shift\n",
    "        self.output_shape = output_shape\n",
    "        self.input_shape = tuple(int(t*scale) for t in self.output_shape)      \n",
    "        self.s_th = s_th #saturation blancking threshold\n",
    "        self.p_th = 1000*(self.output_shape[0]//256)**2 #threshold for the minimum number of pixels\n",
    "\n",
    "        identity = rasterio.Affine(1, 0, 0, 0, 1, 0)\n",
    "        self.data = rasterio.open(file, transform = identity, num_threads='all_cpus')\n",
    "        if self.data.count != 3:\n",
    "            subdatasets = self.data.subdatasets\n",
    "            self.layers = []\n",
    "            if len(subdatasets) > 0:\n",
    "                for i, subdataset in enumerate(subdatasets, 0):\n",
    "                    self.layers.append(rasterio.open(subdataset))\n",
    "            \n",
    "        # Tiling\n",
    "        self.slices = []\n",
    "        self.out_slices = []\n",
    "        self.out_data_shape = tuple(int(x//self.scale) for x in self.data.shape)\n",
    "        start_points = [o//2 for o in self.output_shape]\n",
    "        end_points = [(s - st) for s, st in zip(self.out_data_shape, start_points)]\n",
    "        n_points = [int(s//(o*self.shift))+1 for s, o in zip(self.out_data_shape, self.output_shape)]\n",
    "        center_points = [np.linspace(st, e, num=n, endpoint=True, dtype=np.int64) for st, e, n in zip(start_points, end_points, n_points)]\n",
    "        for cx in center_points[1]:\n",
    "            for cy in center_points[0]:\n",
    "                # Calculate output slices for whole image\n",
    "                slices = tuple(slice(int((c*self.scale - o/2).clip(0, s)), int((c*self.scale + o/2).clip(max=s)))\n",
    "                                 for (c, o, s) in zip((cy, cx), self.input_shape, self.data.shape))\n",
    "                self.slices.append(slices)\n",
    "                \n",
    "                out_slices = tuple(slice(int((c - o/2).clip(0, s)), int((c + o/2).clip(max=s)))\n",
    "                                 for (c, o, s) in zip((cy, cx), self.output_shape, self.out_data_shape))\n",
    "                self.out_slices.append(out_slices)\n",
    "                \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.slices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        slices = self.slices[idx]\n",
    "        if self.data.count == 3: # normal\n",
    "            img = self.data.read([1, 2, 3], \n",
    "                window=Window.from_slices(*slices)\n",
    "            )\n",
    "            img = np.moveaxis(img, 0, -1)\n",
    "        else: # with subdatasets/layers\n",
    "            img = np.zeros((*self.input_shape, 3), dtype=np.uint8)\n",
    "            for fl in range(3):\n",
    "                img[:, :, fl] = self.layers[fl].read(\n",
    "                    window=Window.from_slices(*slices)\n",
    "                )\n",
    "        \n",
    "        if self.scale!=1:\n",
    "            img = cv2.resize(img, self.output_shape, interpolation = cv2.INTER_AREA)\n",
    "        \n",
    "        #check for empty imges\n",
    "        hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "        h,s,v = cv2.split(hsv)\n",
    "        if (s>self.s_th).sum() <= self.p_th or img.sum() <= self.p_th:\n",
    "            # Remove if idx=-1\n",
    "            idx = -1\n",
    "        \n",
    "        img = (img/255.0 - self.mean)/self.std\n",
    "        img = img.transpose(2, 0, 1).astype('float32')\n",
    "        \n",
    "        return torch.from_numpy(img), idx\n",
    "    \n",
    "class Model_pred:\n",
    "    'Class for prediction with multiple models'\n",
    "    def __init__(self, models, use_tta=True, batch_size=32):\n",
    "        self.models = models\n",
    "        self.bs = batch_size\n",
    "        self.tfms = [tta.HorizontalFlip(), tta.VerticalFlip()] if use_tta else []\n",
    "        \n",
    "    def predict(self, ds):\n",
    "        #rasterio cannot be used with multiple workers\n",
    "        dl = DataLoader(ds, self.bs, num_workers=0, shuffle=False, pin_memory=True)\n",
    "        \n",
    "        # Create zero arrays\n",
    "        pred = np.zeros(ds.out_data_shape, dtype='float32')\n",
    "        merge_map = np.zeros(ds.out_data_shape, dtype='float32')\n",
    "        \n",
    "        # Gaussian weights\n",
    "        gw_numpy = _get_gaussian(ds.output_shape)\n",
    "        gw = torch.from_numpy(gw_numpy).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, idxs in tqdm(iter(dl), total=len(dl)):\n",
    "                if ((idxs>=0).sum() > 0): #exclude empty images\n",
    "                    images = images[idxs>=0].to(device)\n",
    "                    idxs = idxs[idxs>=0]\n",
    "                    merger = tta.Merger()\n",
    "                    for t in tta.Compose(self.tfms):\n",
    "                        aug_images = t.augment_image(images)\n",
    "                        model_merger = tta.Merger()\n",
    "                        for model in self.models:\n",
    "                            out = model(aug_images)\n",
    "                            out = F.softmax(out, dim=1)\n",
    "                            model_merger.append(out)\n",
    "                        out = t.deaugment_mask(model_merger.result())\n",
    "                        merger.append(out)\n",
    "            \n",
    "                    # Apply gaussian weigthing\n",
    "                    batch_smx = merger.result()*gw.view(1,1,*gw.shape)\n",
    "                    batch_smx = [x for x in batch_smx.permute(0,2,3,1).cpu().numpy()]\n",
    "\n",
    "                    for smx, idx in zip(batch_smx, idxs):\n",
    "                        slcs = ds.out_slices[idx]\n",
    "                        # Only using positive class here\n",
    "                        pred[slcs] += smx[...,1]\n",
    "                        merge_map[slcs] += gw_numpy\n",
    "\n",
    "        pred /= merge_map\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CONFIG():\n",
    "    path = r\"C:\\\\Users\\\\soodn\\\\Downloads\\\\Naveksha\\\\Kaggle HuBMAP\\\\Scripts\\\\5. DeepFlash - Colon\\\\\"\n",
    "    # data paths\n",
    "    data_path = r'C:\\Users\\soodn\\Downloads\\Naveksha\\Kaggle HuBMAP\\Data\\hubmap_colon_data\\Colon_data_reprocessed\\Colon_data_reprocessed'\n",
    "    # Local train on all data\n",
    "    model_path = path+'models'\n",
    "    #model_path = Path('../input/hubmap-deepflash2-train')\n",
    "    \n",
    "    # zoom factor (e.g., 3 means downscaling from 1536 to 512)\n",
    "    scale = 3 \n",
    "    # tile shift for prediction\n",
    "    shift = 0.8 \n",
    "    tile_shape = (512, 512)\n",
    "\n",
    "    # pytorch model (https://github.com/qubvel/segmentation_models.pytorch)\n",
    "    encoder_name = \"efficientnet-b2\"\n",
    "    encoder_weights = None\n",
    "    in_channels = 3\n",
    "    classes = 2\n",
    "    \n",
    "    # dataloader \n",
    "    batch_size = 32\n",
    "    \n",
    "    # test time augmentation\n",
    "    tta = True\n",
    "    # prediction threshold\n",
    "    threshold = 0.5\n",
    "    \n",
    "cfg = CONFIG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".tooltip {\n",
       "  position: relative;\n",
       "  display: inline-block;\n",
       "}\n",
       "\n",
       ".tooltip .tooltiptext {\n",
       "  visibility: hidden;\n",
       "  width: max-content;\n",
       "  max-width: 280px;\n",
       "  background-color: #00bcd4;\n",
       "  text-align: left;\n",
       "  color: white;\n",
       "  border-radius: 4px;\n",
       "  padding: 4px 4px;\n",
       "  border: solid 0px black;\n",
       "  line-height: 1em;\n",
       "\n",
       "  /* Position the tooltip */\n",
       "  position: absolute;\n",
       "  z-index: 1;\n",
       "}\n",
       "\n",
       ".tooltip:hover .tooltiptext {\n",
       "  visibility: visible;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 models C:\\\\Users\\\\soodn\\\\Downloads\\\\Naveksha\\\\Kaggle HuBMAP\\\\Scripts\\\\5. DeepFlash - Colon\\\\models\\unet_efficientnet-b2_0.pth C:\\\\Users\\\\soodn\\\\Downloads\\\\Naveksha\\\\Kaggle HuBMAP\\\\Scripts\\\\5. DeepFlash - Colon\\\\models\\unet_efficientnet-b2_1.pth C:\\\\Users\\\\soodn\\\\Downloads\\\\Naveksha\\\\Kaggle HuBMAP\\\\Scripts\\\\5. DeepFlash - Colon\\\\models\\unet_efficientnet-b2_2.pth C:\\\\Users\\\\soodn\\\\Downloads\\\\Naveksha\\\\Kaggle HuBMAP\\\\Scripts\\\\5. DeepFlash - Colon\\\\models\\unet_efficientnet-b2_3.pth C:\\\\Users\\\\soodn\\\\Downloads\\\\Naveksha\\\\Kaggle HuBMAP\\\\Scripts\\\\5. DeepFlash - Colon\\\\models\\unet_efficientnet-b2_4.pth\n"
     ]
    }
   ],
   "source": [
    "# Sample submissions for ids\n",
    "df_sample = pd.read_csv(cfg.data_path+'/sample_submission.csv', index_col = \"id\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Models (see https://github.com/qubvel/segmentation_models.pytorch)\n",
    "MODELS = [name for name in glob.glob(cfg.model_path+'/*.pth')]\n",
    "print(f'Found {len(MODELS)} models', *MODELS)\n",
    "\n",
    "models = []\n",
    "for i, m_path in enumerate(MODELS):\n",
    "    #state_dict = torch.load(path,map_location=torch.device('cpu'))\n",
    "    model = smp.Unet(encoder_name=cfg.encoder_name, \n",
    "                     encoder_weights=cfg.encoder_weights, \n",
    "                     in_channels=cfg.in_channels, \n",
    "                     classes=cfg.classes)\n",
    "    model, stats = load_model_weights(model, m_path)\n",
    "    model.float()\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    models.append(model)\n",
    "\n",
    "mp = Model_pred(models, use_tta=cfg.tta, batch_size=cfg.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".tooltip {\n",
       "  position: relative;\n",
       "  display: inline-block;\n",
       "}\n",
       "\n",
       ".tooltip .tooltiptext {\n",
       "  visibility: hidden;\n",
       "  width: max-content;\n",
       "  max-width: 280px;\n",
       "  background-color: #00bcd4;\n",
       "  text-align: left;\n",
       "  color: white;\n",
       "  border-radius: 4px;\n",
       "  padding: 4px 4px;\n",
       "  border: solid 0px black;\n",
       "  line-height: 1em;\n",
       "\n",
       "  /* Position the tooltip */\n",
       "  position: absolute;\n",
       "  z-index: 1;\n",
       "}\n",
       "\n",
       ".tooltip:hover .tooltiptext {\n",
       "  visibility: visible;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Model_pred at 0x1ae5d0b23a0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".tooltip {\n",
       "  position: relative;\n",
       "  display: inline-block;\n",
       "}\n",
       "\n",
       ".tooltip .tooltiptext {\n",
       "  visibility: hidden;\n",
       "  width: max-content;\n",
       "  max-width: 280px;\n",
       "  background-color: #00bcd4;\n",
       "  text-align: left;\n",
       "  color: white;\n",
       "  border-radius: 4px;\n",
       "  padding: 4px 4px;\n",
       "  border: solid 0px black;\n",
       "  line-height: 1em;\n",
       "\n",
       "  /* Position the tooltip */\n",
       "  position: absolute;\n",
       "  z-index: 1;\n",
       "}\n",
       "\n",
       ".tooltip:hover .tooltiptext {\n",
       "  visibility: visible;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CL_HandE_1234_B004_bottomleft</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HandE_B005_CL_b_RGB_bottomleft</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               id  predicted\n",
       "0   CL_HandE_1234_B004_bottomleft        NaN\n",
       "1  HandE_B005_CL_b_RGB_bottomleft        NaN"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".tooltip {\n",
       "  position: relative;\n",
       "  display: inline-block;\n",
       "}\n",
       "\n",
       ".tooltip .tooltiptext {\n",
       "  visibility: hidden;\n",
       "  width: max-content;\n",
       "  max-width: 280px;\n",
       "  background-color: #00bcd4;\n",
       "  text-align: left;\n",
       "  color: white;\n",
       "  border-radius: 4px;\n",
       "  padding: 4px 4px;\n",
       "  border: solid 0px black;\n",
       "  line-height: 1em;\n",
       "\n",
       "  /* Position the tooltip */\n",
       "  position: absolute;\n",
       "  z-index: 1;\n",
       "}\n",
       "\n",
       ".tooltip:hover .tooltiptext {\n",
       "  visibility: visible;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d3898456bc94f3498b00976ec81f920",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###### File 0 ######\n",
      "C:\\Users\\soodn\\Downloads\\Naveksha\\Kaggle HuBMAP\\Data\\hubmap_colon_data\\Colon_data_reprocessed\\Colon_data_reprocessed/test/0.tiff\n"
     ]
    },
    {
     "ename": "RasterioIOError",
     "evalue": "C:\\Users\\soodn\\Downloads\\Naveksha\\Kaggle HuBMAP\\Data\\hubmap_colon_data\\Colon_data_reprocessed\\Colon_data_reprocessed/test/0.tiff: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCPLE_OpenFailedError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mrasterio\\_base.pyx\u001b[0m in \u001b[0;36mrasterio._base.DatasetBase.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mrasterio\\_shim.pyx\u001b[0m in \u001b[0;36mrasterio._shim.open_dataset\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mrasterio\\_err.pyx\u001b[0m in \u001b[0;36mrasterio._err.exc_wrap_pointer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mCPLE_OpenFailedError\u001b[0m: C:\\Users\\soodn\\Downloads\\Naveksha\\Kaggle HuBMAP\\Data\\hubmap_colon_data\\Colon_data_reprocessed\\Colon_data_reprocessed/test/0.tiff: No such file or directory",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mRasterioIOError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-b7ee9117ce2d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcfg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34mf'/test/{idx}.tiff'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mHubmapDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstats\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshift\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshift\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtile_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Predicting...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-4d0e3d7cfeb5>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, file, stats, scale, shift, output_shape, s_th)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0midentity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrasterio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAffine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrasterio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0midentity\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_threads\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'all_cpus'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[0msubdatasets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdatasets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\rasterio\\env.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0menv_ctor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 445\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\rasterio\\__init__.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(fp, mode, driver, width, height, count, crs, transform, dtype, nodata, sharing, **kwargs)\u001b[0m\n\u001b[0;32m    217\u001b[0m         \u001b[1;31m# None.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 219\u001b[1;33m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDatasetReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdriver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msharing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msharing\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    220\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'r+'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_writer_for_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdriver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msharing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msharing\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mrasterio\\_base.pyx\u001b[0m in \u001b[0;36mrasterio._base.DatasetBase.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mRasterioIOError\u001b[0m: C:\\Users\\soodn\\Downloads\\Naveksha\\Kaggle HuBMAP\\Data\\hubmap_colon_data\\Colon_data_reprocessed\\Colon_data_reprocessed/test/0.tiff: No such file or directory"
     ]
    }
   ],
   "source": [
    "names,preds = [],[]\n",
    "for idx,row in tqdm(df_sample.iterrows(),total=len(df_sample)):\n",
    "    \n",
    "    print(f'###### File {idx} ######')\n",
    "    f = cfg.data_path+f'/test/{idx}.tiff'\n",
    "    print (f)\n",
    "    ds = HubmapDataset(f, stats, scale=cfg.scale, shift=cfg.shift, output_shape=cfg.tile_shape)\n",
    "    \n",
    "    print('Predicting...')   \n",
    "    pred = mp.predict(ds)\n",
    "       \n",
    "    print('Rezising...')\n",
    "    shape = ds.data.shape\n",
    "    pred = cv2.resize((pred*255).astype('uint8'), (shape[1], shape[0]))\n",
    "    \n",
    "    pred = (pred>cfg.threshold*255).astype(np.uint8)\n",
    "    \n",
    "    #convert to rle\n",
    "    #https://www.kaggle.com/bguberfain/memory-aware-rle-encoding\n",
    "    rle = rle_encode_less_memory(pred)\n",
    "    names.append(idx)\n",
    "    preds.append(rle)\n",
    "    \n",
    "    print('Plotting')\n",
    "    fig, ax = plt.subplots(figsize=(15,15))\n",
    "    ax.imshow(cv2.resize(pred, (1024, 1024*shape[0]//shape[1])))\n",
    "    plt.show()\n",
    "    \n",
    "    del pred\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'id':names,'predicted':preds})\n",
    "df.to_csv('submission.csv',index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enc2mask(encs, shape):\n",
    "    img = np.zeros(shape[0] * shape[1], dtype=np.uint8)\n",
    "    for m, enc in enumerate(encs):\n",
    "        if isinstance(enc, np.float) and np.isnan(enc):\n",
    "            continue\n",
    "        enc_split = enc.split()\n",
    "        for i in range(len(enc_split) // 2):\n",
    "            start = int(enc_split[2 * i]) - 1\n",
    "            length = int(enc_split[2 * i + 1])\n",
    "            img[start: start + length] = 1 + m\n",
    "    return img.reshape(shape).T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
