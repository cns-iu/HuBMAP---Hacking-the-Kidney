{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OcsetTMwKXqC"
   },
   "source": [
    "# HuBMAP - deepflash2 submission\n",
    "\n",
    "> Submission kernel for model trained with efficient region based sampling. \n",
    "\n",
    "***\n",
    "\n",
    "\n",
    "## Highlights\n",
    "\n",
    "- *Super fast submission*: ~30 min for a 5-model ensemble on the public test set\n",
    "- Using overlapping tiles and gaussian weighting similar to [nnuet](https://www.nature.com/articles/s41592-020-01008-z), which removes almost all prediction artifacts\n",
    "\n",
    "## Related Kernels\n",
    "- Training: https://www.kaggle.com/matjes/hubmap-deepflash2-train\n",
    "- Sampling: https://www.kaggle.com/matjes/hubmap-efficient-sampling-ii-deepflash2\n",
    "\n",
    "To make this kernel fast and reliable, special thanks go to @leighplt ([kernel](https://www.kaggle.com/leighplt/pytorch-fcn-resnet50)) and @iafoss ([kernel](https://www.kaggle.com/iafoss/hubmap-pytorch-fast-ai-starter-sub))!\n",
    "\n",
    "## Overview\n",
    "\n",
    "1. Installation and package loading\n",
    "2. Functions and classes for prediction\n",
    "3. Configuration\n",
    "4. Prediction\n",
    "5. Submission\n",
    "\n",
    "### Installation and package loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '../input/deepflash2-lfs'\n"
     ]
    }
   ],
   "source": [
    "!pip install -q --no-deps ../input/deepflash2-lfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2021-06-25T12:31:48.266757Z",
     "iopub.status.busy": "2021-06-25T12:31:48.26637Z",
     "iopub.status.idle": "2021-06-25T12:32:18.319795Z",
     "shell.execute_reply": "2021-06-25T12:32:18.318807Z",
     "shell.execute_reply.started": "2021-06-25T12:31:48.266647Z"
    }
   },
   "outputs": [],
   "source": [
    "# Install deepflash2 and dependencies\n",
    "import sys\n",
    "sys.path.append(\"../input/segmentation-models-pytorch-install\")\n",
    "\n",
    "import cv2, torch, gc, rasterio\n",
    "import torch.nn.functional as F\n",
    "import deepflash2.tta as tta\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd, numpy as np\n",
    "import segmentation_models_pytorch as smp\n",
    "from pathlib import Path\n",
    "from rasterio.windows import Window\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from scipy .ndimage.filters import gaussian_filter\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions and classes for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2021-06-25T12:32:18.322312Z",
     "iopub.status.busy": "2021-06-25T12:32:18.321767Z",
     "iopub.status.idle": "2021-06-25T12:32:18.334433Z",
     "shell.execute_reply": "2021-06-25T12:32:18.333446Z",
     "shell.execute_reply.started": "2021-06-25T12:32:18.322277Z"
    }
   },
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/bguberfain/memory-aware-rle-encoding\n",
    "#with transposed mask\n",
    "def rle_encode_less_memory(img):\n",
    "    #the image should be transposed\n",
    "    pixels = img.T.flatten()\n",
    "    \n",
    "    # This simplified method requires first and last pixel to be zero\n",
    "    pixels[0] = 0\n",
    "    pixels[-1] = 0\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 2\n",
    "    runs[1::2] -= runs[::2]\n",
    "    \n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "def load_model_weights(model, file, strict=True):\n",
    "    state = torch.load(file, map_location='cpu')\n",
    "    stats = state['stats']\n",
    "    model_state = state['model']\n",
    "    model.load_state_dict(model_state, strict=strict)\n",
    "    return model, stats\n",
    "\n",
    "# from https://github.com/MIC-DKFZ/nnUNet/blob/2fade8f32607220f8598544f0d5b5e5fa73768e5/nnunet/network_architecture/neural_network.py#L250\n",
    "def _get_gaussian(patch_size, sigma_scale=1. / 8) -> np.ndarray:\n",
    "    tmp = np.zeros(patch_size)\n",
    "    center_coords = [i // 2 for i in patch_size]\n",
    "    sigmas = [i * sigma_scale for i in patch_size]\n",
    "    tmp[tuple(center_coords)] = 1\n",
    "    gaussian_importance_map = gaussian_filter(tmp, sigmas, 0, mode='constant', cval=0)\n",
    "    gaussian_importance_map = gaussian_importance_map / np.max(gaussian_importance_map) * 1\n",
    "    gaussian_importance_map = gaussian_importance_map.astype(np.float32)\n",
    "\n",
    "    # gaussian_importance_map cannot be 0, otherwise we may end up with nans!\n",
    "    gaussian_importance_map[gaussian_importance_map == 0] = np.min(\n",
    "        gaussian_importance_map[gaussian_importance_map != 0])\n",
    "\n",
    "    return gaussian_importance_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2021-06-25T12:32:18.33714Z",
     "iopub.status.busy": "2021-06-25T12:32:18.336375Z",
     "iopub.status.idle": "2021-06-25T12:32:18.382958Z",
     "shell.execute_reply": "2021-06-25T12:32:18.381631Z",
     "shell.execute_reply.started": "2021-06-25T12:32:18.337101Z"
    }
   },
   "outputs": [],
   "source": [
    "# Some code adapted from https://www.kaggle.com/iafoss/hubmap-pytorch-fast-ai-starter-sub\n",
    "class HubmapDataset(Dataset):\n",
    "    'HubmapDataset class that does not load the full tiff files.'\n",
    "    def __init__(self, file, stats, scale=3, shift=.8, output_shape=(512,512), s_th = 40):\n",
    "        \n",
    "        self.mean, self.std = stats\n",
    "        self.scale = scale\n",
    "        self.shift = shift\n",
    "        self.output_shape = output_shape\n",
    "        self.input_shape = tuple(int(t*scale) for t in self.output_shape)      \n",
    "        self.s_th = s_th #saturation blancking threshold\n",
    "        self.p_th = 1000*(self.output_shape[0]//256)**2 #threshold for the minimum number of pixels\n",
    "\n",
    "        identity = rasterio.Affine(1, 0, 0, 0, 1, 0)\n",
    "        self.data = rasterio.open(file, transform = identity, num_threads='all_cpus')\n",
    "        if self.data.count != 3:\n",
    "            subdatasets = self.data.subdatasets\n",
    "            self.layers = []\n",
    "            if len(subdatasets) > 0:\n",
    "                for i, subdataset in enumerate(subdatasets, 0):\n",
    "                    self.layers.append(rasterio.open(subdataset))\n",
    "            \n",
    "        # Tiling\n",
    "        self.slices = []\n",
    "        self.out_slices = []\n",
    "        self.out_data_shape = tuple(int(x//self.scale) for x in self.data.shape)\n",
    "        start_points = [o//2 for o in self.output_shape]\n",
    "        end_points = [(s - st) for s, st in zip(self.out_data_shape, start_points)]\n",
    "        n_points = [int(s//(o*self.shift))+1 for s, o in zip(self.out_data_shape, self.output_shape)]\n",
    "        center_points = [np.linspace(st, e, num=n, endpoint=True, dtype=np.int64) for st, e, n in zip(start_points, end_points, n_points)]\n",
    "        for cx in center_points[1]:\n",
    "            for cy in center_points[0]:\n",
    "                # Calculate output slices for whole image\n",
    "                slices = tuple(slice(int((c*self.scale - o/2).clip(0, s)), int((c*self.scale + o/2).clip(max=s)))\n",
    "                                 for (c, o, s) in zip((cy, cx), self.input_shape, self.data.shape))\n",
    "                self.slices.append(slices)\n",
    "                \n",
    "                out_slices = tuple(slice(int((c - o/2).clip(0, s)), int((c + o/2).clip(max=s)))\n",
    "                                 for (c, o, s) in zip((cy, cx), self.output_shape, self.out_data_shape))\n",
    "                self.out_slices.append(out_slices)\n",
    "                \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.slices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        slices = self.slices[idx]\n",
    "        if self.data.count == 3: # normal\n",
    "            img = self.data.read([1, 2, 3], \n",
    "                window=Window.from_slices(*slices)\n",
    "            )\n",
    "            img = np.moveaxis(img, 0, -1)\n",
    "        else: # with subdatasets/layers\n",
    "            img = np.zeros((*self.input_shape, 3), dtype=np.uint8)\n",
    "            for fl in range(3):\n",
    "                img[:, :, fl] = self.layers[fl].read(\n",
    "                    window=Window.from_slices(*slices)\n",
    "                )\n",
    "        \n",
    "        if self.scale!=1:\n",
    "            img = cv2.resize(img, self.output_shape, interpolation = cv2.INTER_AREA)\n",
    "        \n",
    "        #check for empty imges\n",
    "        hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "        h,s,v = cv2.split(hsv)\n",
    "        if (s>self.s_th).sum() <= self.p_th or img.sum() <= self.p_th:\n",
    "            # Remove if idx=-1\n",
    "            idx = -1\n",
    "        \n",
    "        img = (img/255.0 - self.mean)/self.std\n",
    "        img = img.transpose(2, 0, 1).astype('float32')\n",
    "        \n",
    "        return torch.from_numpy(img), idx\n",
    "    \n",
    "class Model_pred:\n",
    "    'Class for prediction with multiple models'\n",
    "    def __init__(self, models, use_tta=True, batch_size=32):\n",
    "        self.models = models\n",
    "        self.bs = batch_size\n",
    "        self.tfms = [tta.HorizontalFlip(), tta.VerticalFlip()] if use_tta else []\n",
    "        \n",
    "    def predict(self, ds):\n",
    "        #rasterio cannot be used with multiple workers\n",
    "        dl = DataLoader(ds, self.bs, num_workers=0, shuffle=False, pin_memory=True)\n",
    "        \n",
    "        # Create zero arrays\n",
    "        pred = np.zeros(ds.out_data_shape, dtype='float32')\n",
    "        merge_map = np.zeros(ds.out_data_shape, dtype='float32')\n",
    "        \n",
    "        # Gaussian weights\n",
    "        gw_numpy = _get_gaussian(ds.output_shape)\n",
    "        gw = torch.from_numpy(gw_numpy).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, idxs in tqdm(iter(dl), total=len(dl)):\n",
    "                if ((idxs>=0).sum() > 0): #exclude empty images\n",
    "                    images = images[idxs>=0].to(device)\n",
    "                    idxs = idxs[idxs>=0]\n",
    "                    merger = tta.Merger()\n",
    "                    for t in tta.Compose(self.tfms):\n",
    "                        aug_images = t.augment_image(images)\n",
    "                        model_merger = tta.Merger()\n",
    "                        for model in self.models:\n",
    "                            out = model(aug_images)\n",
    "                            out = F.softmax(out, dim=1)\n",
    "                            model_merger.append(out)\n",
    "                        out = t.deaugment_mask(model_merger.result())\n",
    "                        merger.append(out)\n",
    "            \n",
    "                    # Apply gaussian weigthing\n",
    "                    batch_smx = merger.result()*gw.view(1,1,*gw.shape)\n",
    "                    batch_smx = [x for x in batch_smx.permute(0,2,3,1).cpu().numpy()]\n",
    "\n",
    "                    for smx, idx in zip(batch_smx, idxs):\n",
    "                        slcs = ds.out_slices[idx]\n",
    "                        # Only using positive class here\n",
    "                        pred[slcs] += smx[...,1]\n",
    "                        merge_map[slcs] += gw_numpy\n",
    "\n",
    "        pred /= merge_map\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-25T12:32:18.385307Z",
     "iopub.status.busy": "2021-06-25T12:32:18.384542Z",
     "iopub.status.idle": "2021-06-25T12:32:18.396737Z",
     "shell.execute_reply": "2021-06-25T12:32:18.395588Z",
     "shell.execute_reply.started": "2021-06-25T12:32:18.385263Z"
    }
   },
   "outputs": [],
   "source": [
    "class CONFIG():  \n",
    "    # data paths\n",
    "    data_path = Path('../')\n",
    "    # Local train on all data\n",
    "    model_path = Path('../input/hubmap-models-final/Unet_efficientnet-b2_1.5_2500_Unet_b2_aughard_DiceCELoss_no-d48/Unet_efficientnet-b2_1.5_2500_Unet_b2_aughard_DiceCELoss_no-d48')\n",
    "    #model_path = Path('../input/hubmap-deepflash2-train')\n",
    "   \n",
    "    # zoom factor (e.g., 3 means downscaling from 1536 to 512)\n",
    "    scale = 3 \n",
    "    # tile shift for prediction\n",
    "    shift = 0.8 \n",
    "    tile_shape = (512, 512)\n",
    "\n",
    "    # pytorch model (https://github.com/qubvel/segmentation_models.pytorch)\n",
    "    encoder_name = \"efficientnet-b2\"\n",
    "    encoder_weights = None\n",
    "    in_channels = 3\n",
    "    classes = 2\n",
    "    \n",
    "    # dataloader \n",
    "    batch_size = 32\n",
    "    \n",
    "    # test time augmentation\n",
    "    tta = True\n",
    "    # prediction threshold\n",
    "    threshold = 0.5\n",
    "    \n",
    "cfg = CONFIG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-25T12:33:52.802892Z",
     "iopub.status.busy": "2021-06-25T12:33:52.802542Z",
     "iopub.status.idle": "2021-06-25T12:33:53.032895Z",
     "shell.execute_reply": "2021-06-25T12:33:53.030941Z",
     "shell.execute_reply.started": "2021-06-25T12:33:52.802863Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '..\\\\input\\\\hubmap-kidney-segmentation\\\\sample_submission.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-f127784436a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Sample submissions for ids\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf_sample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;34m'sample_submission.csv'\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cuda'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'cpu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Models (see https://github.com/qubvel/segmentation_models.pytorch)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1048\u001b[0m             )\n\u001b[0;32m   1049\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1050\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1051\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1866\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1867\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1868\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1869\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"encoding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m         \"\"\"\n\u001b[1;32m-> 1362\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"replace\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 642\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '..\\\\input\\\\hubmap-kidney-segmentation\\\\sample_submission.csv'"
     ]
    }
   ],
   "source": [
    "# Sample submissions for ids\n",
    "df_sample = pd.read_csv(cfg.data_path/'sample_submission.csv',  index_col='id')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Models (see https://github.com/qubvel/segmentation_models.pytorch)\n",
    "MODELS = [f for f in cfg.model_path.iterdir() if f.suffix=='.pth']\n",
    "print(f'Found {len(MODELS)} models', *MODELS)\n",
    "\n",
    "models = []\n",
    "for i, m_path in enumerate(MODELS):\n",
    "    #state_dict = torch.load(path,map_location=torch.device('cpu'))\n",
    "    model = smp.Unet(encoder_name=cfg.encoder_name, \n",
    "                     encoder_weights=cfg.encoder_weights, \n",
    "                     in_channels=cfg.in_channels, \n",
    "                     classes=cfg.classes)\n",
    "    model, stats = load_model_weights(model, m_path)\n",
    "    model.float()\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    models.append(model)\n",
    "\n",
    "mp = Model_pred(models, use_tta=cfg.tta, batch_size=cfg.batch_size)\n",
    "print(len(models))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-25T12:33:55.29449Z",
     "iopub.status.busy": "2021-06-25T12:33:55.294173Z",
     "iopub.status.idle": "2021-06-25T12:33:55.355699Z",
     "shell.execute_reply": "2021-06-25T12:33:55.354379Z",
     "shell.execute_reply.started": "2021-06-25T12:33:55.294459Z"
    }
   },
   "outputs": [],
   "source": [
    "names,preds = [],[]\n",
    "for idx,row in tqdm(df_sample.iterrows(),total=len(df_sample)):\n",
    "    \n",
    "    print(f'###### File {idx} ######')\n",
    "    f = cfg.data_path/'test'/f'{idx}.tiff'\n",
    "    ds = HubmapDataset(f, stats, scale=cfg.scale, shift=cfg.shift, output_shape=cfg.tile_shape)\n",
    "    \n",
    "    print('Predicting...')   \n",
    "    pred = mp.predict(ds)\n",
    "       \n",
    "    print('Rezising...')\n",
    "    shape = ds.data.shape\n",
    "    pred = cv2.resize((pred*255).astype('uint8'), (shape[1], shape[0]))\n",
    "    \n",
    "    pred = (pred>cfg.threshold*255).astype(np.uint8)\n",
    "    \n",
    "    #convert to rle\n",
    "    #https://www.kaggle.com/bguberfain/memory-aware-rle-encoding\n",
    "    rle = rle_encode_less_memory(pred)\n",
    "    names.append(idx)\n",
    "    preds.append(rle)\n",
    "    \n",
    "    print('Plotting')\n",
    "    fig, ax = plt.subplots(figsize=(15,15))\n",
    "    ax.imshow(cv2.resize(pred, (1024, 1024*shape[0]//shape[1])))\n",
    "    plt.show()\n",
    "    \n",
    "    del pred\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'id':names,'predicted':preds})\n",
    "df.to_csv('submission.csv',index=False)\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
