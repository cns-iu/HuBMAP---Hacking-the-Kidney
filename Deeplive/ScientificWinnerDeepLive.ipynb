{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "distant-large",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import cv2\n",
    "import json\n",
    "import glob\n",
    "import torch\n",
    "import random\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "from rasterio.windows import Window\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "superior-speed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    \"\"\"\n",
    "    Seeds basic parameters for reproductibility of results.\n",
    "\n",
    "    Args:\n",
    "        seed (int): Number of the seed.\n",
    "    \"\"\"\n",
    "\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    \n",
    "SEED = 2021\n",
    "seed_everything(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "smoking-latest",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = [\"ftus\"]\n",
    "\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "\n",
    "MEAN = np.array([0.66437738, 0.50478148, 0.70114894])\n",
    "STD = np.array([0.15825711, 0.24371008, 0.13832686])\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "IDENTITY = rasterio.Affine(1, 0, 0, 0, 1, 0)\n",
    "FLIPS = [[-1], [-2], [-2, -1]]\n",
    "\n",
    "DATA_PATH = r'C:/Users/soodn/Downloads/Naveksha/Kaggle HuBMAP/'\n",
    "IMG_PATH = DATA_PATH + 'Data/hubmap-kidney-segmentation-data/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "suspended-thailand",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tifffile as tiff\n",
    "\n",
    "\n",
    "def load_image(img_path, df_info, reduce_factor=1):\n",
    "    \"\"\"\n",
    "    Load image and make sure sizes matches df_info\n",
    "    \"\"\"\n",
    "    image_fname = img_path.rsplit(\"/\", -1)[-1]\n",
    "    \n",
    "    \n",
    "    W = int(df_info[df_info.image_file == image_fname][\"width_pixels\"])\n",
    "    H = int(df_info[df_info.image_file == image_fname][\"height_pixels\"])\n",
    "\n",
    "    img = tiff.imread(img_path).squeeze()\n",
    "\n",
    "    channel_pos = np.argwhere(np.array(img.shape) == 3)[0][0]\n",
    "    W_pos = np.argwhere(np.array(img.shape) == W)[0][0]\n",
    "    H_pos = np.argwhere(np.array(img.shape) == H)[0][0]\n",
    "\n",
    "    img = np.moveaxis(img, (H_pos, W_pos, channel_pos), (0, 1, 2))\n",
    "    \n",
    "    if reduce_factor > 1:\n",
    "        img = cv2.resize(\n",
    "            img,\n",
    "            (img.shape[1] // reduce_factor, img.shape[0] // reduce_factor),\n",
    "            interpolation=cv2.INTER_AREA,\n",
    "        )\n",
    "        \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "permanent-patient",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as albu\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "def HE_preprocess(augment=False, visualize=False, mean=MEAN, std=STD):\n",
    "    \"\"\"\n",
    "    Returns transformations for the H&E images.\n",
    "\n",
    "    Args:\n",
    "        augment (bool, optional): Whether to apply augmentations. Defaults to True.\n",
    "        visualize (bool, optional): Whether to use transforms for visualization. Defaults to False.\n",
    "        mean ([type], optional): Mean for normalization. Defaults to MEAN.\n",
    "        std ([type], optional): Standard deviation for normalization. Defaults to STD.\n",
    "\n",
    "    Returns:\n",
    "        albumentation transforms: transforms.\n",
    "    \"\"\"\n",
    "    if visualize:\n",
    "        normalizer = albu.Compose(\n",
    "            [albu.Normalize(mean=[0, 0, 0], std=[1, 1, 1]), ToTensorV2()], p=1\n",
    "        )\n",
    "    else:\n",
    "        normalizer = albu.Compose(\n",
    "            [albu.Normalize(mean=mean, std=std), ToTensorV2()], p=1\n",
    "        )\n",
    "    \n",
    "    if augment:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    return normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "alive-japan",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rle_encode_less_memory(img):\n",
    "    pixels = img.T.flatten()\n",
    "    pixels[0] = 0\n",
    "    pixels[-1] = 0\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 2\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "def enc2mask(encs, shape):\n",
    "    img = np.zeros(shape[0] * shape[1], dtype=np.uint8)\n",
    "    for m, enc in enumerate(encs):\n",
    "        if isinstance(enc, np.float) and np.isnan(enc):\n",
    "            continue\n",
    "        enc_split = enc.split()\n",
    "        for i in range(len(enc_split) // 2):\n",
    "            start = int(enc_split[2 * i]) - 1\n",
    "            length = int(enc_split[2 * i + 1])\n",
    "            img[start: start + length] = 1 + m\n",
    "\n",
    "    return img.reshape(shape).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "involved-industry",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        original_img_path,\n",
    "        rle=None,\n",
    "        overlap_factor=1,\n",
    "        tile_size=256,\n",
    "        reduce_factor=4,\n",
    "        transforms=None,\n",
    "    ):\n",
    "        self.original_img = load_image(original_img_path, full_size=reduce_factor > 1)\n",
    "        self.orig_size = self.original_img.shape\n",
    "\n",
    "        # self.original_img = lab_normalization(self.original_img)\n",
    "\n",
    "        self.raw_tile_size = tile_size\n",
    "        self.reduce_factor = reduce_factor\n",
    "        self.tile_size = tile_size * reduce_factor\n",
    "\n",
    "        self.overlap_factor = overlap_factor\n",
    "\n",
    "        self.positions = self.get_positions()\n",
    "        self.transforms = transforms\n",
    "\n",
    "        if rle is not None:\n",
    "            self.mask = enc2mask(rle, (self.orig_size[1], self.orig_size[0])) > 0\n",
    "        else:\n",
    "            self.mask = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.positions)\n",
    "\n",
    "    def get_positions(self):\n",
    "        top_x = np.arange(\n",
    "            0,\n",
    "            self.orig_size[0],  # +self.tile_size,\n",
    "            int(self.tile_size / self.overlap_factor),\n",
    "        )\n",
    "        top_y = np.arange(\n",
    "            0,\n",
    "            self.orig_size[1],  # +self.tile_size,\n",
    "            int(self.tile_size / self.overlap_factor),\n",
    "        )\n",
    "        starting_positions = []\n",
    "        for x in top_x:\n",
    "            right_space = self.orig_size[0] - (x + self.tile_size)\n",
    "            if right_space > 0:\n",
    "                boundaries_x = (x, x + self.tile_size)\n",
    "            else:\n",
    "                boundaries_x = (x + right_space, x + right_space + self.tile_size)\n",
    "\n",
    "            for y in top_y:\n",
    "                down_space = self.orig_size[1] - (y + self.tile_size)\n",
    "                if down_space > 0:\n",
    "                    boundaries_y = (y, y + self.tile_size)\n",
    "                else:\n",
    "                    boundaries_y = (y + down_space, y + down_space + self.tile_size)\n",
    "                starting_positions.append((boundaries_x, boundaries_y))\n",
    "\n",
    "        return starting_positions\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pos_x, pos_y = self.positions[idx]\n",
    "        img = self.original_img[pos_x[0]: pos_x[1], pos_y[0]: pos_y[1], :]\n",
    "\n",
    "        # img = lab_normalization(img)\n",
    "\n",
    "        # down scale to tile size\n",
    "        if self.reduce_factor > 1:\n",
    "            img = cv2.resize(\n",
    "                img, (self.raw_tile_size, self.raw_tile_size), interpolation=cv2.INTER_AREA\n",
    "            )\n",
    "\n",
    "        if self.transforms:\n",
    "            img = self.transforms(image=img)[\"image\"]\n",
    "\n",
    "        pos = np.array([pos_x[0], pos_x[1], pos_y[0], pos_y[1]])\n",
    "\n",
    "        return img, pos\n",
    "    \n",
    "class InferenceEfficientDataset(InferenceDataset):\n",
    "    \"\"\"\n",
    "    Refs : \n",
    "    https://www.kaggle.com/iafoss/hubmap-pytorch-fast-ai-starter-sub/\n",
    "    https://www.kaggle.com/finlay/pytorch-fcn-resnet50-in-20-minute\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        original_img_path,\n",
    "        rle=None,\n",
    "        overlap_factor=1,\n",
    "        tile_size=256,\n",
    "        reduce_factor=4,\n",
    "        transforms=None,\n",
    "    ):\n",
    "            \n",
    "        self.raw_tile_size = tile_size\n",
    "        self.reduce_factor = reduce_factor\n",
    "        self.tile_size = tile_size * reduce_factor\n",
    "        \n",
    "        self.overlap_factor = overlap_factor\n",
    "        self.transforms = transforms\n",
    "\n",
    "        # Load image with rasterio        \n",
    "        self.original_img = rasterio.open(original_img_path, transform=IDENTITY, num_threads='all_cpus')\n",
    "        if self.original_img.count != 3:\n",
    "            self.layers = [rasterio.open(subd) for subd in self.original_img.subdatasets]\n",
    "                    \n",
    "        self.orig_size = self.original_img.shape\n",
    "\n",
    "        self.positions = self.get_positions()\n",
    "        \n",
    "        if rle is not None:\n",
    "            self.mask = enc2mask(rle, (self.orig_size[1], self.orig_size[0])) > 0\n",
    "        else:\n",
    "            self.mask = None\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # Window\n",
    "        pos_x, pos_y = self.positions[idx]\n",
    "        x1, x2 = pos_x[0], pos_x[1]\n",
    "        y1, y2 = pos_y[0], pos_y[1]\n",
    "        window = Window.from_slices((x1, x2), (y1, y2))\n",
    "\n",
    "        # Retrieve slice\n",
    "        if self.original_img.count == 3:  # normal\n",
    "            img = self.original_img.read([1, 2, 3], window=window)\n",
    "            img = np.moveaxis(img, 0, -1)\n",
    "        else:  # with subdatasets/layers\n",
    "            img = np.zeros((self.tile_size, self.tile_size, 3), dtype=np.uint8)\n",
    "            for fl in range(3):\n",
    "                img[:, :, fl] = self.layers[fl].read(window=window) \n",
    "\n",
    "        # Downscale to tile size\n",
    "        img = cv2.resize(\n",
    "            img, (self.raw_tile_size, self.raw_tile_size), interpolation=cv2.INTER_AREA\n",
    "        )\n",
    "        img = self.transforms(image=img)[\"image\"]\n",
    "        \n",
    "        pos = np.array([pos_x[0], pos_x[1], pos_y[0], pos_y[1]])\n",
    "\n",
    "        return img, pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "younger-worse",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_weights(model, filename, verbose=1, cp_folder=\"\"):\n",
    "    \"\"\"\n",
    "    Loads the weights of a PyTorch model. The exception handles cpu/gpu incompatibilities.\n",
    "\n",
    "    Args:\n",
    "        model (torch model): Model to load the weights to.\n",
    "        filename (str): Name of the checkpoint.\n",
    "        verbose (int, optional): Whether to display infos. Defaults to 1.\n",
    "        cp_folder (str, optional): Folder to load from. Defaults to \"\".\n",
    "\n",
    "    Returns:\n",
    "        torch model: Model with loaded weights.\n",
    "    \"\"\"\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\n -> Loading weights from {os.path.join(cp_folder,filename)}\\n\")\n",
    "    try:\n",
    "        model.load_state_dict(os.path.join(cp_folder, filename), strict=True)\n",
    "    except BaseException:\n",
    "        model.load_state_dict(\n",
    "            torch.load(os.path.join(cp_folder, filename), map_location=\"cpu\"),\n",
    "            strict=True,\n",
    "        )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "previous-accuracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch\n",
    "from segmentation_models_pytorch.encoders import encoders\n",
    "\n",
    "\n",
    "DECODERS = [\"Unet\", \"Linknet\", \"FPN\", \"PSPNet\", \"DeepLabV3\", \"DeepLabV3Plus\", \"PAN\"]\n",
    "ENCODERS = list(encoders.keys())\n",
    "\n",
    "\n",
    "def define_model(\n",
    "    decoder_name, encoder_name, num_classes=1, activation=None, encoder_weights=\"imagenet\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Loads a segmentation architecture\n",
    "\n",
    "    Args:\n",
    "        decoder_name (str): Decoder name.\n",
    "        encoder_name (str): Encoder name.\n",
    "        num_classes (int, optional): Number of classes. Defaults to 1.\n",
    "        pretrained : pretrained original weights\n",
    "    Returns:\n",
    "        torch model -- Pretrained model.\n",
    "    \"\"\"\n",
    "    assert decoder_name in DECODERS, \"Decoder name not supported\"\n",
    "    assert encoder_name in ENCODERS, \"Encoder name not supported\"\n",
    "\n",
    "    decoder = getattr(segmentation_models_pytorch, decoder_name)\n",
    "\n",
    "    model = decoder(\n",
    "        encoder_name,\n",
    "        encoder_weights=encoder_weights,\n",
    "        classes=num_classes,\n",
    "        activation=activation,\n",
    "    )\n",
    "    model.num_classes = num_classes\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "italic-biology",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self, **entries):\n",
    "        self.__dict__.update(entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fewer-collector",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(cp_folder):\n",
    "    config = json.load(open(cp_folder + 'config.json', 'r'))\n",
    "    config = Config(**config)\n",
    "    \n",
    "    weights = sorted(glob.glob(cp_folder + \"*.pt\"))\n",
    "    models = []\n",
    "    \n",
    "    for weight in weights:\n",
    "        model = define_model(\n",
    "            config.decoder,\n",
    "            config.encoder,\n",
    "            num_classes=config.num_classes,\n",
    "            encoder_weights=None,\n",
    "        )\n",
    "        \n",
    "        model = load_model_weights(model, weight).to(DEVICE)\n",
    "        model.eval()\n",
    "        models.append(model)\n",
    "        \n",
    "#         break\n",
    "        \n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dated-growing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_scores_img(pred, truth, eps=1e-8):\n",
    "    \"\"\"\n",
    "    Dice metric for a single image as array.\n",
    "\n",
    "    Args:\n",
    "        pred (np array): Predictions.\n",
    "        truth (np array): Ground truths.\n",
    "        eps (float, optional): epsilon to avoid dividing by 0. Defaults to 1e-8.\n",
    "\n",
    "    Returns:\n",
    "        np array : dice value for each class\n",
    "    \"\"\"\n",
    "    pred = pred.reshape(-1) > 0\n",
    "    truth = truth.reshape(-1) > 0\n",
    "    intersect = (pred & truth).sum(-1)\n",
    "    union = pred.sum(-1) + truth.sum(-1)\n",
    "\n",
    "    dice = (2.0 * intersect + eps) / (union + eps)\n",
    "    return dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "wanted-dominant",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tile_weighting(size, sigma=1, alpha=1, eps=1e-6):\n",
    "    half = size // 2\n",
    "    w = np.ones((size, size), np.float32)\n",
    "\n",
    "    x = np.concatenate([np.mgrid[-half:0], np.mgrid[1: half + 1]])[:, None]\n",
    "    x = np.tile(x, (1, size))\n",
    "    x = half + 1 - np.abs(x)\n",
    "    y = x.T\n",
    "\n",
    "    w = np.minimum(x, y)\n",
    "    w = (w / w.max()) ** sigma\n",
    "    w = np.minimum(w, 1)\n",
    "\n",
    "    w = (w - np.min(w) + eps) / (np.max(w) - np.min(w) + eps)\n",
    "\n",
    "    w = np.where(w > alpha, 1, w)\n",
    "    w = w / alpha\n",
    "    w = np.clip(w, 1e-3, 1)\n",
    "\n",
    "    w = np.round(w, 3)\n",
    "    return w.astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "serial-stuart",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_entire_mask(dataset, models, batch_size=32, tta=False):\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "    weighting = torch.from_numpy(get_tile_weighting(dataset.tile_size, sigma=1, alpha=1))\n",
    "    weighting_cuda = weighting.clone().cuda().unsqueeze(0)\n",
    "    weighting = weighting.cuda().half()\n",
    "\n",
    "    global_pred = torch.zeros(\n",
    "        (dataset.orig_size[0], dataset.orig_size[1]),\n",
    "        dtype=torch.half, device=\"cuda\"\n",
    "    )\n",
    "    global_counter = torch.zeros(\n",
    "        (dataset.orig_size[0], dataset.orig_size[1]),\n",
    "        dtype=torch.half, device=\"cuda\"\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img, pos in tqdm(loader):\n",
    "            img = img.to(\"cuda\")\n",
    "            _, _, h, w = img.shape\n",
    "            \n",
    "            model_preds = []\n",
    "            for model in models:\n",
    "                if model.num_classes == 1:\n",
    "                    pred = model(img).view(1, -1, h, w).sigmoid().detach()\n",
    "                else:\n",
    "                    pred = model(img)[:, 0].view(1, -1, h, w).sigmoid().detach()\n",
    "\n",
    "                if tta:\n",
    "                    for f in FLIPS:\n",
    "                        pred_flip = model(torch.flip(img, f))\n",
    "                        if model.num_classes == 2:\n",
    "                            pred_flip = pred_flip[:, :1]\n",
    "\n",
    "                        pred_flip = torch.flip(pred_flip, f).view(1, -1, h, w).sigmoid().detach()\n",
    "                        pred += pred_flip\n",
    "                    pred = torch.div(pred, len(FLIPS) + 1)\n",
    "\n",
    "                model_preds.append(pred)\n",
    "\n",
    "            pred = torch.cat(model_preds, 0).mean(0)\n",
    "\n",
    "            pred = torch.nn.functional.interpolate(\n",
    "                pred.unsqueeze(1), (dataset.tile_size, dataset.tile_size), mode='area'\n",
    "            ).squeeze(1)\n",
    "            \n",
    "            pred = (pred * weighting_cuda).half()\n",
    "\n",
    "            for tile_idx, (x0, x1, y0, y1) in enumerate(pos):\n",
    "                global_pred[x0: x1, y0: y1] += pred[tile_idx]\n",
    "                global_counter[x0: x1, y0: y1] += weighting\n",
    "\n",
    "    for i in range(len(global_pred)):\n",
    "        global_pred[i] = torch.div(global_pred[i], global_counter[i])\n",
    "\n",
    "    return global_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "veterinary-technology",
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0.5\n",
    "\n",
    "USE_TTA = True # not DEBUG\n",
    "OVERLAP_FACTOR = 1.5\n",
    "\n",
    "CP_FOLDERS = [\n",
    "    DATA_PATH + 'Scripts/4. DeepLive/hubmap-cp/b1_2cfix/b1_2cfix/',\n",
    "    DATA_PATH + 'Scripts/4. DeepLive/hubmap-cp/b1_last/b1_last/'\n",
    "]\n",
    "CP_FOLDER = CP_FOLDERS[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "loved-thompson",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH + 'Data/hubmap-kidney-segmentation-data/sample_submission.csv')\n",
    "df_info = pd.read_csv(DATA_PATH + 'Data/hubmap-kidney-segmentation-data/HuBMAP-20-dataset_information.csv')\n",
    "rles = pd.read_csv(DATA_PATH + 'Data/hubmap-kidney-segmentation-data/test.csv')\n",
    "\n",
    "config = json.load(open(CP_FOLDER + 'config.json', 'r'))\n",
    "config = Config(**config)\n",
    "config.overlap_factor = OVERLAP_FACTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "personalized-municipality",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_test = r'C:\\Users\\soodn\\Downloads\\Naveksha\\Kaggle HuBMAP\\Data\\hubmap-kidney-segmentation-data\\test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "built-cookie",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_file</th>\n",
       "      <th>width_pixels</th>\n",
       "      <th>height_pixels</th>\n",
       "      <th>anatomical_structures_segmention_file</th>\n",
       "      <th>glomerulus_segmentation_file</th>\n",
       "      <th>patient_number</th>\n",
       "      <th>race</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>weight_kilograms</th>\n",
       "      <th>height_centimeters</th>\n",
       "      <th>bmi_kg/m^2</th>\n",
       "      <th>laterality</th>\n",
       "      <th>percent_cortex</th>\n",
       "      <th>percent_medulla</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CL_HandE_1234_B004_bottomleft</td>\n",
       "      <td>4704</td>\n",
       "      <td>4536</td>\n",
       "      <td>aa05346ff-anatomical-structure.json</td>\n",
       "      <td>aa05346ff.json</td>\n",
       "      <td>67347</td>\n",
       "      <td>White</td>\n",
       "      <td>Not Hispanic or Latino</td>\n",
       "      <td>Female</td>\n",
       "      <td>58</td>\n",
       "      <td>59.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>Right</td>\n",
       "      <td>80</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CL_HandE_1234_B004_bottomright</td>\n",
       "      <td>4704</td>\n",
       "      <td>4536</td>\n",
       "      <td>afa5e8098-anatomical-structure.json</td>\n",
       "      <td>afa5e8098.json</td>\n",
       "      <td>67377</td>\n",
       "      <td>White</td>\n",
       "      <td>Not Hispanic or Latino</td>\n",
       "      <td>Female</td>\n",
       "      <td>58</td>\n",
       "      <td>59.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>Right</td>\n",
       "      <td>55</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CL_HandE_1234_B004_topleft</td>\n",
       "      <td>4704</td>\n",
       "      <td>4536</td>\n",
       "      <td>54f2eec69-anatomical-structure.json</td>\n",
       "      <td>54f2eec69.json</td>\n",
       "      <td>67548</td>\n",
       "      <td>Black or African American</td>\n",
       "      <td>Not Hispanic or Latino</td>\n",
       "      <td>Male</td>\n",
       "      <td>58</td>\n",
       "      <td>79.9</td>\n",
       "      <td>190.5</td>\n",
       "      <td>22.0</td>\n",
       "      <td>Right</td>\n",
       "      <td>75</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CL_HandE_1234_B004_topright</td>\n",
       "      <td>4704</td>\n",
       "      <td>4536</td>\n",
       "      <td>d488c759a-anatomical-structure.json</td>\n",
       "      <td>d488c759a.json</td>\n",
       "      <td>68138</td>\n",
       "      <td>White</td>\n",
       "      <td>Not Hispanic or Latino</td>\n",
       "      <td>Female</td>\n",
       "      <td>66</td>\n",
       "      <td>81.5</td>\n",
       "      <td>158.8</td>\n",
       "      <td>32.2</td>\n",
       "      <td>Left</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HandE_B005_CL_b_RGB_bottomleft</td>\n",
       "      <td>4704</td>\n",
       "      <td>4536</td>\n",
       "      <td>1e2425f28-anatomical-structure.json</td>\n",
       "      <td>1e2425f28.json</td>\n",
       "      <td>63921</td>\n",
       "      <td>White</td>\n",
       "      <td>Not Hispanic or Latino</td>\n",
       "      <td>Male</td>\n",
       "      <td>48</td>\n",
       "      <td>131.5</td>\n",
       "      <td>193.0</td>\n",
       "      <td>35.3</td>\n",
       "      <td>Right</td>\n",
       "      <td>65</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HandE_B005_CL_b_RGB_bottomright</td>\n",
       "      <td>4704</td>\n",
       "      <td>4536</td>\n",
       "      <td>e79de561c-anatomical-structure.json</td>\n",
       "      <td>e79de561c.json</td>\n",
       "      <td>67026</td>\n",
       "      <td>Black or African American</td>\n",
       "      <td>Not Hispanic or Latino</td>\n",
       "      <td>Male</td>\n",
       "      <td>53</td>\n",
       "      <td>73.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>26.5</td>\n",
       "      <td>Left</td>\n",
       "      <td>55</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HandE_B005_CL_b_RGB_topleft</td>\n",
       "      <td>4704</td>\n",
       "      <td>4536</td>\n",
       "      <td>c68fe75ea-anatomical-structure.json</td>\n",
       "      <td>c68fe75ea.json</td>\n",
       "      <td>67112</td>\n",
       "      <td>White</td>\n",
       "      <td>Not Hispanic or Latino</td>\n",
       "      <td>Male</td>\n",
       "      <td>56</td>\n",
       "      <td>91.2</td>\n",
       "      <td>167.6</td>\n",
       "      <td>32.5</td>\n",
       "      <td>Left</td>\n",
       "      <td>80</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HandE_B005_CL_b_RGB_topright</td>\n",
       "      <td>4704</td>\n",
       "      <td>4536</td>\n",
       "      <td>095bf7a1f-anatomical-structure.json</td>\n",
       "      <td>095bf7a1f.json</td>\n",
       "      <td>68250</td>\n",
       "      <td>White</td>\n",
       "      <td>Not Hispanic or Latino</td>\n",
       "      <td>Female</td>\n",
       "      <td>44</td>\n",
       "      <td>71.7</td>\n",
       "      <td>160.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>Right</td>\n",
       "      <td>65</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        image_file  width_pixels  height_pixels  \\\n",
       "0    CL_HandE_1234_B004_bottomleft          4704           4536   \n",
       "1   CL_HandE_1234_B004_bottomright          4704           4536   \n",
       "2       CL_HandE_1234_B004_topleft          4704           4536   \n",
       "3      CL_HandE_1234_B004_topright          4704           4536   \n",
       "4   HandE_B005_CL_b_RGB_bottomleft          4704           4536   \n",
       "5  HandE_B005_CL_b_RGB_bottomright          4704           4536   \n",
       "6      HandE_B005_CL_b_RGB_topleft          4704           4536   \n",
       "7     HandE_B005_CL_b_RGB_topright          4704           4536   \n",
       "\n",
       "  anatomical_structures_segmention_file glomerulus_segmentation_file  \\\n",
       "0   aa05346ff-anatomical-structure.json               aa05346ff.json   \n",
       "1   afa5e8098-anatomical-structure.json               afa5e8098.json   \n",
       "2   54f2eec69-anatomical-structure.json               54f2eec69.json   \n",
       "3   d488c759a-anatomical-structure.json               d488c759a.json   \n",
       "4   1e2425f28-anatomical-structure.json               1e2425f28.json   \n",
       "5   e79de561c-anatomical-structure.json               e79de561c.json   \n",
       "6   c68fe75ea-anatomical-structure.json               c68fe75ea.json   \n",
       "7   095bf7a1f-anatomical-structure.json               095bf7a1f.json   \n",
       "\n",
       "   patient_number                       race               ethnicity     sex  \\\n",
       "0           67347                      White  Not Hispanic or Latino  Female   \n",
       "1           67377                      White  Not Hispanic or Latino  Female   \n",
       "2           67548  Black or African American  Not Hispanic or Latino    Male   \n",
       "3           68138                      White  Not Hispanic or Latino  Female   \n",
       "4           63921                      White  Not Hispanic or Latino    Male   \n",
       "5           67026  Black or African American  Not Hispanic or Latino    Male   \n",
       "6           67112                      White  Not Hispanic or Latino    Male   \n",
       "7           68250                      White  Not Hispanic or Latino  Female   \n",
       "\n",
       "   age  weight_kilograms  height_centimeters  bmi_kg/m^2 laterality  \\\n",
       "0   58              59.0               160.0        23.0      Right   \n",
       "1   58              59.0               160.0        23.0      Right   \n",
       "2   58              79.9               190.5        22.0      Right   \n",
       "3   66              81.5               158.8        32.2       Left   \n",
       "4   48             131.5               193.0        35.3      Right   \n",
       "5   53              73.0               166.0        26.5       Left   \n",
       "6   56              91.2               167.6        32.5       Left   \n",
       "7   44              71.7               160.0        28.0      Right   \n",
       "\n",
       "   percent_cortex  percent_medulla  \n",
       "0              80               20  \n",
       "1              55               45  \n",
       "2              75               25  \n",
       "3             100                0  \n",
       "4              65               35  \n",
       "5              55               45  \n",
       "6              80               20  \n",
       "7              65               35  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_info_colon = pd.read_csv(r'C:\\Users\\soodn\\Downloads\\Naveksha\\Kaggle HuBMAP\\Data\\hubmap_colon_data\\Colon_data_reprocessed\\Colon_data_reprocessed/HuBMAP-20-dataset_information.csv')\n",
    "df_info_colon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "medium-daily",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -> Loading weights from C:/Users/soodn/Downloads/Naveksha/Kaggle HuBMAP/Scripts/4. DeepLive/hubmap-cp/b1_2cfix/b1_2cfix\\Unet_efficientnet-b1_0.pt\n",
      "\n",
      "\n",
      " -> Loading weights from C:/Users/soodn/Downloads/Naveksha/Kaggle HuBMAP/Scripts/4. DeepLive/hubmap-cp/b1_2cfix/b1_2cfix\\Unet_efficientnet-b1_1.pt\n",
      "\n",
      "\n",
      " -> Loading weights from C:/Users/soodn/Downloads/Naveksha/Kaggle HuBMAP/Scripts/4. DeepLive/hubmap-cp/b1_2cfix/b1_2cfix\\Unet_efficientnet-b1_2.pt\n",
      "\n",
      "\n",
      " -> Loading weights from C:/Users/soodn/Downloads/Naveksha/Kaggle HuBMAP/Scripts/4. DeepLive/hubmap-cp/b1_2cfix/b1_2cfix\\Unet_efficientnet-b1_3.pt\n",
      "\n",
      "\n",
      " -> Loading weights from C:/Users/soodn/Downloads/Naveksha/Kaggle HuBMAP/Scripts/4. DeepLive/hubmap-cp/b1_2cfix/b1_2cfix\\Unet_efficientnet-b1_4.pt\n",
      "\n",
      "\n",
      " -> Loading weights from C:/Users/soodn/Downloads/Naveksha/Kaggle HuBMAP/Scripts/4. DeepLive/hubmap-cp/b1_last/b1_last\\Unet_efficientnet-b1_0.pt\n",
      "\n",
      "\n",
      " -> Loading weights from C:/Users/soodn/Downloads/Naveksha/Kaggle HuBMAP/Scripts/4. DeepLive/hubmap-cp/b1_last/b1_last\\Unet_efficientnet-b1_1.pt\n",
      "\n",
      "\n",
      " -> Loading weights from C:/Users/soodn/Downloads/Naveksha/Kaggle HuBMAP/Scripts/4. DeepLive/hubmap-cp/b1_last/b1_last\\Unet_efficientnet-b1_2.pt\n",
      "\n",
      "\n",
      " -> Loading weights from C:/Users/soodn/Downloads/Naveksha/Kaggle HuBMAP/Scripts/4. DeepLive/hubmap-cp/b1_last/b1_last\\Unet_efficientnet-b1_3.pt\n",
      "\n",
      "\n",
      " -> Loading weights from C:/Users/soodn/Downloads/Naveksha/Kaggle HuBMAP/Scripts/4. DeepLive/hubmap-cp/b1_last/b1_last\\Unet_efficientnet-b1_4.pt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "\n",
    "for cp_folder in CP_FOLDERS:\n",
    "    models += load_models(cp_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "military-stable",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perf_metrics(gt, pred):\n",
    "    n = 0\n",
    "    d = 0\n",
    "    for i in range(gt.shape[0]):\n",
    "        for j in range (gt.shape[1]):\n",
    "            if (gt[i][j]==pred[i][j]):\n",
    "                n = n+1\n",
    "            d = d+1\n",
    "    \n",
    "    return n/d, jaccard_score(gt.flatten(order='C'), pred.flatten(order='C')), directed_hausdorff(gt, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "mechanical-display",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t Image 2ec3f1bb9\n",
      "\n",
      " - Building dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soodn\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\rasterio\\__init__.py:219: NotGeoreferencedWarning: Dataset has no geotransform set. The identity matrix may be returned.\n",
      "  s = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " - Predicting masks\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc2fc57ef2904de2a2fab7b7f11f103e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " - Encoding\n",
      "False\n",
      "\n",
      "\t Image 3589adb90\n",
      "\n",
      " - Building dataset\n",
      "\n",
      " - Predicting masks\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "429b74a1fe1b428e8c0d8636609f1ca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/46 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " - Encoding\n",
      "False\n",
      "\n",
      "\t Image d488c759a\n",
      "\n",
      " - Building dataset\n",
      "\n",
      " - Predicting masks\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3fac9a832194611a9a1d811d964b295",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/93 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " - Encoding\n",
      "False\n",
      "\n",
      "\t Image aa05346ff\n",
      "\n",
      " - Building dataset\n",
      "\n",
      " - Predicting masks\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bc2da58edc9414cab616ca5b9bcc435",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " - Encoding\n",
      "False\n",
      "\n",
      "\t Image 57512b7f1\n",
      "\n",
      " - Building dataset\n",
      "\n",
      " - Predicting masks\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5242c6d990c460abac11156e20dc61e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " - Encoding\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "DEBUG = False\n",
    "sum_score = 0\n",
    "sum_pa = 0\n",
    "sum_ji = 0\n",
    "sum_haus = 0\n",
    "\n",
    "for img in df['id'].unique():\n",
    "    if DEBUG:  # Check performances on a validation image\n",
    "        img = \"2f6ecfcdf\"  # check repro\n",
    "#         img = \"4ef6695ce\" # biggest img\n",
    "        IMG_PATH = DATA_PATH + \"train\"\n",
    "        models = [models[0], models[5]]\n",
    "                  \n",
    "    \n",
    "    print(f'\\n\\t Image {img}')\n",
    "    \n",
    "    print(f'\\n - Building dataset')\n",
    "    \n",
    "    rle_truth = rles[rles['id'] == img][\"encoding\"]\n",
    "    \n",
    "    predict_dataset = InferenceEfficientDataset(\n",
    "        f\"{IMG_PATH}/{img}.tiff\",\n",
    "        rle=rle_truth,\n",
    "        overlap_factor=config.overlap_factor,\n",
    "        reduce_factor=config.reduce_factor,\n",
    "        tile_size=config.tile_size,\n",
    "        transforms=HE_preprocess(augment=False, visualize=False),\n",
    "    )\n",
    "    \n",
    "    print(f'\\n - Predicting masks')\n",
    "\n",
    "    global_pred = predict_entire_mask(\n",
    "        predict_dataset, models, batch_size=config.val_bs, tta=USE_TTA\n",
    "    )\n",
    "    \n",
    "    del predict_dataset\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    print('\\n - Encoding')\n",
    "    \n",
    "    global_pred_np = np.zeros(global_pred.size(), dtype=np.uint8)\n",
    "\n",
    "    for i in range(global_pred_np.shape[0]):\n",
    "        global_pred_np[i] = (global_pred[i] > THRESHOLD).cpu().numpy().astype(np.uint8)\n",
    "    print (np.all(global_pred_np==0))\n",
    "            \n",
    "    shape = df_info[df_info.image_file == img + \".tiff\"][['width_pixels', 'height_pixels']].values.astype(int)[0]\n",
    "    \n",
    "    rle = rle_encode_less_memory(global_pred_np)\n",
    "    df.loc[df.id == img, 'predicted'] = rle\n",
    "    \n",
    "    if DEBUG:\n",
    "        shape = df_info[df_info.image_file == img + \".tiff\"][['width_pixels', 'height_pixels']].values.astype(int)[0]\n",
    "        mask_truth = enc2mask(rle_truth, shape)\n",
    "        score = dice_scores_img(global_pred_np, mask_truth)\n",
    "        print(f\" -> Scored {score :.4f} with threshold {THRESHOLD:.2f}\")\n",
    "        break\n",
    "        \n",
    "    del global_pred, global_pred_np\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "df.to_csv('submission-deeplive.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pediatric-following",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "material-summit",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
