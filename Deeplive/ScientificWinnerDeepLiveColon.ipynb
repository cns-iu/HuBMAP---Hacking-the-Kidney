{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aggressive-paint",
   "metadata": {},
   "source": [
    "# HubMap- Hacking the Kidney\n",
    "#### Goal - Mapping the human body at function tissue unit level - detect FTUs in kidney and colon\n",
    "\n",
    "#### Implementation of Kaggle Notebooks - Generalizing DeepLive's Model on Colon Data\n",
    "\n",
    "Link 1 - https://github.com/navekshasood/HubMap\n",
    "\n",
    "###### Step 1 - Install useful libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "powered-karaoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import cv2\n",
    "import json\n",
    "import glob\n",
    "import torch\n",
    "import random\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tifffile as tiff\n",
    "import albumentations as albu\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "from rasterio.windows import Window\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wicked-international",
   "metadata": {},
   "source": [
    "###### Step 2 - Write utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sudden-tucson",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    \n",
    "SEED = 2021\n",
    "seed_everything(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "pointed-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(img_path, df_info, reduce_factor=1):\n",
    "    \"\"\"\n",
    "    Load image and make sure sizes matches df_info\n",
    "    \"\"\"\n",
    "    image_fname = img_path.rsplit(\"/\", -1)[-1]\n",
    "    \n",
    "    \n",
    "    W = int(df_info[df_info.image_file == image_fname][\"width_pixels\"])\n",
    "    H = int(df_info[df_info.image_file == image_fname][\"height_pixels\"])\n",
    "\n",
    "    img = tiff.imread(img_path).squeeze()\n",
    "\n",
    "    channel_pos = np.argwhere(np.array(img.shape) == 3)[0][0]\n",
    "    W_pos = np.argwhere(np.array(img.shape) == W)[0][0]\n",
    "    H_pos = np.argwhere(np.array(img.shape) == H)[0][0]\n",
    "\n",
    "    img = np.moveaxis(img, (H_pos, W_pos, channel_pos), (0, 1, 2))\n",
    "    \n",
    "    if reduce_factor > 1:\n",
    "        img = cv2.resize(\n",
    "            img,\n",
    "            (img.shape[1] // reduce_factor, img.shape[0] // reduce_factor),\n",
    "            interpolation=cv2.INTER_AREA,\n",
    "        )\n",
    "        \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "approved-stamp",
   "metadata": {},
   "outputs": [],
   "source": [
    "def HE_preprocess(augment=False, visualize=False, mean=MEAN, std=STD):\n",
    "    if visualize:\n",
    "        normalizer = albu.Compose(\n",
    "            [albu.Normalize(mean=[0, 0, 0], std=[1, 1, 1]), ToTensorV2()], p=1\n",
    "        )\n",
    "    else:\n",
    "        normalizer = albu.Compose(\n",
    "            [albu.Normalize(mean=mean, std=std), ToTensorV2()], p=1\n",
    "        )\n",
    "    \n",
    "    if augment:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    return normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "functioning-preserve",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rle_encode_less_memory(img):\n",
    "    pixels = img.T.flatten()\n",
    "    pixels[0] = 0\n",
    "    pixels[-1] = 0\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 2\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "def enc2mask(encs, shape):\n",
    "    print (encs)\n",
    "    img = np.zeros(shape[0] * shape[1], dtype=np.uint8)\n",
    "    for m, enc in enumerate(encs):\n",
    "        if isinstance(enc, np.float) and np.isnan(enc):\n",
    "            continue\n",
    "        enc_split = enc.split()\n",
    "        for i in range(len(enc_split) // 2):\n",
    "            start = int(enc_split[2 * i]) - 1\n",
    "            length = int(enc_split[2 * i + 1])\n",
    "            img[start: start + length] = 1 + m\n",
    "    return img.reshape(shape).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ultimate-despite",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        original_img_path,\n",
    "        rle=None,\n",
    "        overlap_factor=1,\n",
    "        tile_size=256,\n",
    "        reduce_factor=4,\n",
    "        transforms=None,\n",
    "    ):\n",
    "        self.original_img = load_image(original_img_path, full_size=reduce_factor > 1)\n",
    "        self.orig_size = self.original_img.shape\n",
    "        self.raw_tile_size = tile_size\n",
    "        self.reduce_factor = reduce_factor\n",
    "        self.tile_size = tile_size * reduce_factor\n",
    "        self.overlap_factor = overlap_factor\n",
    "        self.positions = self.get_positions()\n",
    "        self.transforms = transforms\n",
    "\n",
    "        if rle is not None:\n",
    "            self.mask = enc2mask(rle, (self.orig_size[1], self.orig_size[0])) > 0\n",
    "        else:\n",
    "            self.mask = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.positions)\n",
    "\n",
    "    def get_positions(self):\n",
    "        top_x = np.arange(\n",
    "            0,\n",
    "            self.orig_size[0],  # +self.tile_size,\n",
    "            int(self.tile_size / self.overlap_factor),\n",
    "        )\n",
    "        top_y = np.arange(\n",
    "            0,\n",
    "            self.orig_size[1],  # +self.tile_size,\n",
    "            int(self.tile_size / self.overlap_factor),\n",
    "        )\n",
    "        starting_positions = []\n",
    "        for x in top_x:\n",
    "            right_space = self.orig_size[0] - (x + self.tile_size)\n",
    "            if right_space > 0:\n",
    "                boundaries_x = (x, x + self.tile_size)\n",
    "            else:\n",
    "                boundaries_x = (x + right_space, x + right_space + self.tile_size)\n",
    "\n",
    "            for y in top_y:\n",
    "                down_space = self.orig_size[1] - (y + self.tile_size)\n",
    "                if down_space > 0:\n",
    "                    boundaries_y = (y, y + self.tile_size)\n",
    "                else:\n",
    "                    boundaries_y = (y + down_space, y + down_space + self.tile_size)\n",
    "                starting_positions.append((boundaries_x, boundaries_y))\n",
    "\n",
    "        return starting_positions\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pos_x, pos_y = self.positions[idx]\n",
    "        img = self.original_img[pos_x[0]: pos_x[1], pos_y[0]: pos_y[1], :]\n",
    "\n",
    "        if self.reduce_factor > 1:\n",
    "            img = cv2.resize(\n",
    "                img, (self.raw_tile_size, self.raw_tile_size), interpolation=cv2.INTER_AREA\n",
    "            )\n",
    "\n",
    "        if self.transforms:\n",
    "            img = self.transforms(image=img)[\"image\"]\n",
    "\n",
    "        pos = np.array([pos_x[0], pos_x[1], pos_y[0], pos_y[1]])\n",
    "\n",
    "        return img, pos\n",
    "    \n",
    "class InferenceEfficientDataset(InferenceDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        original_img_path,\n",
    "        rle=None,\n",
    "        overlap_factor=1,\n",
    "        tile_size=256,\n",
    "        reduce_factor=4,\n",
    "        transforms=None,\n",
    "    ):\n",
    "            \n",
    "        self.raw_tile_size = tile_size\n",
    "        self.reduce_factor = reduce_factor\n",
    "        self.tile_size = tile_size * reduce_factor\n",
    "        \n",
    "        self.overlap_factor = overlap_factor\n",
    "        self.transforms = transforms\n",
    "\n",
    "        # Load image with rasterio        \n",
    "        self.original_img = rasterio.open(original_img_path, transform=IDENTITY, num_threads='all_cpus')\n",
    "        if self.original_img.count != 3:\n",
    "            self.layers = [rasterio.open(subd) for subd in self.original_img.subdatasets]\n",
    "                    \n",
    "        self.orig_size = self.original_img.shape\n",
    "\n",
    "        self.positions = self.get_positions()\n",
    "        \n",
    "        if rle is not None:\n",
    "            self.mask = enc2mask(rle, (self.orig_size[1], self.orig_size[0])) > 0\n",
    "        else:\n",
    "            self.mask = None\n",
    "\n",
    "    def __getitem__(self, idx):        \n",
    "        # Window\n",
    "        pos_x, pos_y = self.positions[idx]\n",
    "        x1, x2 = pos_x[0], pos_x[1]\n",
    "        y1, y2 = pos_y[0], pos_y[1]\n",
    "        window = Window.from_slices((x1, x2), (y1, y2))\n",
    "\n",
    "        # Retrieve slice\n",
    "        if self.original_img.count == 3:  # normal\n",
    "            img = self.original_img.read([1, 2, 3], window=window)\n",
    "            img = np.moveaxis(img, 0, -1)\n",
    "        else:  # with subdatasets/layers\n",
    "            img = np.zeros((self.tile_size, self.tile_size, 3), dtype=np.uint8)\n",
    "            for fl in range(3):\n",
    "                img[:, :, fl] = self.layers[fl].read(window=window) \n",
    "\n",
    "        # Downscale to tile size\n",
    "        img = cv2.resize(\n",
    "            img, (self.raw_tile_size, self.raw_tile_size), interpolation=cv2.INTER_AREA\n",
    "        )\n",
    "        img = self.transforms(image=img)[\"image\"]\n",
    "        \n",
    "        pos = np.array([pos_x[0], pos_x[1], pos_y[0], pos_y[1]])\n",
    "\n",
    "        return img, pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "rocky-guatemala",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_weights(model, filename, verbose=1, cp_folder=\"\"):\n",
    "    if verbose:\n",
    "        print(f\"\\n -> Loading weights from {os.path.join(cp_folder,filename)}\\n\")\n",
    "    try:\n",
    "        model.load_state_dict(os.path.join(cp_folder, filename), strict=True)\n",
    "    except BaseException:\n",
    "        model.load_state_dict(\n",
    "            torch.load(os.path.join(cp_folder, filename), map_location=\"cpu\"),\n",
    "            strict=True,\n",
    "        )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "educational-situation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch\n",
    "from segmentation_models_pytorch.encoders import encoders\n",
    "\n",
    "\n",
    "DECODERS = [\"Unet\", \"Linknet\", \"FPN\", \"PSPNet\", \"DeepLabV3\", \"DeepLabV3Plus\", \"PAN\"]\n",
    "ENCODERS = list(encoders.keys())\n",
    "\n",
    "\n",
    "def define_model(\n",
    "    decoder_name, encoder_name, num_classes=1, activation=None, encoder_weights=\"imagenet\"\n",
    "):\n",
    "    assert decoder_name in DECODERS, \"Decoder name not supported\"\n",
    "    assert encoder_name in ENCODERS, \"Encoder name not supported\"\n",
    "\n",
    "    decoder = getattr(segmentation_models_pytorch, decoder_name)\n",
    "\n",
    "    model = decoder(\n",
    "        encoder_name,\n",
    "        encoder_weights=encoder_weights,\n",
    "        classes=num_classes,\n",
    "        activation=activation,\n",
    "    )\n",
    "    model.num_classes = num_classes\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "electoral-insured",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self, **entries):\n",
    "        self.__dict__.update(entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "introductory-confirmation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(cp_folder):\n",
    "    config = json.load(open(cp_folder + 'config.json', 'r'))\n",
    "    config = Config(**config)\n",
    "    \n",
    "    weights = sorted(glob.glob(cp_folder + \"*.pt\"))\n",
    "    models = []\n",
    "    \n",
    "    for weight in weights:\n",
    "        model = define_model(\n",
    "            config.decoder,\n",
    "            config.encoder,\n",
    "            num_classes=config.num_classes,\n",
    "            encoder_weights=None,\n",
    "        )\n",
    "        \n",
    "        model = load_model_weights(model, weight).to(DEVICE)\n",
    "        model.eval()\n",
    "        models.append(model)\n",
    "        \n",
    "#         break\n",
    "        \n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "rental-circulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_scores_img(pred, truth, eps=1e-8):\n",
    "    pred = pred.reshape(-1) > 0\n",
    "    truth = truth.reshape(-1) > 0\n",
    "    intersect = (pred & truth).sum(-1)\n",
    "    union = pred.sum(-1) + truth.sum(-1)\n",
    "\n",
    "    dice = (2.0 * intersect + eps) / (union + eps)\n",
    "    return dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "threatened-courtesy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tile_weighting(size, sigma=1, alpha=1, eps=1e-6):\n",
    "    half = size // 2\n",
    "    w = np.ones((size, size), np.float32)\n",
    "\n",
    "    x = np.concatenate([np.mgrid[-half:0], np.mgrid[1: half + 1]])[:, None]\n",
    "    x = np.tile(x, (1, size))\n",
    "    x = half + 1 - np.abs(x)\n",
    "    y = x.T\n",
    "\n",
    "    w = np.minimum(x, y)\n",
    "    w = (w / w.max()) ** sigma\n",
    "    w = np.minimum(w, 1)\n",
    "\n",
    "    w = (w - np.min(w) + eps) / (np.max(w) - np.min(w) + eps)\n",
    "\n",
    "    w = np.where(w > alpha, 1, w)\n",
    "    w = w / alpha\n",
    "    w = np.clip(w, 1e-3, 1)\n",
    "\n",
    "    w = np.round(w, 3)\n",
    "    return w.astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "perfect-exception",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_entire_mask(dataset, models, batch_size=32, tta=False):\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "    weighting = torch.from_numpy(get_tile_weighting(dataset.tile_size, sigma=1, alpha=1))\n",
    "    weighting_cuda = weighting.clone().cuda().unsqueeze(0)\n",
    "    weighting = weighting.cuda().half()\n",
    "\n",
    "    global_pred = torch.zeros(\n",
    "        (dataset.orig_size[0], dataset.orig_size[1]),\n",
    "        dtype=torch.half, device=\"cuda\"\n",
    "    )\n",
    "    global_counter = torch.zeros(\n",
    "        (dataset.orig_size[0], dataset.orig_size[1]),\n",
    "        dtype=torch.half, device=\"cuda\"\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img, pos in tqdm(loader):\n",
    "            img = img.to(\"cuda\")\n",
    "            _, _, h, w = img.shape\n",
    "            \n",
    "            model_preds = []\n",
    "            for model in models:\n",
    "                if model.num_classes == 1:\n",
    "                    pred = model(img).view(1, -1, h, w).sigmoid().detach()\n",
    "                else:\n",
    "                    pred = model(img)[:, 0].view(1, -1, h, w).sigmoid().detach()\n",
    "\n",
    "                if tta:\n",
    "                    for f in FLIPS:\n",
    "                        pred_flip = model(torch.flip(img, f))\n",
    "                        if model.num_classes == 2:\n",
    "                            pred_flip = pred_flip[:, :1]\n",
    "\n",
    "                        pred_flip = torch.flip(pred_flip, f).view(1, -1, h, w).sigmoid().detach()\n",
    "                        pred += pred_flip\n",
    "                    pred = torch.div(pred, len(FLIPS) + 1)\n",
    "\n",
    "                model_preds.append(pred)\n",
    "\n",
    "            pred = torch.cat(model_preds, 0).mean(0)\n",
    "\n",
    "            pred = torch.nn.functional.interpolate(\n",
    "                pred.unsqueeze(1), (dataset.tile_size, dataset.tile_size), mode='area'\n",
    "            ).squeeze(1)\n",
    "            \n",
    "            pred = (pred * weighting_cuda).half()\n",
    "\n",
    "            for tile_idx, (x0, x1, y0, y1) in enumerate(pos):\n",
    "                global_pred[x0: x1, y0: y1] += pred[tile_idx]\n",
    "                global_counter[x0: x1, y0: y1] += weighting\n",
    "\n",
    "    for i in range(len(global_pred)):\n",
    "        global_pred[i] = torch.div(global_pred[i], global_counter[i])\n",
    "\n",
    "    return global_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "determined-delaware",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perf_metrics(gt, pred):\n",
    "    n = 0\n",
    "    d = 0\n",
    "    for i in range(gt.shape[0]):\n",
    "        for j in range (gt.shape[1]):\n",
    "            if (gt[i][j]==pred[i][j]):\n",
    "                n = n+1\n",
    "            d = d+1\n",
    "    \n",
    "    return n/d, jaccard_score(gt.flatten(order='C'), pred.flatten(order='C')), directed_hausdorff(gt, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "homeless-aviation",
   "metadata": {},
   "source": [
    "###### Step 3 - Predict colon rle and calculate performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "pending-fever",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = [\"ftus\"]\n",
    "\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "\n",
    "MEAN = np.array([0.66437738, 0.50478148, 0.70114894])\n",
    "STD = np.array([0.15825711, 0.24371008, 0.13832686])\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "IDENTITY = rasterio.Affine(1, 0, 0, 0, 1, 0)\n",
    "FLIPS = [[-1], [-2], [-2, -1]]\n",
    "\n",
    "DATA_PATH = r'C:/Users/soodn/Downloads/Naveksha/Kaggle HuBMAP/'\n",
    "IMG_PATH = DATA_PATH+'Data/hubmap_colon_data/jpg images/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "sticky-howard",
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0.005\n",
    "\n",
    "USE_TTA = True # not DEBUG\n",
    "OVERLAP_FACTOR = 1.5\n",
    "\n",
    "CP_FOLDERS = [\n",
    "    DATA_PATH + 'Scripts/4. DeepLive/hubmap-cp/b1_2cfix/b1_2cfix/',\n",
    "    DATA_PATH + 'Scripts/4. DeepLive/hubmap-cp/b1_last/b1_last/'\n",
    "]\n",
    "CP_FOLDER = CP_FOLDERS[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "scientific-essex",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH + 'Data/hubmap_colon_data/sample_submission_colon.csv')\n",
    "df_info = pd.read_csv(DATA_PATH + 'Data/hubmap-kidney-segmentation-data/HuBMAP-20-dataset_information.csv')\n",
    "rles = pd.read_csv(DATA_PATH + 'Data/hubmap_colon_data/colon_rle.csv')\n",
    "\n",
    "config = json.load(open(CP_FOLDER + 'config.json', 'r'))\n",
    "config = Config(**config)\n",
    "config.overlap_factor = OVERLAP_FACTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "democratic-grounds",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_test = r'C:\\Users\\soodn\\Downloads\\Naveksha\\Kaggle HuBMAP\\Data\\hubmap-kidney-segmentation-data\\test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "physical-orbit",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -> Loading weights from C:/Users/soodn/Downloads/Naveksha/Kaggle HuBMAP/Scripts/4. DeepLive/hubmap-cp/b1_2cfix/b1_2cfix\\Unet_efficientnet-b1_0.pt\n",
      "\n",
      "\n",
      " -> Loading weights from C:/Users/soodn/Downloads/Naveksha/Kaggle HuBMAP/Scripts/4. DeepLive/hubmap-cp/b1_2cfix/b1_2cfix\\Unet_efficientnet-b1_1.pt\n",
      "\n",
      "\n",
      " -> Loading weights from C:/Users/soodn/Downloads/Naveksha/Kaggle HuBMAP/Scripts/4. DeepLive/hubmap-cp/b1_2cfix/b1_2cfix\\Unet_efficientnet-b1_2.pt\n",
      "\n",
      "\n",
      " -> Loading weights from C:/Users/soodn/Downloads/Naveksha/Kaggle HuBMAP/Scripts/4. DeepLive/hubmap-cp/b1_2cfix/b1_2cfix\\Unet_efficientnet-b1_3.pt\n",
      "\n",
      "\n",
      " -> Loading weights from C:/Users/soodn/Downloads/Naveksha/Kaggle HuBMAP/Scripts/4. DeepLive/hubmap-cp/b1_2cfix/b1_2cfix\\Unet_efficientnet-b1_4.pt\n",
      "\n",
      "\n",
      " -> Loading weights from C:/Users/soodn/Downloads/Naveksha/Kaggle HuBMAP/Scripts/4. DeepLive/hubmap-cp/b1_last/b1_last\\Unet_efficientnet-b1_0.pt\n",
      "\n",
      "\n",
      " -> Loading weights from C:/Users/soodn/Downloads/Naveksha/Kaggle HuBMAP/Scripts/4. DeepLive/hubmap-cp/b1_last/b1_last\\Unet_efficientnet-b1_1.pt\n",
      "\n",
      "\n",
      " -> Loading weights from C:/Users/soodn/Downloads/Naveksha/Kaggle HuBMAP/Scripts/4. DeepLive/hubmap-cp/b1_last/b1_last\\Unet_efficientnet-b1_2.pt\n",
      "\n",
      "\n",
      " -> Loading weights from C:/Users/soodn/Downloads/Naveksha/Kaggle HuBMAP/Scripts/4. DeepLive/hubmap-cp/b1_last/b1_last\\Unet_efficientnet-b1_3.pt\n",
      "\n",
      "\n",
      " -> Loading weights from C:/Users/soodn/Downloads/Naveksha/Kaggle HuBMAP/Scripts/4. DeepLive/hubmap-cp/b1_last/b1_last\\Unet_efficientnet-b1_4.pt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "\n",
    "for cp_folder in CP_FOLDERS:\n",
    "    models += load_models(cp_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "neither-bronze",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t Image CL_HandE_1234_B004.jpg_bottom_left\n",
      "\n",
      " - Building dataset\n",
      "\n",
      " - Predicting masks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soodn\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\rasterio\\__init__.py:219: NotGeoreferencedWarning: Dataset has no geotransform set. The identity matrix may be returned.\n",
      "  s = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abd27e55c17b47c3a72870adc836fe64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " - Encoding\n",
      "2    398717 19 403251 23 407786 25 412318 30 416852...\n",
      "Name: encodings, dtype: object\n",
      " -> Scored 0.0071 with threshold 0.01, and Pixel Accuracy 0.8783, and Jaccard Index 0.0036, and Hausdroff's (17.378147196982766, 4535, 2530)\n",
      "\n",
      "\t Image CL_HandE_1234_B004.jpg_bottom_right\n",
      "\n",
      " - Building dataset\n",
      "\n",
      " - Predicting masks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soodn\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\rasterio\\__init__.py:219: NotGeoreferencedWarning: Dataset has no geotransform set. The identity matrix may be returned.\n",
      "  s = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec1b4cd608414bc28e68a2aa4ef24409",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " - Encoding\n",
      "3    13734728 36 13739260 42 13743792 47 13748326 5...\n",
      "Name: encodings, dtype: object\n",
      " -> Scored 0.0000 with threshold 0.01, and Pixel Accuracy 0.9269, and Jaccard Index 0.0000, and Hausdroff's (18.2208671582886, 4535, 134)\n",
      "\n",
      "\t Image CL_HandE_1234_B004.jpg_top_left\n",
      "\n",
      " - Building dataset\n",
      "\n",
      " - Predicting masks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soodn\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\rasterio\\__init__.py:219: NotGeoreferencedWarning: Dataset has no geotransform set. The identity matrix may be returned.\n",
      "  s = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3757673912ee492cbc0edbff4bb2f4a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " - Encoding\n",
      "0    2023883 36 2028417 40 2032951 44 2037485 47 20...\n",
      "Name: encodings, dtype: object\n",
      " -> Scored 0.0051 with threshold 0.01, and Pixel Accuracy 0.9097, and Jaccard Index 0.0025, and Hausdroff's (14.7648230602334, 4535, 3409)\n",
      "\n",
      "\t Image CL_HandE_1234_B004.jpg_top_right\n",
      "\n",
      " - Building dataset\n",
      "\n",
      " - Predicting masks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soodn\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\rasterio\\__init__.py:219: NotGeoreferencedWarning: Dataset has no geotransform set. The identity matrix may be returned.\n",
      "  s = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c578df9cc03245bab52003d7731af816",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " - Encoding\n",
      "1    411 8 784 1 977 4 1555 7 1769 3 2321 12 2524 5...\n",
      "Name: encodings, dtype: object\n",
      " -> Scored 0.0000 with threshold 0.01, and Pixel Accuracy 0.9531, and Jaccard Index 0.0000, and Hausdroff's (19.849433241279208, 0, 1573)\n",
      "\n",
      "\t Image HandE_B005_CL_b_RGB.jpg_bottom_left\n",
      "\n",
      " - Building dataset\n",
      "\n",
      " - Predicting masks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soodn\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\rasterio\\__init__.py:219: NotGeoreferencedWarning: Dataset has no geotransform set. The identity matrix may be returned.\n",
      "  s = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29eb250898fa46a8a632f90a245e8fa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " - Encoding\n",
      "6    13716546 23 13721078 29 13725612 32 13730146 3...\n",
      "Name: encodings, dtype: object\n",
      " -> Scored 0.0034 with threshold 0.01, and Pixel Accuracy 0.9731, and Jaccard Index 0.0017, and Hausdroff's (9.486832980505138, 0, 1603)\n",
      "\n",
      "\t Image HandE_B005_CL_b_RGB.jpg_bottom_right\n",
      "\n",
      " - Building dataset\n",
      "\n",
      " - Predicting masks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soodn\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\rasterio\\__init__.py:219: NotGeoreferencedWarning: Dataset has no geotransform set. The identity matrix may be returned.\n",
      "  s = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "914c09451ffc4c91b727135b3cd66233",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " - Encoding\n",
      "7    151 4 227 8 285 1 1352 12 1592 76 4681 10 4748...\n",
      "Name: encodings, dtype: object\n",
      " -> Scored 0.0000 with threshold 0.01, and Pixel Accuracy 0.9700, and Jaccard Index 0.0000, and Hausdroff's (17.944358444926362, 0, 4298)\n",
      "\n",
      "\t Image HandE_B005_CL_b_RGB.jpg_top_left\n",
      "\n",
      " - Building dataset\n",
      "\n",
      " - Predicting masks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soodn\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\rasterio\\__init__.py:219: NotGeoreferencedWarning: Dataset has no geotransform set. The identity matrix may be returned.\n",
      "  s = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47ec0f2c68b440729d8835e8d7075560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " - Encoding\n",
      "4    366046 27 370576 43 375109 48 379644 52 384179...\n",
      "Name: encodings, dtype: object\n",
      " -> Scored 0.0079 with threshold 0.01, and Pixel Accuracy 0.9612, and Jaccard Index 0.0039, and Hausdroff's (17.916472867168917, 4535, 2894)\n",
      "\n",
      "\t Image HandE_B005_CL_b_RGB.jpg_top_right\n",
      "\n",
      " - Building dataset\n",
      "\n",
      " - Predicting masks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soodn\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\rasterio\\__init__.py:219: NotGeoreferencedWarning: Dataset has no geotransform set. The identity matrix may be returned.\n",
      "  s = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef19c118af264d3ea6e29e0fcd0de33c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " - Encoding\n",
      "5    NaN\n",
      "Name: encodings, dtype: object\n",
      " -> Scored 0.0000 with threshold 0.01, and Pixel Accuracy 0.9999, and Jaccard Index 0.0000, and Hausdroff's (7.615773105863909, 0, 2530)\n",
      "0.0234533185276735 7.572158418592305 0.011764230616815908 123.17670805524831\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import jaccard_score\n",
    "from scipy.spatial.distance import directed_hausdorff\n",
    "\n",
    "DEBUG = False\n",
    "sum_score = 0\n",
    "sum_pa = 0\n",
    "sum_ji = 0\n",
    "sum_haus = 0\n",
    "for img in df['id'].unique():\n",
    "    if DEBUG:  # Check performances on a validation image\n",
    "        img = \"2f6ecfcdf\"  # check repro\n",
    "#         img = \"4ef6695ce\" # biggest img\n",
    "        IMG_PATH = DATA_PATH + \"train\"\n",
    "        models = [models[0], models[5]]\n",
    "                  \n",
    "    \n",
    "    print(f'\\n\\t Image {img}')\n",
    "    \n",
    "    if img == \"d488c759a\":\n",
    "        print('\\n - Using precomputed rle')\n",
    "        local_file_fc = '../input/hubmap-fast-submission/submission_0933_fc.csv'\n",
    "        df_local_fc = pd.read_csv(local_file_fc, index_col='id')\n",
    "        rle = df_local_fc.loc['d488c759a', 'predicted']\n",
    "        df.loc[df.id == img, 'predicted'] = rle\n",
    "\n",
    "        continue\n",
    "    \n",
    "    print(f'\\n - Building dataset')\n",
    "    \n",
    "    rle_truth = rles[rles['id'] == img][\"encoding\"] if DEBUG else None\n",
    "    \n",
    "    predict_dataset = InferenceEfficientDataset(\n",
    "        f\"{IMG_PATH}/{img}.tif\",\n",
    "        rle=rle_truth,\n",
    "        overlap_factor=config.overlap_factor,\n",
    "        reduce_factor=config.reduce_factor,\n",
    "        tile_size=config.tile_size,\n",
    "        transforms=HE_preprocess(augment=False, visualize=False),\n",
    "    )\n",
    "    \n",
    "    print(f'\\n - Predicting masks')\n",
    "\n",
    "    global_pred = predict_entire_mask(\n",
    "        predict_dataset, models, batch_size=config.val_bs, tta=USE_TTA\n",
    "    )\n",
    "    \n",
    "    del predict_dataset\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    print('\\n - Encoding')\n",
    "    \n",
    "    global_pred_np = np.zeros(global_pred.size(), dtype=np.uint8)\n",
    "\n",
    "    for i in range(global_pred_np.shape[0]):\n",
    "        global_pred_np[i] = (global_pred[i] > THRESHOLD).cpu().numpy().astype(np.uint8)\n",
    "        \n",
    "    ########## Performance Metrics ###########    \n",
    "    \n",
    "    colon_shape = (4704, 4536)\n",
    "    shape = colon_shape\n",
    "    rle_truth = rles[rles['id'] == img][\"encodings\"]\n",
    "    mask_truth = enc2mask(rle_truth, shape)\n",
    "    score = dice_scores_img(global_pred_np, mask_truth)\n",
    "    pa, ji, haus = perf_metrics(global_pred_np, mask_truth)\n",
    "    sum_score += score\n",
    "    sum_pa += pa\n",
    "    sum_ji += ji\n",
    "    sum_haus += haus[0]\n",
    "    print(f\" -> Scored {score :.4f} with threshold {THRESHOLD:.2f}, and Pixel Accuracy {pa :.4f}, and Jaccard Index {ji :.4f}, and Hausdroff's {haus}\")\n",
    "    \n",
    "    ##########################################\n",
    "    \n",
    "    rle = rle_encode_less_memory(global_pred_np)\n",
    "    df.loc[df.id == img, 'predicted'] = rle\n",
    "    \n",
    "    if DEBUG:\n",
    "        shape = df_info[df_info.image_file == img + \".tiff\"][['width_pixels', 'height_pixels']].values.astype(int)[0]\n",
    "        mask_truth = enc2mask(rle_truth, shape)\n",
    "        score = dice_scores_img(global_pred_np, mask_truth)\n",
    "        print(f\" -> Scored {score :.4f} with threshold {THRESHOLD:.2f}\")\n",
    "        break\n",
    "        \n",
    "    del global_pred, global_pred_np\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "print (sum_score, sum_pa, sum_ji, sum_haus)\n",
    "df.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
