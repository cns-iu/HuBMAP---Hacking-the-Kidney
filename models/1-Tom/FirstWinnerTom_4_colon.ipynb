{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bl7GrtwBzGiJ",
    "papermill": {
     "duration": 0.017453,
     "end_time": "2021-04-12T09:41:30.555048",
     "exception": false,
     "start_time": "2021-04-12T09:41:30.537595",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "  --- updated data, old models ---  \n",
    "* v01(old v53) : 10_01 with fold0,1,2,3, LB=(old 0.874)(th=0.5, pad=256, tta2) -> Timeout Error  \n",
    "* v02(old v53) : rasterIO, 10_01 with fold0, LB=(old ?)(th=0.5, pad=256, tta2) -> Exceeded Error  \n",
    "* v03(old v53) : rasterIO, 10_01 with fold0,1,2,3, LB=(old 0.874)(th=0.5, pad=256, tta2)  \n",
    "* v04 : rasterIO, 10_01 with fold0, LB= (th=0.5, pad=64, no tta)  \n",
    "* v05 : debug  \n",
    "* v06 : re-upload the weights, rasterIO, 10_01 with fold0, LB= (th=0.5, pad=64, no tta)  \n",
    "* v09 : debug, sample submission -> OK  \n",
    "* v10 : rasterIO, 10_01 with fold0, LB= (th=0.5, pad=0, no tta)  \n",
    "* v11 : bug-corrected, rasterIO, 10_01 with fold0, LB= (th=0.5, pad=0, no tta)  \n",
    "  \n",
    "  \n",
    "  --- OK ---  \n",
    "* v14 : better approach, rasterIO, 10_01 with fold0, LB=0.908 (th=0.5, input_sz=256, pad=0, no tta)  \n",
    "* v18 : better approach, rasterIO, 10_01 with fold0123, LB= (th=0.5, input_sz=320, pad=0, tta2) <-- GPU OOM  \n",
    "* v17 : better approach, rasterIO, 10_01 with fold0123456, LB=0.920 (th=0.5, input_sz=320, pad=0, tta4)  \n",
    "* v21 : better approach, rasterIO, 10_01 with fold0123, LB=0.924 (th=0.5, input_sz=320, pad=64, tta2)  \n",
    "* v22 : better approach, rasterIO, 10_01 with fold0123, LB=0.923 (th=0.5, input_sz=320, pad=128, tta2)  \n",
    "* v23 : better approach, rasterIO, 10_01 with fold0123, LB=0.924 (th=0.5, input_sz=320, pad=256, tta2)  \n",
    "* v24 : better approach, rasterIO, 10_01 with fold0123456, LB=0.923 (th=0.5, input_sz=320, pad=256, tta2)  \n",
    "* v25 : better approach, rasterIO, 10_01 with fold0123, LB= (th=0.5, input_sz=320, pad=256, tta4) -> TLO  \n",
    "* v26 : better approach, rasterIO, 10_01 with fold0123, LB=0.925 (th=0.5, input_sz=320, pad=256, tta3)  \n",
    "  \n",
    "  \n",
    "  --- trained with new data ---  \n",
    "* v28 : new_01_01 : baseline with new kaggle dataset, unet seresnext101, more hue-sat(0.3), classification head, deepsupervision, 320x320, coarse dropout corrected, 1024x1024 tiling, 5folds, data balance with (00_01, 00_02), group by patient number, balanced sampling, aug(), cosine_annealing(1e-4to1e-6, 20epochs), CV=0.9354, LB=0.917(th=0.5, input_sz=320, pad=256, tta2)  \n",
    "* v29 : new_01_01 : baseline with new kaggle dataset, unet seresnext101, more hue-sat(0.3), classification head, deepsupervision, 320x320, coarse dropout corrected, 1024x1024 tiling, 5folds, data balance with (00_01, 00_02), group by patient number, balanced sampling, aug(), cosine_annealing(1e-4to1e-6, 20epochs), CV=0.9354, LB=0.921(th=0.5, input_sz=320, pad=64, tta4)  \n",
    "* v30 : 01_02 : manually specified val_patient_numbers (4folds), baseline with new kaggle dataset, unet seresnext101, more hue-sat(0.3), classification head, deepsupervision, 320x320, coarse dropout corrected, 1024x1024 tiling, data balance with (00_02, 00_04), group by patient number, balanced sampling, aug(), cosine_annealing(1e-4to1e-6, 20epochs), CV=0.9331, LB=0.921(th=0.5, input_sz=320, pad=256, tta3)  \n",
    "* v31 : 01_04 : unet seresnext50, manually specified val_patient_numbers (4folds), baseline with new kaggle dataset, more hue-sat(0.3), classification head, deepsupervision, 320x320, coarse dropout corrected, 1024x1024 tiling, data balance with (00_02, 00_04), group by patient number, balanced sampling, aug(), cosine_annealing(1e-4to1e-6, 20epochs), CV=0.9327, LB=0.921(th=0.5, input_sz=320, pad=256, tta3)  \n",
    "* v33 : 03_01 : pseudo-label for train+test(00_05,00_06), manually specified val_patient_numbers (4folds), baseline with new kaggle dataset, unet seresnext101, more hue-sat(0.3), classification head, deepsupervision, 320x320, coarse dropout corrected, 1024x1024 tiling, data balance, group by patient number, balanced sampling, aug(), cosine_annealing(1e-4to1e-6, 20epochs), CV=0.9779, LB=0.921(th=0.5, input_sz=320, pad=256, tta3)  \n",
    "* v35 : v30(LB0.921), create sub for LB probing  \n",
    "* v36 : v26(LB0.925), create sub for LB probing  \n",
    "* v37 : v33(LB0.921), create sub for LB probing  \n",
    "* v38 : 01_07 : focal loss, manually specified val_patient_numbers (4folds), baseline with new kaggle dataset, unet seresnext101, more hue-sat(0.3), classification head, deepsupervision, 320x320, coarse dropout corrected, 1024x1024 tiling, data balance with (00_02, 00_04), group by patient number, balanced sampling, aug(), cosine_annealing(1e-4to1e-6, 20epochs), CV=0.9326, LB=0.921(th=0.5, input_sz=320, pad=256, tta3)  \n",
    "* v39 : v38(LB=), create sub for LB probing  \n",
    "* v41 : 03_02 : pseudo-label (test(00_05,00_06), external(00,10,00_11), dataset_a_dib(00_08,00_09)), manually specified val_patient_numbers (4folds), baseline with new kaggle dataset, unet seresnext101, more hue-sat(0.3), classification head, deepsupervision, 320x320, coarse dropout corrected, 1024x1024 tiling, data balance, group by patient number, balanced sampling, aug(), cosine_annealing(1e-4to1e-6, 20epochs), CV=0.9385  (fold2 looks not good, should be removed ?), LB=0.922(th=0.5, input_sz=320, pad=256, tta3)  \n",
    "* v42 : fold013 of 03_02 : pseudo-label (test(00_05,00_06), external(00,10,00_11), dataset_a_dib(00_08,00_09)), manually specified val_patient_numbers (4folds), baseline with new kaggle dataset, unet seresnext101, more hue-sat(0.3), classification head, deepsupervision, 320x320, coarse dropout corrected, 1024x1024 tiling, data balance, group by patient number, balanced sampling, aug(), cosine_annealing(1e-4to1e-6, 20epochs), CV=0.9385  (fold2 looks not good, should be removed ?), LB=0.922(th=0.5, input_sz=320, pad=256, tta4)  \n",
    "* v43 : 03_03 : add Carno Chao's hand-labeling data (d488c759a), pseudo-label (test(00_05,00_06), external(00,10,00_11), dataset_a_dib(00_08,00_09)), manually specified val_patient_numbers (4folds), baseline with new kaggle dataset, unet seresnext101, more hue-sat(0.3), classification head, deepsupervision, 320x320, coarse dropout corrected, 1024x1024 tiling, data balance, group by patient number, balanced sampling, aug(), cosine_annealing(1e-4to1e-6, 20epochs), CV=0.9375* v41 : 03_02 : pseudo-label (test(00_05,00_06), external(00,10,00_11), dataset_a_dib(00_08,00_09)), manually specified val_patient_numbers (4folds), baseline with new kaggle dataset, unet seresnext101, more hue-sat(0.3), classification head, deepsupervision, 320x320, coarse dropout corrected, 1024x1024 tiling, data balance, group by patient number, balanced sampling, aug(), cosine_annealing(1e-4to1e-6, 20epochs), CV=0.9385  (fold2 looks not good, should be removed ?), LB=(th=0.5, input_sz=320, pad=256, tta3)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kfekwv7LzGiN",
    "papermill": {
     "duration": 0.016685,
     "end_time": "2021-04-12T09:41:30.588975",
     "exception": false,
     "start_time": "2021-04-12T09:41:30.572290",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-12T09:41:30.628788Z",
     "iopub.status.busy": "2021-04-12T09:41:30.627865Z",
     "iopub.status.idle": "2021-04-12T09:41:30.630501Z",
     "shell.execute_reply": "2021-04-12T09:41:30.630978Z"
    },
    "id": "og6DcnU2zGiQ",
    "papermill": {
     "duration": 0.025534,
     "end_time": "2021-04-12T09:41:30.631140",
     "exception": false,
     "start_time": "2021-04-12T09:41:30.605606",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-12T09:41:30.674216Z",
     "iopub.status.busy": "2021-04-12T09:41:30.673601Z",
     "iopub.status.idle": "2021-04-12T09:41:32.616154Z",
     "shell.execute_reply": "2021-04-12T09:41:32.615547Z"
    },
    "id": "vyRK-uifzGiR",
    "papermill": {
     "duration": 1.967484,
     "end_time": "2021-04-12T09:41:32.616276",
     "exception": false,
     "start_time": "2021-04-12T09:41:30.648792",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "config = {\n",
    "    'split_seed_list':[0],\n",
    "    'FOLD_LIST':[0,1,2,3], \n",
    "    'model_path':'train/new_result/02/',\n",
    "    'model_name':'seresnext101',\n",
    "    \n",
    "    'num_classes':1,\n",
    "    'resolution':1024, #(1024,1024),(512,512),\n",
    "    'input_resolution':320, #(320,320), #(256,256), #(512,512), #(384,384)\n",
    "    'deepsupervision':False, # always false for inference\n",
    "    'clfhead':False,\n",
    "    'clf_threshold':0.5,\n",
    "    'small_mask_threshold':0, #256*256*0.03, #512*512*0.03,\n",
    "    'mask_threshold':0.5,\n",
    "    # 'mask_threshold':0.0003,\n",
    "    'pad_size':256, #(64,64), #(256,256), #(128,128)\n",
    "    \n",
    "    'tta':3,\n",
    "    'test_batch_size':12,\n",
    "    \n",
    "    'FP16':False,\n",
    "    'num_workers':4,\n",
    "    'device':torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "}\n",
    "\n",
    "device = config['device']\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFwcJNbazGiS",
    "papermill": {
     "duration": 0.014977,
     "end_time": "2021-04-12T09:41:32.647131",
     "exception": false,
     "start_time": "2021-04-12T09:41:32.632154",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Import Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-04-12T09:41:32.687705Z",
     "iopub.status.busy": "2021-04-12T09:41:32.687126Z",
     "iopub.status.idle": "2021-04-12T09:41:33.092365Z",
     "shell.execute_reply": "2021-04-12T09:41:33.091302Z"
    },
    "id": "u0M2sNDszGiT",
    "papermill": {
     "duration": 0.430263,
     "end_time": "2021-04-12T09:41:33.092482",
     "exception": false,
     "start_time": "2021-04-12T09:41:32.662219",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.get_option(\"display.max_columns\")\n",
    "pd.set_option('display.max_columns', 300)\n",
    "pd.get_option(\"display.max_rows\")\n",
    "pd.set_option('display.max_rows', 300)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from os.path import join as opj\n",
    "import gc\n",
    "\n",
    "import cv2\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "\n",
    "INPUT_PATH = r'C:\\Users\\yiju\\Desktop\\Copy\\Data\\Colon_data_reprocessed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-12T09:41:33.129446Z",
     "iopub.status.busy": "2021-04-12T09:41:33.128533Z",
     "iopub.status.idle": "2021-04-12T09:41:33.133024Z",
     "shell.execute_reply": "2021-04-12T09:41:33.132580Z"
    },
    "id": "j1fe0pKTzGiU",
    "outputId": "0e0b9b41-e6e4-4d8a-c9b2-07c2b3d35fc0",
    "papermill": {
     "duration": 0.024963,
     "end_time": "2021-04-12T09:41:33.133118",
     "exception": false,
     "start_time": "2021-04-12T09:41:33.108155",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python        : 3.7.10 | packaged by conda-forge | (default, Feb 19 2021, 15:37:01) [MSC v.1916 64 bit (AMD64)]\n",
      "Numpy         : 1.21.0\n",
      "Pandas        : 1.2.5\n",
      "Rasterio      : 1.2.6\n",
      "OpenCV        : 4.5.1\n"
     ]
    }
   ],
   "source": [
    "print('Python        : ' + sys.version.split('\\n')[0])\n",
    "print('Numpy         : ' + np.__version__)\n",
    "print('Pandas        : ' + pd.__version__)\n",
    "print('Rasterio      : ' + rasterio.__version__)\n",
    "print('OpenCV        : ' + cv2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-12T09:41:33.172222Z",
     "iopub.status.busy": "2021-04-12T09:41:33.171684Z",
     "iopub.status.idle": "2021-04-12T09:41:33.529811Z",
     "shell.execute_reply": "2021-04-12T09:41:33.528804Z"
    },
    "id": "4JhVePtMzGiV",
    "outputId": "d8531a24-2e10-448b-f227-6e24382f4836",
    "papermill": {
     "duration": 0.381025,
     "end_time": "2021-04-12T09:41:33.529960",
     "exception": false,
     "start_time": "2021-04-12T09:41:33.148935",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df.shape =  (6, 2)\n",
      "sub_df.shape =  (2, 2)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(opj(INPUT_PATH, 'train.csv'))\n",
    "# info_df  = pd.read_csv(opj(INPUT_PATH,'HuBMAP-20-dataset_information.csv'))\n",
    "sub_df = pd.read_csv(opj(INPUT_PATH, 'sample_submission.csv'))\n",
    "\n",
    "print('train_df.shape = ', train_df.shape)\n",
    "# print('info_df.shape  = ', info_df.shape)\n",
    "print('sub_df.shape = ', sub_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-12T09:41:33.568424Z",
     "iopub.status.busy": "2021-04-12T09:41:33.566622Z",
     "iopub.status.idle": "2021-04-12T09:41:33.569064Z",
     "shell.execute_reply": "2021-04-12T09:41:33.569465Z"
    },
    "id": "QqruPHINzGiV",
    "papermill": {
     "duration": 0.023066,
     "end_time": "2021-04-12T09:41:33.569567",
     "exception": false,
     "start_time": "2021-04-12T09:41:33.546501",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#sub_df['predicted'] = '1 1'\n",
    "#sub_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "if len(sub_df) == 5:\n",
    "    if DEBUG:\n",
    "        sub_df = sub_df[:1]\n",
    "    else:\n",
    "        sub_df = sub_df[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FOQXBviKzGiV",
    "papermill": {
     "duration": 0.015704,
     "end_time": "2021-04-12T09:41:33.600839",
     "exception": false,
     "start_time": "2021-04-12T09:41:33.585135",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Utils  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-12T09:41:33.640305Z",
     "iopub.status.busy": "2021-04-12T09:41:33.639575Z",
     "iopub.status.idle": "2021-04-12T09:41:33.645420Z",
     "shell.execute_reply": "2021-04-12T09:41:33.644872Z"
    },
    "id": "dQ8Uu-7BzGiV",
    "papermill": {
     "duration": 0.028975,
     "end_time": "2021-04-12T09:41:33.645507",
     "exception": false,
     "start_time": "2021-04-12T09:41:33.616532",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "def fix_seed(seed):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "def elapsed_time(start_time):\n",
    "    return time.time() - start_time\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "fix_seed(2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-12T09:41:33.690300Z",
     "iopub.status.busy": "2021-04-12T09:41:33.689132Z",
     "iopub.status.idle": "2021-04-12T09:41:33.692048Z",
     "shell.execute_reply": "2021-04-12T09:41:33.691592Z"
    },
    "id": "r1eqfIOHzGiW",
    "papermill": {
     "duration": 0.031053,
     "end_time": "2021-04-12T09:41:33.692136",
     "exception": false,
     "start_time": "2021-04-12T09:41:33.661083",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def rle2mask(rle, shape):\n",
    "    '''\n",
    "    mask_rle: run-length as string formatted (start length)\n",
    "    shape: (height, width) of array to return \n",
    "    Returns numpy array <- 1(mask), 0(background)\n",
    "    '''\n",
    "    s = rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape, order='F')  # Needed to align to RLE direction\n",
    "\n",
    "\n",
    "def mask2rle(img, shape, small_mask_threshold):\n",
    "    '''\n",
    "    Convert mask to rle.\n",
    "    img: numpy array <- 1(mask), 0(background)\n",
    "    Returns run length as string formated\n",
    "    \n",
    "    pixels = np.array([1,1,1,0,0,1,0,1,1]) #-> rle = '1 3 6 1 8 2'\n",
    "    pixels = np.concatenate([[0], pixels, [0]]) #[0,1,1,1,0,0,1,0,1,1,0]\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1 #[ 1  4  6  7  8 10] bit change points\n",
    "    print(runs[1::2]) #[4 7 10]\n",
    "    print(runs[::2]) #[1 6 8]\n",
    "    runs[1::2] -= runs[::2]\n",
    "    print(runs) #[1 3 6 1 8 2]\n",
    "    '''\n",
    "    if img.shape != shape:\n",
    "        h,w = shape\n",
    "        img = cv2.resize(img, dsize=(w,h), interpolation=cv2.INTER_LINEAR)\n",
    "    img = img.astype(np.int8) \n",
    "    pixels = img.T.flatten()\n",
    "    #pixels = np.concatenate([[0], pixels, [0]])\n",
    "    pixels = np.pad(pixels, ((1, 1), ))\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    if runs[1::2].sum() <= small_mask_threshold:\n",
    "        return ''\n",
    "    else:\n",
    "        return ' '.join(str(x) for x in runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yhc-EIBUzGiW",
    "papermill": {
     "duration": 0.015709,
     "end_time": "2021-04-12T09:41:33.723272",
     "exception": false,
     "start_time": "2021-04-12T09:41:33.707563",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-12T09:41:33.784587Z",
     "iopub.status.busy": "2021-04-12T09:41:33.768710Z",
     "iopub.status.idle": "2021-04-12T09:41:35.775129Z",
     "shell.execute_reply": "2021-04-12T09:41:35.773750Z"
    },
    "id": "FXO0_RaOzGiW",
    "papermill": {
     "duration": 2.036145,
     "end_time": "2021-04-12T09:41:35.775241",
     "exception": false,
     "start_time": "2021-04-12T09:41:33.739096",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "package_dir = \"pretrainedmodels/pretrained-models.pytorch-master/\"\n",
    "sys.path.insert(0, package_dir)\n",
    "import pretrainedmodels\n",
    "\n",
    "\n",
    "def conv3x3(in_channel, out_channel): #not change resolusion\n",
    "    return nn.Conv2d(in_channel,out_channel,\n",
    "                      kernel_size=3,stride=1,padding=1,dilation=1,bias=False)\n",
    "\n",
    "def conv1x1(in_channel, out_channel): #not change resolution\n",
    "    return nn.Conv2d(in_channel,out_channel,\n",
    "                      kernel_size=1,stride=1,padding=0,dilation=1,bias=False)\n",
    "\n",
    "def init_weight(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        #nn.init.xavier_uniform_(m.weight, gain=1)\n",
    "        #nn.init.xavier_normal_(m.weight, gain=1)\n",
    "        #nn.init.kaiming_uniform_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "        #nn.init.orthogonal_(m.weight, gain=1)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.zero_()\n",
    "    elif classname.find('Batch') != -1:\n",
    "        m.weight.data.normal_(1,0.02)\n",
    "        m.bias.data.zero_()\n",
    "    elif classname.find('Linear') != -1:\n",
    "        nn.init.orthogonal_(m.weight, gain=1)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.zero_()\n",
    "    elif classname.find('Embedding') != -1:\n",
    "        nn.init.orthogonal_(m.weight, gain=1)\n",
    "\n",
    "        \n",
    "class cSEBlock(nn.Module):\n",
    "    def __init__(self, c, feat):\n",
    "        super().__init__()\n",
    "        self.attention_fc = nn.Linear(feat,1, bias=False)\n",
    "        self.bias         = nn.Parameter(torch.zeros((1,c,1), requires_grad=True))\n",
    "        self.sigmoid      = nn.Sigmoid()\n",
    "        self.dropout      = nn.Dropout2d(0.1)\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        batch,c,h,w = inputs.size()\n",
    "        x = inputs.view(batch,c,-1)\n",
    "        x = self.attention_fc(x) + self.bias\n",
    "        x = x.view(batch,c,1,1)\n",
    "        x = self.sigmoid(x)\n",
    "        x = self.dropout(x)\n",
    "        return inputs * x\n",
    "\n",
    "class sSEBlock(nn.Module):\n",
    "    def __init__(self, c, h, w):\n",
    "        super().__init__()\n",
    "        self.attention_fc = nn.Linear(c,1, bias=False).apply(init_weight)\n",
    "        self.bias         = nn.Parameter(torch.zeros((1,h,w,1), requires_grad=True))\n",
    "        self.sigmoid      = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        batch,c,h,w = inputs.size()\n",
    "        x = torch.transpose(inputs, 1,2) #(*,c,h,w)->(*,h,c,w)\n",
    "        x = torch.transpose(x, 2,3) #(*,h,c,w)->(*,h,w,c)\n",
    "        x = self.attention_fc(x) + self.bias\n",
    "        x = torch.transpose(x, 2,3) #(*,h,w,1)->(*,h,1,w)\n",
    "        x = torch.transpose(x, 1,2) #(*,h,1,w)->(*,1,h,w)\n",
    "        x = self.sigmoid(x)\n",
    "        return inputs * x\n",
    "    \n",
    "class scSEBlock(nn.Module):\n",
    "    def __init__(self, c, h, w):\n",
    "        super().__init__()\n",
    "        self.cSE = cSEBlock(c,h*w)\n",
    "        self.sSE = sSEBlock(c,h,w)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        x1 = self.cSE(inputs)\n",
    "        x2 = self.sSE(inputs)\n",
    "        return x1+x2\n",
    "    \n",
    "    \n",
    "# class SpatialAttention2d(nn.Module):\n",
    "#     def __init__(self, in_channel):\n",
    "#         super().__init__()\n",
    "#         self.squeeze = conv1x1(in_channel,1).apply(init_weight)\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "#     def forward(self, inputs):\n",
    "#         x = self.squeeze(inputs)\n",
    "#         x = self.sigmoid(x)\n",
    "#         return inputs * x\n",
    "    \n",
    "    \n",
    "# class GAB(nn.Module):\n",
    "#     def __init__(self, in_channel, reduction=4):\n",
    "#         super().__init__()\n",
    "#         self.global_avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "#         self.conv1 = conv1x1(in_channel, in_channel//reduction).apply(init_weight)\n",
    "#         self.conv2 = conv1x1(in_channel//reduction, in_channel).apply(init_weight)\n",
    "#         self.relu  = nn.ReLU(True)\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "#     def forward(self, inputs):\n",
    "#         x = self.global_avgpool(inputs)\n",
    "#         x = self.relu(self.conv1(x))\n",
    "#         x = self.sigmoid(self.conv2(x))\n",
    "#         return inputs * x\n",
    "\n",
    "    \n",
    "# class scSEBlock2(nn.Module):\n",
    "#     def __init__(self, in_channel, reduction=4):\n",
    "#         super().__init__()\n",
    "#         self.cSE = GAB(in_channel, reduction)\n",
    "#         self.sSE = SpatialAttention2d(in_channel)\n",
    "    \n",
    "#     def forward(self, inputs):\n",
    "#         x1 = self.cSE(inputs)\n",
    "#         x2 = self.sSE(inputs)\n",
    "#         return x1+x2\n",
    "    \n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.theta    = nn.utils.spectral_norm(conv1x1(channels, channels//8)).apply(init_weight)\n",
    "        self.phi      = nn.utils.spectral_norm(conv1x1(channels, channels//8)).apply(init_weight)\n",
    "        self.g        = nn.utils.spectral_norm(conv1x1(channels, channels//2)).apply(init_weight)\n",
    "        self.o        = nn.utils.spectral_norm(conv1x1(channels//2, channels)).apply(init_weight)\n",
    "        self.gamma    = nn.Parameter(torch.tensor(0.), requires_grad=True)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        batch,c,h,w = inputs.size()\n",
    "        theta = self.theta(inputs) #->(*,c/8,h,w)\n",
    "        phi   = F.max_pool2d(self.phi(inputs), [2,2]) #->(*,c/8,h/2,w/2)\n",
    "        g     = F.max_pool2d(self.g(inputs), [2,2]) #->(*,c/2,h/2,w/2)\n",
    "        \n",
    "        theta = theta.view(batch, self.channels//8, -1) #->(*,c/8,h*w)\n",
    "        phi   = phi.view(batch, self.channels//8, -1) #->(*,c/8,h*w/4)\n",
    "        g     = g.view(batch, self.channels//2, -1) #->(*,c/2,h*w/4)\n",
    "        \n",
    "        beta = F.softmax(torch.bmm(theta.transpose(1,2), phi), -1) #->(*,h*w,h*w/4)\n",
    "        o    = self.o(torch.bmm(g, beta.transpose(1,2)).view(batch,self.channels//2,h,w)) #->(*,c,h,w)\n",
    "        return self.gamma*o + inputs\n",
    "    \n",
    "    \n",
    "\n",
    "class ChannelAttentionModule(nn.Module):\n",
    "    def __init__(self, in_channel, reduction):\n",
    "        super().__init__()\n",
    "        self.global_maxpool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.global_avgpool = nn.AdaptiveAvgPool2d(1) \n",
    "        self.fc = nn.Sequential(\n",
    "            conv1x1(in_channel, in_channel//reduction).apply(init_weight),\n",
    "            nn.ReLU(True),\n",
    "            conv1x1(in_channel//reduction, in_channel).apply(init_weight)\n",
    "        )\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x1 = self.global_maxpool(inputs)\n",
    "        x2 = self.global_avgpool(inputs)\n",
    "        x1 = self.fc(x1)\n",
    "        x2 = self.fc(x2)\n",
    "        x  = torch.sigmoid(x1 + x2)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class SpatialAttentionModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv3x3 = conv3x3(2,1).apply(init_weight)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x1,_ = torch.max(inputs, dim=1, keepdim=True)\n",
    "        x2 = torch.mean(inputs, dim=1, keepdim=True)\n",
    "        x  = torch.cat([x1,x2], dim=1)\n",
    "        x  = self.conv3x3(x)\n",
    "        x  = torch.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, in_channel, reduction):\n",
    "        super().__init__()\n",
    "        self.channel_attention = ChannelAttentionModule(in_channel, reduction)\n",
    "        self.spatial_attention = SpatialAttentionModule()\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = inputs * self.channel_attention(inputs)\n",
    "        x = x * self.spatial_attention(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class CenterBlock(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super().__init__()\n",
    "        self.conv = conv3x3(in_channel, out_channel).apply(init_weight)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.conv(inputs)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DecodeBlock(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, upsample):\n",
    "        super().__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_channel).apply(init_weight)\n",
    "        self.upsample = nn.Sequential()\n",
    "        if upsample:\n",
    "            self.upsample.add_module('upsample',nn.Upsample(scale_factor=2, mode='nearest'))\n",
    "        self.conv3x3_1 = conv3x3(in_channel, in_channel).apply(init_weight)\n",
    "        self.bn2 = nn.BatchNorm2d(in_channel).apply(init_weight)\n",
    "        self.conv3x3_2 = conv3x3(in_channel, out_channel).apply(init_weight)\n",
    "        self.cbam = CBAM(out_channel, reduction=16)\n",
    "        self.conv1x1   = conv1x1(in_channel, out_channel).apply(init_weight)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x  = F.relu(self.bn1(inputs))\n",
    "        x  = self.upsample(x)\n",
    "        x  = self.conv3x3_1(x)\n",
    "        x  = self.conv3x3_2(F.relu(self.bn2(x)))\n",
    "        x  = self.cbam(x)\n",
    "        x += self.conv1x1(self.upsample(inputs)) #shortcut\n",
    "        return x\n",
    "    \n",
    "    \n",
    "#U-Net ResNet34 + CBAM + hypercolumns + deepsupervision\n",
    "class UNET_RESNET34(nn.Module):\n",
    "    def __init__(self, resolution, deepsupervision, clfhead, load_weights=True):\n",
    "        super().__init__()\n",
    "        h,w = resolution\n",
    "        self.deepsupervision = deepsupervision\n",
    "        self.clfhead = clfhead\n",
    "        \n",
    "        #encoder\n",
    "        model_name = 'resnet34' #26M\n",
    "        resnet34 = pretrainedmodels.__dict__['resnet34'](num_classes=1000,pretrained=None)\n",
    "        if load_weights:\n",
    "            resnet34.load_state_dict(torch.load(f'../../../pretrainedmodels_weight/{model_name}.pth'))\n",
    "        self.conv1   = resnet34.conv1 #(*,3,h,w)->(*,64,h/2,w/2)\n",
    "        self.bn1     = resnet34.bn1\n",
    "        self.maxpool = resnet34.maxpool #->(*,64,h/4,w/4)\n",
    "        self.layer1  = resnet34.layer1 #->(*,64,h/4,w/4) \n",
    "        self.layer2  = resnet34.layer2 #->(*,128,h/8,w/8) \n",
    "        self.layer3  = resnet34.layer3 #->(*,256,h/16,w/16) \n",
    "        self.layer4  = resnet34.layer4 #->(*,512,h/32,w/32) \n",
    "        \n",
    "        #center\n",
    "        self.center  = CenterBlock(512,512) #->(*,512,h/32,w/32) \n",
    "        \n",
    "        #decoder\n",
    "        self.decoder4 = DecodeBlock(512+512,64, upsample=True) #->(*,64,h/16,w/16) \n",
    "        self.decoder3 = DecodeBlock(64+256,64, upsample=True) #->(*,64,h/8,w/8) \n",
    "        self.decoder2 = DecodeBlock(64+128,64,  upsample=True) #->(*,64,h/4,w/4) \n",
    "        self.decoder1 = DecodeBlock(64+64,64,   upsample=True) #->(*,64,h/2,w/2)\n",
    "        self.decoder0 = DecodeBlock(64,64, upsample=True) #->(*,64,h,w) \n",
    "        \n",
    "        #upsample\n",
    "        self.upsample4 = nn.Upsample(scale_factor=16, mode='bilinear', align_corners=True)\n",
    "        self.upsample3 = nn.Upsample(scale_factor=8, mode='bilinear', align_corners=True)\n",
    "        self.upsample2 = nn.Upsample(scale_factor=4, mode='bilinear', align_corners=True)\n",
    "        self.upsample1 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        \n",
    "        #deep supervision\n",
    "        self.deep4 = conv1x1(64,1).apply(init_weight)\n",
    "        self.deep3 = conv1x1(64,1).apply(init_weight)\n",
    "        self.deep2 = conv1x1(64,1).apply(init_weight)\n",
    "        self.deep1 = conv1x1(64,1).apply(init_weight)\n",
    "        \n",
    "        #final conv\n",
    "        self.final_conv = nn.Sequential(\n",
    "            conv3x3(320,64).apply(init_weight),\n",
    "            nn.ELU(True),\n",
    "            conv1x1(64,1).apply(init_weight)\n",
    "        )\n",
    "        \n",
    "        #clf head\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.clf = nn.Sequential(\n",
    "            nn.BatchNorm1d(512).apply(init_weight),\n",
    "            nn.Linear(512,512).apply(init_weight),\n",
    "            nn.ELU(True),\n",
    "            nn.BatchNorm1d(512).apply(init_weight),\n",
    "            nn.Linear(512,1).apply(init_weight)\n",
    "        )\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        #encoder\n",
    "        x0 = F.relu(self.bn1(self.conv1(inputs))) #->(*,64,h/2,w/2) \n",
    "        x0 = self.maxpool(x0) #->(*,64,h/4,w/4)\n",
    "        x1 = self.layer1(x0) #->(*,64,h/4,w/4)\n",
    "        x2 = self.layer2(x1) #->(*,128,h/8,w/8)\n",
    "        x3 = self.layer3(x2) #->(*,256,h/16,w/16)\n",
    "        x4 = self.layer4(x3) #->(*,512,h/32,w/32)\n",
    "        \n",
    "        #clf head\n",
    "        logits_clf = self.clf(self.avgpool(x4).squeeze(-1).squeeze(-1)) #->(*,1)\n",
    "        if config['clf_threshold'] is not None:\n",
    "            if (torch.sigmoid(logits_clf)>config['clf_threshold']).sum().item()==0:\n",
    "                bs,_,h,w = inputs.shape\n",
    "                logits = torch.zeros((bs,1,h,w))\n",
    "                if self.clfhead:\n",
    "                    if self.deepsupervision:\n",
    "                        return logits,_,_\n",
    "                    else:\n",
    "                        return logits,_\n",
    "                else:\n",
    "                    if self.deepsupervision:\n",
    "                        return logits,_\n",
    "                    else:\n",
    "                        return logits\n",
    "        \n",
    "        #center\n",
    "        y5 = self.center(x4) #->(*,512,h/32,w/32)\n",
    "        \n",
    "        #decoder\n",
    "        y4 = self.decoder4(torch.cat([x4,y5], dim=1)) #->(*,64,h/16,w/16)\n",
    "        y3 = self.decoder3(torch.cat([x3,y4], dim=1)) #->(*,64,h/8,w/8)\n",
    "        y2 = self.decoder2(torch.cat([x2,y3], dim=1)) #->(*,64,h/4,w/4)\n",
    "        y1 = self.decoder1(torch.cat([x1,y2], dim=1)) #->(*,64,h/2,w/2)\n",
    "        y0 = self.decoder0(y1) #->(*,64,h,w)\n",
    "        \n",
    "        #hypercolumns\n",
    "        y4 = self.upsample4(y4) #->(*,64,h,w)\n",
    "        y3 = self.upsample3(y3) #->(*,64,h,w)\n",
    "        y2 = self.upsample2(y2) #->(*,64,h,w)\n",
    "        y1 = self.upsample1(y1) #->(*,64,h,w)\n",
    "        hypercol = torch.cat([y0,y1,y2,y3,y4], dim=1)\n",
    "        \n",
    "        #final conv\n",
    "        logits = self.final_conv(hypercol) #->(*,1,h,w)\n",
    "        \n",
    "        #clf head\n",
    "        logits_clf = self.clf(self.avgpool(x4).squeeze(-1).squeeze(-1)) #->(*,1)\n",
    "        \n",
    "        if self.clfhead:\n",
    "            if self.deepsupervision:\n",
    "                s4 = self.deep4(y4)\n",
    "                s3 = self.deep3(y3)\n",
    "                s2 = self.deep2(y2)\n",
    "                s1 = self.deep1(y1)\n",
    "                logits_deeps = [s4,s3,s2,s1]\n",
    "                return logits, logits_deeps, logits_clf\n",
    "            else:\n",
    "                return logits, logits_clf\n",
    "        else:\n",
    "            if self.deepsupervision:\n",
    "                s4 = self.deep4(y4)\n",
    "                s3 = self.deep3(y3)\n",
    "                s2 = self.deep2(y2)\n",
    "                s1 = self.deep1(y1)\n",
    "                logits_deeps = [s4,s3,s2,s1]\n",
    "                return logits, logits_deeps\n",
    "            else:\n",
    "                return logits\n",
    "\n",
    "        \n",
    "#U-Net SeResNext50 + CBAM + hypercolumns + deepsupervision\n",
    "class UNET_SERESNEXT50(nn.Module):\n",
    "    def __init__(self, resolution, deepsupervision, clfhead, load_weights=True):\n",
    "        super().__init__()\n",
    "        h,w = resolution\n",
    "        self.deepsupervision = deepsupervision\n",
    "        self.clfhead = clfhead\n",
    "        \n",
    "        #encoder\n",
    "        model_name = 'se_resnext50_32x4d' #26M\n",
    "        seresnext50 = pretrainedmodels.__dict__[model_name](pretrained=None)\n",
    "        if load_weights:\n",
    "            seresnext50.load_state_dict(torch.load(f'../../../pretrainedmodels_weight/{model_name}.pth'))\n",
    "        \n",
    "        self.encoder0 = nn.Sequential(\n",
    "            seresnext50.layer0.conv1, #(*,3,h,w)->(*,64,h/2,w/2)\n",
    "            seresnext50.layer0.bn1,\n",
    "            seresnext50.layer0.relu1,\n",
    "        )\n",
    "        self.encoder1 = nn.Sequential(\n",
    "            seresnext50.layer0.pool, #->(*,64,h/4,w/4)\n",
    "            seresnext50.layer1 #->(*,256,h/4,w/4)\n",
    "        )\n",
    "        self.encoder2 = seresnext50.layer2 #->(*,512,h/8,w/8)\n",
    "        self.encoder3 = seresnext50.layer3 #->(*,1024,h/16,w/16)\n",
    "        self.encoder4 = seresnext50.layer4 #->(*,2048,h/32,w/32)\n",
    "        \n",
    "        #center\n",
    "        self.center  = CenterBlock(2048,512) #->(*,512,h/32,w/32) 10,16\n",
    "        \n",
    "        #decoder\n",
    "        self.decoder4 = DecodeBlock(512+2048,64, upsample=True) #->(*,64,h/16,w/16) 20,32\n",
    "        self.decoder3 = DecodeBlock(64+1024,64, upsample=True) #->(*,64,h/8,w/8) 40,64\n",
    "        self.decoder2 = DecodeBlock(64+512,64,  upsample=True) #->(*,64,h/4,w/4) 80,128\n",
    "        self.decoder1 = DecodeBlock(64+256,64,   upsample=True) #->(*,64,h/2,w/2) 160,256\n",
    "        self.decoder0 = DecodeBlock(64,64, upsample=True) #->(*,64,h,w) 320,512\n",
    "        \n",
    "        #upsample\n",
    "        self.upsample4 = nn.Upsample(scale_factor=16, mode='bilinear', align_corners=True)\n",
    "        self.upsample3 = nn.Upsample(scale_factor=8, mode='bilinear', align_corners=True)\n",
    "        self.upsample2 = nn.Upsample(scale_factor=4, mode='bilinear', align_corners=True)\n",
    "        self.upsample1 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        \n",
    "        #deep supervision\n",
    "        self.deep4 = conv1x1(64,1).apply(init_weight)\n",
    "        self.deep3 = conv1x1(64,1).apply(init_weight)\n",
    "        self.deep2 = conv1x1(64,1).apply(init_weight)\n",
    "        self.deep1 = conv1x1(64,1).apply(init_weight)\n",
    "        \n",
    "        #final conv\n",
    "        self.final_conv = nn.Sequential(\n",
    "            conv3x3(320,64).apply(init_weight),\n",
    "            nn.ELU(True),\n",
    "            conv1x1(64,1).apply(init_weight)\n",
    "        )\n",
    "        \n",
    "        #clf head\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.clf = nn.Sequential(\n",
    "            nn.BatchNorm1d(2048).apply(init_weight),\n",
    "            nn.Linear(2048,512).apply(init_weight),\n",
    "            nn.ELU(True),\n",
    "            nn.BatchNorm1d(512).apply(init_weight),\n",
    "            nn.Linear(512,1).apply(init_weight)\n",
    "        )\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        #encoder\n",
    "        x0 = self.encoder0(inputs) #->(*,64,h/2,w/2) 160,256\n",
    "        x1 = self.encoder1(x0) #->(*,256,h/4,w/4)\n",
    "        x2 = self.encoder2(x1) #->(*,512,h/8,w/8)\n",
    "        x3 = self.encoder3(x2) #->(*,1024,h/16,w/16)\n",
    "        x4 = self.encoder4(x3) #->(*,2048,h/32,w/32)\n",
    "        \n",
    "        #clf head\n",
    "        logits_clf = self.clf(self.avgpool(x4).squeeze(-1).squeeze(-1)) #->(*,1)\n",
    "        if config['clf_threshold'] is not None:\n",
    "            if (torch.sigmoid(logits_clf)>config['clf_threshold']).sum().item()==0:\n",
    "                bs,_,h,w = inputs.shape\n",
    "                logits = torch.zeros((bs,1,h,w))\n",
    "                if self.clfhead:\n",
    "                    if self.deepsupervision:\n",
    "                        return logits,_,_\n",
    "                    else:\n",
    "                        return logits,_\n",
    "                else:\n",
    "                    if self.deepsupervision:\n",
    "                        return logits,_\n",
    "                    else:\n",
    "                        return logits\n",
    "        \n",
    "        #center\n",
    "        y5 = self.center(x4) #->(*,320,h/32,w/32)\n",
    "        \n",
    "        #decoder\n",
    "        y4 = self.decoder4(torch.cat([x4,y5], dim=1)) #->(*,64,h/16,w/16)\n",
    "        y3 = self.decoder3(torch.cat([x3,y4], dim=1)) #->(*,64,h/8,w/8)\n",
    "        y2 = self.decoder2(torch.cat([x2,y3], dim=1)) #->(*,64,h/4,w/4)\n",
    "        y1 = self.decoder1(torch.cat([x1,y2], dim=1)) #->(*,64,h/2,w/2) 160,256\n",
    "        y0 = self.decoder0(y1) #->(*,64,h,w) 320,512\n",
    "        \n",
    "        #hypercolumns\n",
    "        y4 = self.upsample4(y4) #->(*,64,h,w)\n",
    "        y3 = self.upsample3(y3) #->(*,64,h,w)\n",
    "        y2 = self.upsample2(y2) #->(*,64,h,w)\n",
    "        y1 = self.upsample1(y1) #->(*,64,h,w)\n",
    "        hypercol = torch.cat([y0,y1,y2,y3,y4], dim=1)\n",
    "        \n",
    "        #final conv\n",
    "        logits = self.final_conv(hypercol) #->(*,4,h,w)\n",
    "        \n",
    "        #clf head\n",
    "        logits_clf = self.clf(self.avgpool(x4).squeeze(-1).squeeze(-1)) #->(*,1)\n",
    "        \n",
    "        if self.clfhead:\n",
    "            if self.deepsupervision:\n",
    "                s4 = self.deep4(y4)\n",
    "                s3 = self.deep3(y3)\n",
    "                s2 = self.deep2(y2)\n",
    "                s1 = self.deep1(y1)\n",
    "                logits_deeps = [s4,s3,s2,s1]\n",
    "                return logits, logits_deeps, logits_clf\n",
    "            else:\n",
    "                return logits, logits_clf\n",
    "        else:\n",
    "            if self.deepsupervision:\n",
    "                s4 = self.deep4(y4)\n",
    "                s3 = self.deep3(y3)\n",
    "                s2 = self.deep2(y2)\n",
    "                s1 = self.deep1(y1)\n",
    "                logits_deeps = [s4,s3,s2,s1]\n",
    "                return logits, logits_deeps\n",
    "            else:\n",
    "                return logits\n",
    "    \n",
    "\n",
    "#U-Net SeResNext101 + CBAM + hypercolumns + deepsupervision\n",
    "class UNET_SERESNEXT101(nn.Module):\n",
    "    def __init__(self, resolution, deepsupervision, clfhead, load_weights=True):\n",
    "        super().__init__()\n",
    "        h,w = resolution\n",
    "        self.deepsupervision = deepsupervision\n",
    "        self.clfhead = clfhead\n",
    "        \n",
    "        #encoder\n",
    "        model_name = 'se_resnext101_32x4d'\n",
    "        seresnext101 = pretrainedmodels.__dict__[model_name](pretrained=None)\n",
    "        if load_weights:\n",
    "            seresnext101.load_state_dict(torch.load(f'../../../pretrainedmodels_weight/{model_name}.pth'))\n",
    "        \n",
    "        self.encoder0 = nn.Sequential(\n",
    "            seresnext101.layer0.conv1, #(*,3,h,w)->(*,64,h/2,w/2)\n",
    "            seresnext101.layer0.bn1,\n",
    "            seresnext101.layer0.relu1,\n",
    "        )\n",
    "        self.encoder1 = nn.Sequential(\n",
    "            seresnext101.layer0.pool, #->(*,64,h/4,w/4)\n",
    "            seresnext101.layer1 #->(*,256,h/4,w/4)\n",
    "        )\n",
    "        self.encoder2 = seresnext101.layer2 #->(*,512,h/8,w/8)\n",
    "        self.encoder3 = seresnext101.layer3 #->(*,1024,h/16,w/16)\n",
    "        self.encoder4 = seresnext101.layer4 #->(*,2048,h/32,w/32)\n",
    "        \n",
    "        #center\n",
    "        self.center  = CenterBlock(2048,512) #->(*,512,h/32,w/32)\n",
    "        \n",
    "        #decoder\n",
    "        self.decoder4 = DecodeBlock(512+2048,64, upsample=True) #->(*,64,h/16,w/16)\n",
    "        self.decoder3 = DecodeBlock(64+1024,64, upsample=True) #->(*,64,h/8,w/8)\n",
    "        self.decoder2 = DecodeBlock(64+512,64,  upsample=True) #->(*,64,h/4,w/4) \n",
    "        self.decoder1 = DecodeBlock(64+256,64,   upsample=True) #->(*,64,h/2,w/2) \n",
    "        self.decoder0 = DecodeBlock(64,64, upsample=True) #->(*,64,h,w) \n",
    "        \n",
    "        #upsample\n",
    "        self.upsample4 = nn.Upsample(scale_factor=16, mode='bilinear', align_corners=True)\n",
    "        self.upsample3 = nn.Upsample(scale_factor=8, mode='bilinear', align_corners=True)\n",
    "        self.upsample2 = nn.Upsample(scale_factor=4, mode='bilinear', align_corners=True)\n",
    "        self.upsample1 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        \n",
    "        #deep supervision\n",
    "        self.deep4 = conv1x1(64,1).apply(init_weight)\n",
    "        self.deep3 = conv1x1(64,1).apply(init_weight)\n",
    "        self.deep2 = conv1x1(64,1).apply(init_weight)\n",
    "        self.deep1 = conv1x1(64,1).apply(init_weight)\n",
    "        \n",
    "        #final conv\n",
    "        self.final_conv = nn.Sequential(\n",
    "            conv3x3(320,64).apply(init_weight),\n",
    "            nn.ELU(True),\n",
    "            conv1x1(64,1).apply(init_weight)\n",
    "        )\n",
    "        \n",
    "        #clf head\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.clf = nn.Sequential(\n",
    "            nn.BatchNorm1d(2048).apply(init_weight),\n",
    "            nn.Linear(2048,512).apply(init_weight),\n",
    "            nn.ELU(True),\n",
    "            nn.BatchNorm1d(512).apply(init_weight),\n",
    "            nn.Linear(512,1).apply(init_weight)\n",
    "        )\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        #encoder\n",
    "        x0 = self.encoder0(inputs) #->(*,64,h/2,w/2)\n",
    "        x1 = self.encoder1(x0) #->(*,256,h/4,w/4)\n",
    "        x2 = self.encoder2(x1) #->(*,512,h/8,w/8)\n",
    "        x3 = self.encoder3(x2) #->(*,1024,h/16,w/16)\n",
    "        x4 = self.encoder4(x3) #->(*,2048,h/32,w/32)\n",
    "        \n",
    "        #clf head\n",
    "        logits_clf = self.clf(self.avgpool(x4).squeeze(-1).squeeze(-1)) #->(*,1)\n",
    "        if config['clf_threshold'] is not None:\n",
    "            if (torch.sigmoid(logits_clf)>config['clf_threshold']).sum().item()==0:\n",
    "                bs,_,h,w = inputs.shape\n",
    "                logits = torch.zeros((bs,1,h,w))\n",
    "                if self.clfhead:\n",
    "                    if self.deepsupervision:\n",
    "                        return logits,_,_\n",
    "                    else:\n",
    "                        return logits,_\n",
    "                else:\n",
    "                    if self.deepsupervision:\n",
    "                        return logits,_\n",
    "                    else:\n",
    "                        return logits\n",
    "        \n",
    "        #center\n",
    "        y5 = self.center(x4) #->(*,320,h/32,w/32)\n",
    "        \n",
    "        #decoder\n",
    "        y4 = self.decoder4(torch.cat([x4,y5], dim=1)) #->(*,64,h/16,w/16)\n",
    "        y3 = self.decoder3(torch.cat([x3,y4], dim=1)) #->(*,64,h/8,w/8)\n",
    "        y2 = self.decoder2(torch.cat([x2,y3], dim=1)) #->(*,64,h/4,w/4)\n",
    "        y1 = self.decoder1(torch.cat([x1,y2], dim=1)) #->(*,64,h/2,w/2) \n",
    "        y0 = self.decoder0(y1) #->(*,64,h,w)\n",
    "        \n",
    "        #hypercolumns\n",
    "        y4 = self.upsample4(y4) #->(*,64,h,w)\n",
    "        y3 = self.upsample3(y3) #->(*,64,h,w)\n",
    "        y2 = self.upsample2(y2) #->(*,64,h,w)\n",
    "        y1 = self.upsample1(y1) #->(*,64,h,w)\n",
    "        hypercol = torch.cat([y0,y1,y2,y3,y4], dim=1)\n",
    "        \n",
    "        #final conv\n",
    "        logits = self.final_conv(hypercol) #->(*,1,h,w)\n",
    "        \n",
    "        #clf head\n",
    "        logits_clf = self.clf(self.avgpool(x4).squeeze(-1).squeeze(-1)) #->(*,1)\n",
    "        \n",
    "        if self.clfhead:\n",
    "            if self.deepsupervision:\n",
    "                s4 = self.deep4(y4)\n",
    "                s3 = self.deep3(y3)\n",
    "                s2 = self.deep2(y2)\n",
    "                s1 = self.deep1(y1)\n",
    "                logits_deeps = [s4,s3,s2,s1]\n",
    "                return logits, logits_deeps, logits_clf\n",
    "            else:\n",
    "                return logits, logits_clf\n",
    "        else:\n",
    "            if self.deepsupervision:\n",
    "                s4 = self.deep4(y4)\n",
    "                s3 = self.deep3(y3)\n",
    "                s2 = self.deep2(y2)\n",
    "                s1 = self.deep1(y1)\n",
    "                logits_deeps = [s4,s3,s2,s1]\n",
    "                return logits, logits_deeps\n",
    "            else:\n",
    "                return logits    \n",
    "\n",
    "    \n",
    "def build_model(resolution, deepsupervision, clfhead, load_weights):\n",
    "    model_name = config['model_name']\n",
    "    if model_name=='resnet34':\n",
    "        model = UNET_RESNET34(resolution, deepsupervision, clfhead, load_weights)\n",
    "    elif model_name=='seresnext50':\n",
    "        model = UNET_SERESNEXT50(resolution, deepsupervision, clfhead, load_weights)\n",
    "    elif model_name=='seresnext101':\n",
    "        model = UNET_SERESNEXT101(resolution, deepsupervision, clfhead, load_weights)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d1_daaEVzGiZ",
    "papermill": {
     "duration": 0.015919,
     "end_time": "2021-04-12T09:41:35.807080",
     "exception": false,
     "start_time": "2021-04-12T09:41:35.791161",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-12T09:41:35.858574Z",
     "iopub.status.busy": "2021-04-12T09:41:35.857679Z",
     "iopub.status.idle": "2021-04-12T09:42:08.207172Z",
     "shell.execute_reply": "2021-04-12T09:42:08.207577Z"
    },
    "id": "3r4f7pNMzGia",
    "outputId": "25c7748f-ff0f-4beb-e1d7-dae878e2b428",
    "papermill": {
     "duration": 32.384889,
     "end_time": "2021-04-12T09:42:08.207710",
     "exception": false,
     "start_time": "2021-04-12T09:41:35.822821",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from train/new_result/02/model_seed0_fold0_bestscore.pth\n",
      "Loading weights from train/new_result/02/model_seed0_fold1_bestscore.pth\n",
      "Loading weights from train/new_result/02/model_seed0_fold2_bestscore.pth\n",
      "Loading weights from train/new_result/02/model_seed0_fold3_bestscore.pth\n"
     ]
    }
   ],
   "source": [
    "#from models import build_model\n",
    "\n",
    "LOAD_LOCAL_WEIGHT_PATH_LIST = {}\n",
    "for seed in config['split_seed_list']:\n",
    "    LOAD_LOCAL_WEIGHT_PATH_LIST[seed] = []\n",
    "    for fold in config['FOLD_LIST']:\n",
    "        LOAD_LOCAL_WEIGHT_PATH_LIST[seed].append(opj(config['model_path'],f'model_seed{seed}_fold{fold}_bestscore.pth'))\n",
    "        #LOAD_LOCAL_WEIGHT_PATH_LIST[seed].append(opj(config['model_path'],f'model_seed{seed}_fold{fold}_swa.pth'))\n",
    "\n",
    "model_list = {}\n",
    "for seed in config['split_seed_list']:\n",
    "    model_list[seed] = []\n",
    "    for path in LOAD_LOCAL_WEIGHT_PATH_LIST[seed]:\n",
    "        print(\"Loading weights from %s\" % path)\n",
    "        \n",
    "        model = build_model(resolution=(None,None), #config['resolution'], \n",
    "                            deepsupervision=config['deepsupervision'], \n",
    "                            clfhead=config['clfhead'],\n",
    "                            load_weights=False).to(device)\n",
    "        \n",
    "        model.load_state_dict(torch.load(path))\n",
    "        model.eval()\n",
    "        model_list[seed].append(model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-12T09:42:08.251640Z",
     "iopub.status.busy": "2021-04-12T09:42:08.251099Z",
     "iopub.status.idle": "2021-04-12T09:42:09.595776Z",
     "shell.execute_reply": "2021-04-12T09:42:09.594741Z"
    },
    "id": "NjOclDI3zGib",
    "papermill": {
     "duration": 1.37091,
     "end_time": "2021-04-12T09:42:09.595890",
     "exception": false,
     "start_time": "2021-04-12T09:42:08.224980",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from albumentations import (Compose, HorizontalFlip, VerticalFlip, Rotate, RandomRotate90,\n",
    "                            ShiftScaleRotate, ElasticTransform,\n",
    "                            GridDistortion, RandomSizedCrop, RandomCrop, CenterCrop,\n",
    "                            RandomBrightnessContrast, HueSaturationValue, IAASharpen,\n",
    "                            RandomGamma, RandomBrightness, RandomBrightnessContrast,\n",
    "                            GaussianBlur,CLAHE,\n",
    "                            Cutout, CoarseDropout, GaussNoise, ChannelShuffle, ToGray, OpticalDistortion,\n",
    "                            Normalize, OneOf, NoOp)\n",
    "# from albumentations.pytorch import ToTensor, ToTensorV2\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "#from get_config import *\n",
    "#config = get_config()\n",
    "\n",
    "MEAN = np.array([0.485, 0.456, 0.406])\n",
    "STD  = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "def get_transforms_test():\n",
    "    transforms = Compose([\n",
    "        Normalize(mean=(MEAN[0], MEAN[1], MEAN[2]), \n",
    "                  std=(STD[0], STD[1], STD[2])),\n",
    "        ToTensorV2(),\n",
    "    ] )\n",
    "    return transforms\n",
    "\n",
    "def denormalize(z, mean=MEAN.reshape(-1,1,1), std=STD.reshape(-1,1,1)):\n",
    "    return std*z + mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-12T09:42:09.654299Z",
     "iopub.status.busy": "2021-04-12T09:42:09.648269Z",
     "iopub.status.idle": "2021-04-12T09:42:09.656682Z",
     "shell.execute_reply": "2021-04-12T09:42:09.656266Z"
    },
    "id": "_f5ObWK8zGic",
    "papermill": {
     "duration": 0.043409,
     "end_time": "2021-04-12T09:42:09.656774",
     "exception": false,
     "start_time": "2021-04-12T09:42:09.613365",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class HuBMAPDataset(Dataset):\n",
    "    def __init__(self, idx, df):\n",
    "        super().__init__()\n",
    "        filename = df.loc[idx, 'id']+'.tiff'\n",
    "        path = opj(INPUT_PATH,'test',filename)\n",
    "        self.data = rasterio.open(path)\n",
    "        if self.data.count != 3:\n",
    "            subdatasets = self.data.subdatasets\n",
    "            self.layers = []\n",
    "            if len(subdatasets) > 0:\n",
    "                for i,subdataset in enumerate(subdatasets,0):\n",
    "                    self.layers.append(rasterio.open(subdataset))\n",
    "        self.h, self.w = self.data.height, self.data.width\n",
    "        self.input_sz = config['input_resolution']\n",
    "        self.sz = config['resolution']\n",
    "        self.pad_sz = config['pad_size'] # add to each input tile\n",
    "        self.pred_sz = self.sz - 2*self.pad_sz\n",
    "        self.pad_h = self.pred_sz - self.h % self.pred_sz # add to whole slide\n",
    "        self.pad_w = self.pred_sz - self.w % self.pred_sz # add to whole slide\n",
    "        self.num_h = (self.h + self.pad_h) // self.pred_sz\n",
    "        self.num_w = (self.w + self.pad_w) // self.pred_sz\n",
    "        self.transforms = get_transforms_test()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num_h * self.num_w\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # idx = i_h * self.num_w + i_w\n",
    "        # prepare coordinates for rasterio\n",
    "        i_h = idx // self.num_w\n",
    "        i_w = idx % self.num_w\n",
    "        y = i_h*self.pred_sz \n",
    "        x = i_w*self.pred_sz\n",
    "        py0,py1 = max(0,y), min(y+self.pred_sz, self.h)\n",
    "        px0,px1 = max(0,x), min(x+self.pred_sz, self.w)\n",
    "        \n",
    "        # padding coordinate for rasterio\n",
    "        qy0,qy1 = max(0,y-self.pad_sz), min(y+self.pred_sz+self.pad_sz, self.h)\n",
    "        qx0,qx1 = max(0,x-self.pad_sz), min(x+self.pred_sz+self.pad_sz, self.w)\n",
    "        \n",
    "        # placeholder for input tile (before resize)\n",
    "        img = np.zeros((self.sz,self.sz,3), np.uint8)\n",
    "        \n",
    "        # replace the value\n",
    "        if self.data.count == 3: \n",
    "            img[0:qy1-qy0, 0:qx1-qx0] =\\\n",
    "                np.moveaxis(self.data.read([1,2,3], window=Window.from_slices((qy0,qy1),(qx0,qx1))), 0,-1)\n",
    "        else:\n",
    "            for i,layer in enumerate(self.layers):\n",
    "                img[0:qy1-qy0, 0:qx1-qx0, i] =\\\n",
    "                    layer.read(1,window=Window.from_slices((qy0,qy1),(qx0,qx1)))\n",
    "        if self.sz != self.input_sz:\n",
    "            img = cv2.resize(img, (self.input_sz, self.input_sz), interpolation=cv2.INTER_AREA)\n",
    "        img = self.transforms(image=img)['image'] # to normalized tensor\n",
    "        return {'img':img, 'p':[py0,py1,px0,px1], 'q':[qy0,qy1,qx0,qx1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-12T09:42:09.704520Z",
     "iopub.status.busy": "2021-04-12T09:42:09.693994Z",
     "iopub.status.idle": "2021-04-12T09:42:09.724386Z",
     "shell.execute_reply": "2021-04-12T09:42:09.725175Z"
    },
    "id": "eWmkjZOYzGic",
    "papermill": {
     "duration": 0.051188,
     "end_time": "2021-04-12T09:42:09.725288",
     "exception": false,
     "start_time": "2021-04-12T09:42:09.674100",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "import gc\n",
    "import math\n",
    "\n",
    "\n",
    "def my_collate_fn(batch):\n",
    "    img = []\n",
    "    p = []\n",
    "    q = []\n",
    "    for sample in batch:\n",
    "        img.append(sample['img'])\n",
    "        p.append(sample['p'])\n",
    "        q.append(sample['q'])\n",
    "    img = torch.stack(img)\n",
    "    return {'img':img, 'p':p, 'q':q}\n",
    "\n",
    "\n",
    "seed = 0\n",
    "\n",
    "def get_pred_mask(idx, df, model_list):\n",
    "    ds = HuBMAPDataset(idx, df)\n",
    "    #rasterio cannot be used with multiple workers\n",
    "    dl = DataLoader(ds,batch_size=config['test_batch_size'],\n",
    "                    num_workers=0,shuffle=False,pin_memory=True,\n",
    "                    collate_fn=my_collate_fn) \n",
    "    \n",
    "    pred_mask = np.zeros((len(ds),ds.pred_sz,ds.pred_sz), dtype=np.uint8)\n",
    "    \n",
    "    i_data = 0\n",
    "    for data in tqdm(dl):\n",
    "        bs = data['img'].shape[0]\n",
    "        img_patch = data['img'] # (bs,3,input_res,input_res)\n",
    "        pred_mask_float = 0\n",
    "        for model in model_list[seed]:\n",
    "            with torch.no_grad():\n",
    "                if config['tta']>0:\n",
    "                    pred_mask_float += torch.sigmoid(model(img_patch.to(device, torch.float32, non_blocking=True))).detach().cpu().numpy()[:,0,:,:] #.squeeze()\n",
    "                if config['tta']>1:\n",
    "                    # h-flip\n",
    "                    _pred_mask_float = torch.sigmoid(model(img_patch.flip([-1]).to(device, torch.float32, non_blocking=True))).detach().cpu().numpy()[:,0,:,:] #.squeeze()\n",
    "                    pred_mask_float += _pred_mask_float[:,:,::-1]\n",
    "                if config['tta']>2:\n",
    "                    # v-flip\n",
    "                    _pred_mask_float = torch.sigmoid(model(img_patch.flip([-2]).to(device, torch.float32, non_blocking=True))).detach().cpu().numpy()[:,0,:,:] #.squeeze()\n",
    "                    pred_mask_float += _pred_mask_float[:,::-1,:]\n",
    "                if config['tta']>3:\n",
    "                    # h-v-flip\n",
    "                    _pred_mask_float = torch.sigmoid(model(img_patch.flip([-1,-2]).to(device, torch.float32, non_blocking=True))).detach().cpu().numpy()[:,0,:,:] #.squeeze()\n",
    "                    pred_mask_float += _pred_mask_float[:,::-1,::-1]\n",
    "        pred_mask_float = pred_mask_float / min(config['tta'],4) / len(model_list[seed]) # (bs,input_res,input_res)\n",
    "        \n",
    "        # resize\n",
    "        pred_mask_float = np.vstack([cv2.resize(_mask.astype(np.float32), (ds.sz,ds.sz))[None] for _mask in pred_mask_float])\n",
    "        \n",
    "        # float to uint8\n",
    "        pred_mask_int = (pred_mask_float>config['mask_threshold']).astype(np.uint8)\n",
    "        \n",
    "        # replace the values\n",
    "        for j in range(bs):\n",
    "            py0,py1,px0,px1 = data['p'][j]\n",
    "            qy0,qy1,qx0,qx1 = data['q'][j]\n",
    "            pred_mask[i_data+j,0:py1-py0, 0:px1-px0] = pred_mask_int[j, py0-qy0:py1-qy0, px0-qx0:px1-qx0] # (pred_sz,pred_sz)\n",
    "        i_data += bs\n",
    "    \n",
    "    pred_mask = pred_mask.reshape(ds.num_h*ds.num_w, ds.pred_sz, ds.pred_sz).reshape(ds.num_h, ds.num_w, ds.pred_sz, ds.pred_sz)\n",
    "    pred_mask = pred_mask.transpose(0,2,1,3).reshape(ds.num_h*ds.pred_sz, ds.num_w*ds.pred_sz)\n",
    "    pred_mask = pred_mask[:ds.h,:ds.w] # back to the original slide size\n",
    "    non_zero_ratio = (pred_mask).sum() / (ds.h*ds.w)\n",
    "    print('non_zero_ratio = {:.4f}'.format(non_zero_ratio))\n",
    "    return pred_mask,ds.h,ds.w\n",
    "\n",
    "def get_rle(y_preds, h,w):\n",
    "    rle = mask2rle(y_preds, shape=(h,w), small_mask_threshold=config['small_mask_threshold'])\n",
    "    return rle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CL_HandE_1234_B004_bottomleft</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HandE_B005_CL_b_RGB_bottomleft</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               id  predicted\n",
       "0   CL_HandE_1234_B004_bottomleft        NaN\n",
       "1  HandE_B005_CL_b_RGB_bottomleft        NaN"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "6e445635994a496c80f47433b5e9dd48"
     ]
    },
    "execution": {
     "iopub.execute_input": "2021-04-12T09:42:09.829044Z",
     "iopub.status.busy": "2021-04-12T09:42:09.828429Z",
     "iopub.status.idle": "2021-04-12T10:13:01.152951Z",
     "shell.execute_reply": "2021-04-12T10:13:01.152506Z"
    },
    "id": "bjr3-wehzGid",
    "outputId": "9413c696-d43a-4a12-801b-bd207a46ca52",
    "papermill": {
     "duration": 1851.410521,
     "end_time": "2021-04-12T10:13:01.153085",
     "exception": false,
     "start_time": "2021-04-12T09:42:09.742564",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx =  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yiju\\Anaconda3\\envs\\tf-gpu-37\\lib\\site-packages\\rasterio\\__init__.py:220: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix be returned.\n",
      "  s = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2add7e4c2c994633a238261e26b7c536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non_zero_ratio = 0.1221\n",
      "idx =  1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7bfb1d2a0f14ed8b55eb5daa506cdc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non_zero_ratio = 0.0364\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "# %%time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "for idx in range(len(sub_df)): \n",
    "    print('idx = ', idx)\n",
    "    pred_mask, h, w = get_pred_mask(idx, sub_df, model_list)\n",
    "    rle = get_rle(pred_mask,h,w)\n",
    "    sub_df.loc[idx,'predicted'] = rle\n",
    "    # plt.imsave(f'mask_{idx}.png', pred_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LS3Z3lJfzGid",
    "papermill": {
     "duration": 0.018283,
     "end_time": "2021-04-12T10:13:01.190280",
     "exception": false,
     "start_time": "2021-04-12T10:13:01.171997",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-12T10:13:01.231630Z",
     "iopub.status.busy": "2021-04-12T10:13:01.231070Z",
     "iopub.status.idle": "2021-04-12T10:13:01.484398Z",
     "shell.execute_reply": "2021-04-12T10:13:01.483851Z"
    },
    "id": "dXJn9fbbzGid",
    "papermill": {
     "duration": 0.27566,
     "end_time": "2021-04-12T10:13:01.484515",
     "exception": false,
     "start_time": "2021-04-12T10:13:01.208855",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub_df.to_csv('submission_colon.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-12T10:13:01.536389Z",
     "iopub.status.busy": "2021-04-12T10:13:01.535424Z",
     "iopub.status.idle": "2021-04-12T10:13:01.545768Z",
     "shell.execute_reply": "2021-04-12T10:13:01.546200Z"
    },
    "id": "cGFDXuHizGid",
    "outputId": "5a7c0c58-65ca-465f-96ff-e1e00fd34f34",
    "papermill": {
     "duration": 0.041934,
     "end_time": "2021-04-12T10:13:01.546332",
     "exception": false,
     "start_time": "2021-04-12T10:13:01.504398",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CL_HandE_1234_B004_bottomleft</td>\n",
       "      <td>453303 17 457837 24 462371 28 466905 31 471439...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HandE_B005_CL_b_RGB_bottomleft</td>\n",
       "      <td>13734858 6 13739389 15 13743923 19 13748457 23...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               id  \\\n",
       "0   CL_HandE_1234_B004_bottomleft   \n",
       "1  HandE_B005_CL_b_RGB_bottomleft   \n",
       "\n",
       "                                           predicted  \n",
       "0  453303 17 457837 24 462371 28 466905 31 471439...  \n",
       "1  13734858 6 13739389 15 13743923 19 13748457 23...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1923d0aa4c8>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(pred_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Tom_firstPlace_hubmap-tilespadded-inference-v2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "duration": 1896.250187,
   "end_time": "2021-04-12T10:13:03.030786",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-04-12T09:41:26.780599",
   "version": "2.1.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
