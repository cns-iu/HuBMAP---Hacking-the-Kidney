{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ed8d78b",
   "metadata": {},
   "source": [
    "<h1> HubMap - Hacking the Kidney </h1>\n",
    "<h3> Goal - Mapping the human body at function tissue unit level - detect crypts FTUs in colon </h3>\n",
    "\n",
    "Implementation of Kaggle Notebook - Scientific Prize Winner - Deeplive <br>\n",
    "Description - Use 2-5 fold models to predict on test image masks <br>\n",
    "Input - models, test images, sample_submission.csv  <br>\n",
    "Output - submission_deeplive_generalized.csv (rle for test images) <br>\n",
    "\n",
    "<b>How to use?</b><br> \n",
    "Change the basepath to where your data lives and you're good to go. <br>\n",
    "\n",
    "Link 1 - kaggle.com/optimo/hubmap-inference-th-o \n",
    "\n",
    "<h6> Step 1 - Import useful libraries </h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "56347120",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import cv2\n",
    "import json\n",
    "import glob\n",
    "import torch\n",
    "import random\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import tifffile as tiff\n",
    "import albumentations as albu\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "from rasterio.windows import Window\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867bf238",
   "metadata": {},
   "source": [
    "<h6> Step 2 - Helper functions </h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2013f0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    \n",
    "SEED = 2021\n",
    "seed_everything(SEED)\n",
    "CLASSES = [\"ftus\"]\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "MEAN = np.array([0.66437738, 0.50478148, 0.70114894])\n",
    "STD = np.array([0.15825711, 0.24371008, 0.13832686])\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "IDENTITY = rasterio.Affine(1, 0, 0, 0, 1, 0)\n",
    "FLIPS = [[-1], [-2], [-2, -1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71dc16e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(img_path, df_info, reduce_factor=1):\n",
    "    \"\"\"\n",
    "    Load image and make sure sizes matches df_info\n",
    "    \"\"\"\n",
    "    image_fname = img_path.rsplit(\"/\", -1)[-1]\n",
    "    \n",
    "    \n",
    "    W = int(df_info[df_info.image_file == image_fname][\"width_pixels\"])\n",
    "    H = int(df_info[df_info.image_file == image_fname][\"height_pixels\"])\n",
    "\n",
    "    img = tiff.imread(img_path).squeeze()\n",
    "\n",
    "    channel_pos = np.argwhere(np.array(img.shape) == 3)[0][0]\n",
    "    W_pos = np.argwhere(np.array(img.shape) == W)[0][0]\n",
    "    H_pos = np.argwhere(np.array(img.shape) == H)[0][0]\n",
    "\n",
    "    img = np.moveaxis(img, (H_pos, W_pos, channel_pos), (0, 1, 2))\n",
    "    \n",
    "    if reduce_factor > 1:\n",
    "        img = cv2.resize(\n",
    "            img,\n",
    "            (img.shape[1] // reduce_factor, img.shape[0] // reduce_factor),\n",
    "            interpolation=cv2.INTER_AREA,\n",
    "        )\n",
    "        \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99b396d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def HE_preprocess(augment=False, visualize=False, mean=MEAN, std=STD):\n",
    "    if visualize:\n",
    "        normalizer = albu.Compose(\n",
    "            [albu.Normalize(mean=[0, 0, 0], std=[1, 1, 1]), ToTensorV2()], p=1\n",
    "        )\n",
    "    else:\n",
    "        normalizer = albu.Compose(\n",
    "            [albu.Normalize(mean=mean, std=std), ToTensorV2()], p=1\n",
    "        )\n",
    "    \n",
    "    if augment:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    return normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4b7493a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rle_encode_less_memory(img):\n",
    "    pixels = img.T.flatten()\n",
    "    pixels[0] = 0\n",
    "    pixels[-1] = 0\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 2\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "def enc2mask(encs, shape):\n",
    "    print (encs)\n",
    "    img = np.zeros(shape[0] * shape[1], dtype=np.uint8)\n",
    "    for m, enc in enumerate(encs):\n",
    "        if isinstance(enc, np.float) and np.isnan(enc):\n",
    "            continue\n",
    "        enc_split = enc.split()\n",
    "        for i in range(len(enc_split) // 2):\n",
    "            start = int(enc_split[2 * i]) - 1\n",
    "            length = int(enc_split[2 * i + 1])\n",
    "            img[start: start + length] = 1 + m\n",
    "    return img.reshape(shape).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "518ef037",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        original_img_path,\n",
    "        rle=None,\n",
    "        overlap_factor=1,\n",
    "        tile_size=256,\n",
    "        reduce_factor=4,\n",
    "        transforms=None,\n",
    "    ):\n",
    "        self.original_img = load_image(original_img_path, full_size=reduce_factor > 1)\n",
    "        self.orig_size = self.original_img.shape\n",
    "        self.raw_tile_size = tile_size\n",
    "        self.reduce_factor = reduce_factor\n",
    "        self.tile_size = tile_size * reduce_factor\n",
    "        self.overlap_factor = overlap_factor\n",
    "        self.positions = self.get_positions()\n",
    "        self.transforms = transforms\n",
    "\n",
    "        if rle is not None:\n",
    "            self.mask = enc2mask(rle, (self.orig_size[1], self.orig_size[0])) > 0\n",
    "        else:\n",
    "            self.mask = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.positions)\n",
    "\n",
    "    def get_positions(self):\n",
    "        top_x = np.arange(\n",
    "            0,\n",
    "            self.orig_size[0],  # +self.tile_size,\n",
    "            int(self.tile_size / self.overlap_factor),\n",
    "        )\n",
    "        top_y = np.arange(\n",
    "            0,\n",
    "            self.orig_size[1],  # +self.tile_size,\n",
    "            int(self.tile_size / self.overlap_factor),\n",
    "        )\n",
    "        starting_positions = []\n",
    "        for x in top_x:\n",
    "            right_space = self.orig_size[0] - (x + self.tile_size)\n",
    "            if right_space > 0:\n",
    "                boundaries_x = (x, x + self.tile_size)\n",
    "            else:\n",
    "                boundaries_x = (x + right_space, x + right_space + self.tile_size)\n",
    "\n",
    "            for y in top_y:\n",
    "                down_space = self.orig_size[1] - (y + self.tile_size)\n",
    "                if down_space > 0:\n",
    "                    boundaries_y = (y, y + self.tile_size)\n",
    "                else:\n",
    "                    boundaries_y = (y + down_space, y + down_space + self.tile_size)\n",
    "                starting_positions.append((boundaries_x, boundaries_y))\n",
    "\n",
    "        return starting_positions\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pos_x, pos_y = self.positions[idx]\n",
    "        img = self.original_img[pos_x[0]: pos_x[1], pos_y[0]: pos_y[1], :]\n",
    "\n",
    "        if self.reduce_factor > 1:\n",
    "            img = cv2.resize(\n",
    "                img, (self.raw_tile_size, self.raw_tile_size), interpolation=cv2.INTER_AREA\n",
    "            )\n",
    "\n",
    "        if self.transforms:\n",
    "            img = self.transforms(image=img)[\"image\"]\n",
    "\n",
    "        pos = np.array([pos_x[0], pos_x[1], pos_y[0], pos_y[1]])\n",
    "\n",
    "        return img, pos\n",
    "    \n",
    "class InferenceEfficientDataset(InferenceDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        original_img_path,\n",
    "        rle=None,\n",
    "        overlap_factor=1,\n",
    "        tile_size=256,\n",
    "        reduce_factor=4,\n",
    "        transforms=None,\n",
    "    ):\n",
    "            \n",
    "        self.raw_tile_size = tile_size\n",
    "        self.reduce_factor = reduce_factor\n",
    "        self.tile_size = tile_size * reduce_factor\n",
    "        \n",
    "        self.overlap_factor = overlap_factor\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        self.original_img = rasterio.open(original_img_path, transform=IDENTITY, num_threads='all_cpus')\n",
    "        if self.original_img.count != 3:\n",
    "            self.layers = [rasterio.open(subd) for subd in self.original_img.subdatasets]\n",
    "                    \n",
    "        self.orig_size = self.original_img.shape\n",
    "\n",
    "        self.positions = self.get_positions()\n",
    "        \n",
    "        if rle is not None:\n",
    "            self.mask = enc2mask(rle, (self.orig_size[1], self.orig_size[0])) > 0\n",
    "        else:\n",
    "            self.mask = None\n",
    "\n",
    "    def __getitem__(self, idx):        \n",
    "        # Window\n",
    "        pos_x, pos_y = self.positions[idx]\n",
    "        x1, x2 = pos_x[0], pos_x[1]\n",
    "        y1, y2 = pos_y[0], pos_y[1]\n",
    "        window = Window.from_slices((x1, x2), (y1, y2))\n",
    "\n",
    "        # Retrieve slice\n",
    "        if self.original_img.count == 3:  # normal\n",
    "            img = self.original_img.read([1, 2, 3], window=window)\n",
    "            img = np.moveaxis(img, 0, -1)\n",
    "        else:  # with subdatasets/layers\n",
    "            img = np.zeros((self.tile_size, self.tile_size, 3), dtype=np.uint8)\n",
    "            for fl in range(3):\n",
    "                img[:, :, fl] = self.layers[fl].read(window=window) \n",
    "\n",
    "        # Downscale to tile size\n",
    "        img = cv2.resize(\n",
    "            img, (self.raw_tile_size, self.raw_tile_size), interpolation=cv2.INTER_AREA\n",
    "        )\n",
    "        img = self.transforms(image=img)[\"image\"]\n",
    "        \n",
    "        pos = np.array([pos_x[0], pos_x[1], pos_y[0], pos_y[1]])\n",
    "\n",
    "        return img, pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "550b2db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_weights(model, filename, verbose=1, cp_folder=\"\"):\n",
    "    if verbose:\n",
    "        print(f\"\\n -> Loading weights from {os.path.join(cp_folder,filename)}\\n\")\n",
    "    try:\n",
    "        model.load_state_dict(os.path.join(cp_folder, filename), strict=True)\n",
    "    except BaseException:\n",
    "        model.load_state_dict(\n",
    "            torch.load(os.path.join(cp_folder, filename), map_location=\"cpu\"),\n",
    "            strict=True,\n",
    "        )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdccdb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch\n",
    "from segmentation_models_pytorch.encoders import encoders\n",
    "\n",
    "\n",
    "DECODERS = [\"Unet\", \"Linknet\", \"FPN\", \"PSPNet\", \"DeepLabV3\", \"DeepLabV3Plus\", \"PAN\"]\n",
    "ENCODERS = list(encoders.keys())\n",
    "\n",
    "\n",
    "def define_model(\n",
    "    decoder_name, encoder_name, num_classes=1, activation=None, encoder_weights=\"imagenet\"\n",
    "):\n",
    "    assert decoder_name in DECODERS, \"Decoder name not supported\"\n",
    "    assert encoder_name in ENCODERS, \"Encoder name not supported\"\n",
    "\n",
    "    decoder = getattr(segmentation_models_pytorch, decoder_name)\n",
    "\n",
    "    model = decoder(\n",
    "        encoder_name,\n",
    "        encoder_weights=encoder_weights,\n",
    "        classes=num_classes,\n",
    "        activation=activation,\n",
    "    )\n",
    "    model.num_classes = num_classes\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed387e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self, **entries):\n",
    "        self.__dict__.update(entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "747ef3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(cp_folder):\n",
    "    config = json.load(open(cp_folder/'config.json', 'r'))\n",
    "    config = Config(**config)\n",
    "    \n",
    "    weights = sorted(glob.glob(str(cp_folder/'*.pt')))\n",
    "    models = []\n",
    "    \n",
    "    for weight in weights:\n",
    "        model = define_model(\n",
    "            config.decoder,\n",
    "            config.encoder,\n",
    "            num_classes=config.num_classes,\n",
    "            encoder_weights=None,\n",
    "        )\n",
    "        \n",
    "        model = load_model_weights(model, weight).to(DEVICE)\n",
    "        model.eval()\n",
    "        models.append(model)\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9c082259",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_scores_img(pred, truth, eps=1e-8):\n",
    "    pred = pred.reshape(-1) > 0\n",
    "    truth = truth.reshape(-1) > 0\n",
    "    intersect = (pred & truth).sum(-1)\n",
    "    union = pred.sum(-1) + truth.sum(-1)\n",
    "\n",
    "    dice = (2.0 * intersect + eps) / (union + eps)\n",
    "    return dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "453c30ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tile_weighting(size, sigma=1, alpha=1, eps=1e-6):\n",
    "    half = size // 2\n",
    "    w = np.ones((size, size), np.float32)\n",
    "\n",
    "    x = np.concatenate([np.mgrid[-half:0], np.mgrid[1: half + 1]])[:, None]\n",
    "    x = np.tile(x, (1, size))\n",
    "    x = half + 1 - np.abs(x)\n",
    "    y = x.T\n",
    "\n",
    "    w = np.minimum(x, y)\n",
    "    w = (w / w.max()) ** sigma\n",
    "    w = np.minimum(w, 1)\n",
    "\n",
    "    w = (w - np.min(w) + eps) / (np.max(w) - np.min(w) + eps)\n",
    "\n",
    "    w = np.where(w > alpha, 1, w)\n",
    "    w = w / alpha\n",
    "    w = np.clip(w, 1e-3, 1)\n",
    "\n",
    "    w = np.round(w, 3)\n",
    "    return w.astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d640fbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_entire_mask(dataset, models, batch_size=32, tta=False):\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "    weighting = torch.from_numpy(get_tile_weighting(dataset.tile_size, sigma=1, alpha=1))\n",
    "    weighting_cuda = weighting.clone().cuda().unsqueeze(0)\n",
    "    weighting = weighting.cuda().half()\n",
    "\n",
    "    global_pred = torch.zeros(\n",
    "        (dataset.orig_size[0], dataset.orig_size[1]),\n",
    "        dtype=torch.half, device=\"cuda\"\n",
    "    )\n",
    "    global_counter = torch.zeros(\n",
    "        (dataset.orig_size[0], dataset.orig_size[1]),\n",
    "        dtype=torch.half, device=\"cuda\"\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img, pos in tqdm(loader):\n",
    "            img = img.to(\"cuda\")\n",
    "            _, _, h, w = img.shape\n",
    "            \n",
    "            model_preds = []\n",
    "            for model in models:\n",
    "                if model.num_classes == 1:\n",
    "                    pred = model(img).view(1, -1, h, w).sigmoid().detach()\n",
    "                else:\n",
    "                    pred = model(img)[:, 0].view(1, -1, h, w).sigmoid().detach()\n",
    "\n",
    "                if tta:\n",
    "                    for f in FLIPS:\n",
    "                        pred_flip = model(torch.flip(img, f))\n",
    "                        if model.num_classes == 2:\n",
    "                            pred_flip = pred_flip[:, :1]\n",
    "\n",
    "                        pred_flip = torch.flip(pred_flip, f).view(1, -1, h, w).sigmoid().detach()\n",
    "                        pred += pred_flip\n",
    "                    pred = torch.div(pred, len(FLIPS) + 1)\n",
    "\n",
    "                model_preds.append(pred)\n",
    "\n",
    "            pred = torch.cat(model_preds, 0).mean(0)\n",
    "\n",
    "            pred = torch.nn.functional.interpolate(\n",
    "                pred.unsqueeze(1), (dataset.tile_size, dataset.tile_size), mode='area'\n",
    "            ).squeeze(1)\n",
    "            \n",
    "            pred = (pred * weighting_cuda).half()\n",
    "\n",
    "            for tile_idx, (x0, x1, y0, y1) in enumerate(pos):\n",
    "                global_pred[x0: x1, y0: y1] += pred[tile_idx]\n",
    "                global_counter[x0: x1, y0: y1] += weighting\n",
    "\n",
    "    for i in range(len(global_pred)):\n",
    "        global_pred[i] = torch.div(global_pred[i], global_counter[i])\n",
    "\n",
    "    return global_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4ffdab",
   "metadata": {},
   "source": [
    "<h6> Step 3 - Set configuration and paths </h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9507f3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(r'C:/Users/soodn/Downloads/Naveksha/Kaggle HuBMAP')\n",
    "IMG_PATH = DATA_PATH/'Data/colon-data-reprocessed/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dde1c7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0.5\n",
    "\n",
    "USE_TTA = True # not DEBUG\n",
    "OVERLAP_FACTOR = 1.5\n",
    "\n",
    "CP_FOLDERS = [ Path(r'C:/Users/soodn/Downloads/Naveksha/Kaggle HuBMAP/Scripts/4. DeepLive/models/b1_2cfix/b1_2cfix'), Path(r'C:/Users/soodn/Downloads/Naveksha/Kaggle HuBMAP/Scripts/4. DeepLive/models//b1_last/b1_last')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8f33444f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH/'Data/colon-data-reprocessed/sample_submission.csv')\n",
    "df_info = pd.read_csv(DATA_PATH/'Data/colon-data-reprocessed/HuBMAP-20-dataset_information.csv')\n",
    "rles = pd.read_csv(DATA_PATH/'Data/colon-data-reprocessed/test.csv')\n",
    "\n",
    "config = json.load(open(CP_FOLDERS[0]/'config.json', 'r'))\n",
    "config = Config(**config)\n",
    "config.overlap_factor = OVERLAP_FACTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ddbfd503",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_test = DATA_PATH/'Data/colon-data-reprocessed/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "08ad5c91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -> Loading weights from C:\\Users\\soodn\\Downloads\\Naveksha\\Kaggle HuBMAP\\Scripts\\4. DeepLive\\models\\b1_2cfix\\b1_2cfix\\Unet_efficientnet-b1_0.pt\n",
      "\n",
      "\n",
      " -> Loading weights from C:\\Users\\soodn\\Downloads\\Naveksha\\Kaggle HuBMAP\\Scripts\\4. DeepLive\\models\\b1_2cfix\\b1_2cfix\\Unet_efficientnet-b1_1.pt\n",
      "\n",
      "\n",
      " -> Loading weights from C:\\Users\\soodn\\Downloads\\Naveksha\\Kaggle HuBMAP\\Scripts\\4. DeepLive\\models\\b1_2cfix\\b1_2cfix\\Unet_efficientnet-b1_2.pt\n",
      "\n",
      "\n",
      " -> Loading weights from C:\\Users\\soodn\\Downloads\\Naveksha\\Kaggle HuBMAP\\Scripts\\4. DeepLive\\models\\b1_2cfix\\b1_2cfix\\Unet_efficientnet-b1_3.pt\n",
      "\n",
      "\n",
      " -> Loading weights from C:\\Users\\soodn\\Downloads\\Naveksha\\Kaggle HuBMAP\\Scripts\\4. DeepLive\\models\\b1_2cfix\\b1_2cfix\\Unet_efficientnet-b1_4.pt\n",
      "\n",
      "\n",
      " -> Loading weights from C:\\Users\\soodn\\Downloads\\Naveksha\\Kaggle HuBMAP\\Scripts\\4. DeepLive\\models\\b1_last\\b1_last\\Unet_efficientnet-b1_0.pt\n",
      "\n",
      "\n",
      " -> Loading weights from C:\\Users\\soodn\\Downloads\\Naveksha\\Kaggle HuBMAP\\Scripts\\4. DeepLive\\models\\b1_last\\b1_last\\Unet_efficientnet-b1_1.pt\n",
      "\n",
      "\n",
      " -> Loading weights from C:\\Users\\soodn\\Downloads\\Naveksha\\Kaggle HuBMAP\\Scripts\\4. DeepLive\\models\\b1_last\\b1_last\\Unet_efficientnet-b1_2.pt\n",
      "\n",
      "\n",
      " -> Loading weights from C:\\Users\\soodn\\Downloads\\Naveksha\\Kaggle HuBMAP\\Scripts\\4. DeepLive\\models\\b1_last\\b1_last\\Unet_efficientnet-b1_3.pt\n",
      "\n",
      "\n",
      " -> Loading weights from C:\\Users\\soodn\\Downloads\\Naveksha\\Kaggle HuBMAP\\Scripts\\4. DeepLive\\models\\b1_last\\b1_last\\Unet_efficientnet-b1_4.pt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "\n",
    "for cp_folder in CP_FOLDERS:\n",
    "    models += load_models(cp_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431b03e4",
   "metadata": {},
   "source": [
    "<h6> Step 4 - Make Prediction </h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "00f8e48b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t Image CL_HandE_1234_B004_bottomleft\n",
      "\n",
      " - Building dataset\n",
      "\n",
      " - Predicting masks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soodn\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\rasterio\\__init__.py:219: NotGeoreferencedWarning: Dataset has no geotransform set. The identity matrix may be returned.\n",
      "  s = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44b16ba89d3d471abd97d0823845dc6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " - Encoding\n",
      "\n",
      "\t Image HandE_B005_CL_b_RGB_bottomleft\n",
      "\n",
      " - Building dataset\n",
      "\n",
      " - Predicting masks\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed1e27b772d44439b5ad385eea3ec340",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " - Encoding\n",
      "0 0 0 0\n"
     ]
    }
   ],
   "source": [
    "DEBUG = False\n",
    "sum_score = 0\n",
    "sum_pa = 0\n",
    "sum_ji = 0\n",
    "sum_haus = 0\n",
    "for img in df['id'].unique():\n",
    "    if DEBUG:  # Check performances on a validation image\n",
    "        img = \"2f6ecfcdf\"  # check repro\n",
    "#         img = \"4ef6695ce\" # biggest img\n",
    "        IMG_PATH = DATA_PATH + \"train\"\n",
    "        models = [models[0], models[5]]\n",
    "                  \n",
    "    \n",
    "    print(f'\\n\\t Image {img}')\n",
    "    \n",
    "    if img == \"d488c759a\":\n",
    "        print('\\n - Using precomputed rle')\n",
    "        local_file_fc = '../input/hubmap-fast-submission/submission_0933_fc.csv'\n",
    "        df_local_fc = pd.read_csv(local_file_fc, index_col='id')\n",
    "        rle = df_local_fc.loc['d488c759a', 'predicted']\n",
    "        df.loc[df.id == img, 'predicted'] = rle\n",
    "\n",
    "        continue\n",
    "    \n",
    "    print(f'\\n - Building dataset')\n",
    "    \n",
    "    rle_truth = rles[rles['id'] == img][\"encoding\"] if DEBUG else None\n",
    "    \n",
    "    predict_dataset = InferenceEfficientDataset(\n",
    "        f\"{IMG_PATH}/{img}.tiff\",\n",
    "        rle=rle_truth,\n",
    "        overlap_factor=config.overlap_factor,\n",
    "        reduce_factor=config.reduce_factor,\n",
    "        tile_size=config.tile_size,\n",
    "        transforms=HE_preprocess(augment=False, visualize=False),\n",
    "    )\n",
    "    \n",
    "    print(f'\\n - Predicting masks')\n",
    "\n",
    "    global_pred = predict_entire_mask(\n",
    "        predict_dataset, models, batch_size=config.val_bs, tta=USE_TTA\n",
    "    )\n",
    "    \n",
    "    del predict_dataset\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    print('\\n - Encoding')\n",
    "    \n",
    "    global_pred_np = np.zeros(global_pred.size(), dtype=np.uint8)\n",
    "\n",
    "    for i in range(global_pred_np.shape[0]):\n",
    "        global_pred_np[i] = (global_pred[i] > THRESHOLD).cpu().numpy().astype(np.uint8)\n",
    "    \n",
    "    rle = rle_encode_less_memory(global_pred_np)\n",
    "    df.loc[df.id == img, 'predicted'] = rle\n",
    "    \n",
    "    if DEBUG:\n",
    "        shape = df_info[df_info.image_file == img + \".tiff\"][['width_pixels', 'height_pixels']].values.astype(int)[0]\n",
    "        mask_truth = enc2mask(rle_truth, shape)\n",
    "        score = dice_scores_img(global_pred_np, mask_truth)\n",
    "        print(f\" -> Scored {score :.4f} with threshold {THRESHOLD:.2f}\")\n",
    "        break\n",
    "        \n",
    "    del global_pred, global_pred_np\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "print (sum_score, sum_pa, sum_ji, sum_haus)\n",
    "df.to_csv('deeplive_submission_colon.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "aa231233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CL_HandE_1234_B004_bottomleft</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HandE_B005_CL_b_RGB_bottomleft</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               id predicted\n",
       "0   CL_HandE_1234_B004_bottomleft          \n",
       "1  HandE_B005_CL_b_RGB_bottomleft          "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ef38fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
