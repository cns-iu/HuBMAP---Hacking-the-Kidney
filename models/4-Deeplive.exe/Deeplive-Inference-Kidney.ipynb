{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a319f147",
   "metadata": {},
   "source": [
    "<h1> HubMap - Hacking the Kidney </h1>\n",
    "<h3> Goal - Mapping the human body at function tissue unit level - detect crypts FTUs in colon </h3>\n",
    "\n",
    "Implementation of Kaggle Notebook - Scientific Prize Winner - Deeplive <br>\n",
    "Description - Use 2-5 fold models to predict on test image masks <br>\n",
    "Input - models, test images, sample_submission.csv  <br>\n",
    "Output - submission_deeplive_generalized.csv (rle for test images) <br>\n",
    "\n",
    "<b>How to use?</b><br> \n",
    "Change the basepath to where your data lives and you're good to go. <br>\n",
    "\n",
    "Link 1 - kaggle.com/optimo/hubmap-inference-th-o \n",
    "\n",
    "<h6> Step 1 - Import useful libraries </h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7275c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import cv2\n",
    "import gdal \n",
    "import json\n",
    "import glob\n",
    "import torch\n",
    "import random\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tifffile as tiff\n",
    "from pathlib import Path\n",
    "import segmentation_models_pytorch\n",
    "from segmentation_models_pytorch.encoders import encoders\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "from rasterio.windows import Window\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a443ee",
   "metadata": {},
   "source": [
    "<h6> Step 2 - Helper Functions </h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "867fac47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    \"\"\"\n",
    "    Seeds basic parameters for reproductibility of results.\n",
    "\n",
    "    Args:\n",
    "        seed (int): Number of the seed.\n",
    "    \"\"\"\n",
    "\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    \n",
    "SEED = 2021\n",
    "seed_everything(SEED)\n",
    "CLASSES = [\"ftus\"]\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "MEAN = np.array([0.66437738, 0.50478148, 0.70114894])\n",
    "STD = np.array([0.15825711, 0.24371008, 0.13832686])\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "IDENTITY = rasterio.Affine(1, 0, 0, 0, 1, 0)\n",
    "FLIPS = [[-1], [-2], [-2, -1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "498f08ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(img_path, df_info, reduce_factor=1):\n",
    "    \"\"\"\n",
    "    Load image and make sure sizes matches df_info\n",
    "    \"\"\"\n",
    "    image_fname = img_path.rsplit(\"/\", -1)[-1]\n",
    "    \n",
    "    \n",
    "    W = int(df_info[df_info.image_file == image_fname][\"width_pixels\"])\n",
    "    H = int(df_info[df_info.image_file == image_fname][\"height_pixels\"])\n",
    "\n",
    "    img = tiff.imread(img_path).squeeze()\n",
    "\n",
    "    channel_pos = np.argwhere(np.array(img.shape) == 3)[0][0]\n",
    "    W_pos = np.argwhere(np.array(img.shape) == W)[0][0]\n",
    "    H_pos = np.argwhere(np.array(img.shape) == H)[0][0]\n",
    "\n",
    "    img = np.moveaxis(img, (H_pos, W_pos, channel_pos), (0, 1, 2))\n",
    "    \n",
    "    if reduce_factor > 1:\n",
    "        img = cv2.resize(\n",
    "            img,\n",
    "            (img.shape[1] // reduce_factor, img.shape[0] // reduce_factor),\n",
    "            interpolation=cv2.INTER_AREA,\n",
    "        )\n",
    "        \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c793b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as albu\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "def HE_preprocess(augment=False, visualize=False, mean=MEAN, std=STD):\n",
    "    \"\"\"\n",
    "    Returns transformations for the H&E images.\n",
    "\n",
    "    Args:\n",
    "        augment (bool, optional): Whether to apply augmentations. Defaults to True.\n",
    "        visualize (bool, optional): Whether to use transforms for visualization. Defaults to False.\n",
    "        mean ([type], optional): Mean for normalization. Defaults to MEAN.\n",
    "        std ([type], optional): Standard deviation for normalization. Defaults to STD.\n",
    "\n",
    "    Returns:\n",
    "        albumentation transforms: transforms.\n",
    "    \"\"\"\n",
    "    if visualize:\n",
    "        normalizer = albu.Compose(\n",
    "            [albu.Normalize(mean=[0, 0, 0], std=[1, 1, 1]), ToTensorV2()], p=1\n",
    "        )\n",
    "    else:\n",
    "        normalizer = albu.Compose(\n",
    "            [albu.Normalize(mean=mean, std=std), ToTensorV2()], p=1\n",
    "        )\n",
    "    \n",
    "    if augment:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    return normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "261f792f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rle_encode_less_memory(img):\n",
    "    pixels = img.T.flatten()\n",
    "    pixels[0] = 0\n",
    "    pixels[-1] = 0\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 2\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "def enc2mask(encs, shape):\n",
    "    img = np.zeros(shape[0] * shape[1], dtype=np.uint8)\n",
    "    for m, enc in enumerate(encs):\n",
    "        if isinstance(enc, np.float) and np.isnan(enc):\n",
    "            continue\n",
    "        enc_split = enc.split()\n",
    "        for i in range(len(enc_split) // 2):\n",
    "            start = int(enc_split[2 * i]) - 1\n",
    "            length = int(enc_split[2 * i + 1])\n",
    "            img[start: start + length] = 1 + m\n",
    "\n",
    "    return img.reshape(shape).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfc6d880",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        original_img_path,\n",
    "        rle=None,\n",
    "        overlap_factor=1,\n",
    "        tile_size=256,\n",
    "        reduce_factor=4,\n",
    "        transforms=None,\n",
    "    ):\n",
    "        self.original_img = load_image(original_img_path, full_size=reduce_factor > 1)\n",
    "        self.orig_size = self.original_img.shape\n",
    "\n",
    "        # self.original_img = lab_normalization(self.original_img)\n",
    "\n",
    "        self.raw_tile_size = tile_size\n",
    "        self.reduce_factor = reduce_factor\n",
    "        self.tile_size = tile_size * reduce_factor\n",
    "\n",
    "        self.overlap_factor = overlap_factor\n",
    "\n",
    "        self.positions = self.get_positions()\n",
    "        self.transforms = transforms\n",
    "\n",
    "        if rle is not None:\n",
    "            self.mask = enc2mask(rle, (self.orig_size[1], self.orig_size[0])) > 0\n",
    "        else:\n",
    "            self.mask = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.positions)\n",
    "\n",
    "    def get_positions(self):\n",
    "        top_x = np.arange(\n",
    "            0,\n",
    "            self.orig_size[0],  # +self.tile_size,\n",
    "            int(self.tile_size / self.overlap_factor),\n",
    "        )\n",
    "        top_y = np.arange(\n",
    "            0,\n",
    "            self.orig_size[1],  # +self.tile_size,\n",
    "            int(self.tile_size / self.overlap_factor),\n",
    "        )\n",
    "        starting_positions = []\n",
    "        for x in top_x:\n",
    "            right_space = self.orig_size[0] - (x + self.tile_size)\n",
    "            if right_space > 0:\n",
    "                boundaries_x = (x, x + self.tile_size)\n",
    "            else:\n",
    "                boundaries_x = (x + right_space, x + right_space + self.tile_size)\n",
    "\n",
    "            for y in top_y:\n",
    "                down_space = self.orig_size[1] - (y + self.tile_size)\n",
    "                if down_space > 0:\n",
    "                    boundaries_y = (y, y + self.tile_size)\n",
    "                else:\n",
    "                    boundaries_y = (y + down_space, y + down_space + self.tile_size)\n",
    "                starting_positions.append((boundaries_x, boundaries_y))\n",
    "\n",
    "        return starting_positions\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pos_x, pos_y = self.positions[idx]\n",
    "        img = self.original_img[pos_x[0]: pos_x[1], pos_y[0]: pos_y[1], :]\n",
    "\n",
    "        # img = lab_normalization(img)\n",
    "\n",
    "        # down scale to tile size\n",
    "        if self.reduce_factor > 1:\n",
    "            img = cv2.resize(\n",
    "                img, (self.raw_tile_size, self.raw_tile_size), interpolation=cv2.INTER_AREA\n",
    "            )\n",
    "\n",
    "        if self.transforms:\n",
    "            img = self.transforms(image=img)[\"image\"]\n",
    "\n",
    "        pos = np.array([pos_x[0], pos_x[1], pos_y[0], pos_y[1]])\n",
    "\n",
    "        return img, pos\n",
    "    \n",
    "class InferenceEfficientDataset(InferenceDataset):\n",
    "    \"\"\"\n",
    "    Refs : \n",
    "    https://www.kaggle.com/iafoss/hubmap-pytorch-fast-ai-starter-sub/\n",
    "    https://www.kaggle.com/finlay/pytorch-fcn-resnet50-in-20-minute\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        original_img_path,\n",
    "        rle=None,\n",
    "        overlap_factor=1,\n",
    "        tile_size=256,\n",
    "        reduce_factor=4,\n",
    "        transforms=None,\n",
    "    ):\n",
    "            \n",
    "        self.raw_tile_size = tile_size\n",
    "        self.reduce_factor = reduce_factor\n",
    "        self.tile_size = tile_size * reduce_factor\n",
    "        \n",
    "        self.overlap_factor = overlap_factor\n",
    "        self.transforms = transforms\n",
    "\n",
    "        # Load image with rasterio        \n",
    "        self.original_img = rasterio.open(original_img_path, transform=IDENTITY, num_threads='all_cpus')\n",
    "        if self.original_img.count != 3:\n",
    "            self.layers = [rasterio.open(subd) for subd in self.original_img.subdatasets]\n",
    "                    \n",
    "        self.orig_size = self.original_img.shape\n",
    "\n",
    "        self.positions = self.get_positions()\n",
    "        \n",
    "        if rle is not None:\n",
    "            self.mask = enc2mask(rle, (self.orig_size[1], self.orig_size[0])) > 0\n",
    "        else:\n",
    "            self.mask = None\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # Window\n",
    "        pos_x, pos_y = self.positions[idx]\n",
    "        x1, x2 = pos_x[0], pos_x[1]\n",
    "        y1, y2 = pos_y[0], pos_y[1]\n",
    "        window = Window.from_slices((x1, x2), (y1, y2))\n",
    "\n",
    "        # Retrieve slice\n",
    "        if self.original_img.count == 3:  # normal\n",
    "            img = self.original_img.read([1, 2, 3], window=window)\n",
    "            img = np.moveaxis(img, 0, -1)\n",
    "        else:  # with subdatasets/layers\n",
    "            img = np.zeros((self.tile_size, self.tile_size, 3), dtype=np.uint8)\n",
    "            for fl in range(3):\n",
    "                img[:, :, fl] = self.layers[fl].read(window=window) \n",
    "\n",
    "        # Downscale to tile size\n",
    "        img = cv2.resize(\n",
    "            img, (self.raw_tile_size, self.raw_tile_size), interpolation=cv2.INTER_AREA\n",
    "        )\n",
    "        img = self.transforms(image=img)[\"image\"]\n",
    "        \n",
    "        pos = np.array([pos_x[0], pos_x[1], pos_y[0], pos_y[1]])\n",
    "\n",
    "        return img, pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d202280b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_weights(model, filename, verbose=1, cp_folder=\"\"):\n",
    "    \"\"\"\n",
    "    Loads the weights of a PyTorch model. The exception handles cpu/gpu incompatibilities.\n",
    "\n",
    "    Args:\n",
    "        model (torch model): Model to load the weights to.\n",
    "        filename (str): Name of the checkpoint.\n",
    "        verbose (int, optional): Whether to display infos. Defaults to 1.\n",
    "        cp_folder (str, optional): Folder to load from. Defaults to \"\".\n",
    "\n",
    "    Returns:\n",
    "        torch model: Model with loaded weights.\n",
    "    \"\"\"\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\n -> Loading weights from {os.path.join(cp_folder,filename)}\\n\")\n",
    "    try:\n",
    "        model.load_state_dict(os.path.join(cp_folder, filename), strict=True)\n",
    "    except BaseException:\n",
    "        model.load_state_dict(\n",
    "            torch.load(os.path.join(cp_folder, filename), map_location=\"cpu\"),\n",
    "            strict=True,\n",
    "        )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20a9d206",
   "metadata": {},
   "outputs": [],
   "source": [
    "DECODERS = [\"Unet\", \"Linknet\", \"FPN\", \"PSPNet\", \"DeepLabV3\", \"DeepLabV3Plus\", \"PAN\"]\n",
    "ENCODERS = list(encoders.keys())\n",
    "\n",
    "def define_model(\n",
    "    decoder_name, encoder_name, num_classes=1, activation=None, encoder_weights=\"imagenet\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Loads a segmentation architecture\n",
    "\n",
    "    Args:\n",
    "        decoder_name (str): Decoder name.\n",
    "        encoder_name (str): Encoder name.\n",
    "        num_classes (int, optional): Number of classes. Defaults to 1.\n",
    "        pretrained : pretrained original weights\n",
    "    Returns:\n",
    "        torch model -- Pretrained model.\n",
    "    \"\"\"\n",
    "    assert decoder_name in DECODERS, \"Decoder name not supported\"\n",
    "    assert encoder_name in ENCODERS, \"Encoder name not supported\"\n",
    "\n",
    "    decoder = getattr(segmentation_models_pytorch, decoder_name)\n",
    "\n",
    "    model = decoder(\n",
    "        encoder_name,\n",
    "        encoder_weights=encoder_weights,\n",
    "        classes=num_classes,\n",
    "        activation=activation,\n",
    "    )\n",
    "    model.num_classes = num_classes\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11b206e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self, **entries):\n",
    "        self.__dict__.update(entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "44fb0e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(cp_folder):\n",
    "    config = json.load(open(cp_folder/'config.json', 'r'))\n",
    "    config = Config(**config)\n",
    "    \n",
    "    weights = sorted(glob.glob(str(cp_folder/'*.pt')))\n",
    "    models = []\n",
    "    \n",
    "    for weight in weights:\n",
    "        model = define_model(\n",
    "            config.decoder,\n",
    "            config.encoder,\n",
    "            num_classes=config.num_classes,\n",
    "            encoder_weights=None,\n",
    "        )\n",
    "        \n",
    "        model = load_model_weights(model, weight).to(DEVICE)\n",
    "        model.eval()\n",
    "        models.append(model)        \n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2c2191f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_scores_img(pred, truth, eps=1e-8):\n",
    "    \"\"\"\n",
    "    Dice metric for a single image as array.\n",
    "\n",
    "    Args:\n",
    "        pred (np array): Predictions.\n",
    "        truth (np array): Ground truths.\n",
    "        eps (float, optional): epsilon to avoid dividing by 0. Defaults to 1e-8.\n",
    "\n",
    "    Returns:\n",
    "        np array : dice value for each class\n",
    "    \"\"\"\n",
    "    pred = pred.reshape(-1) > 0\n",
    "    truth = truth.reshape(-1) > 0\n",
    "    intersect = (pred & truth).sum(-1)\n",
    "    union = pred.sum(-1) + truth.sum(-1)\n",
    "\n",
    "    dice = (2.0 * intersect + eps) / (union + eps)\n",
    "    return dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "103c8836",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tile_weighting(size, sigma=1, alpha=1, eps=1e-6):\n",
    "    half = size // 2\n",
    "    w = np.ones((size, size), np.float32)\n",
    "\n",
    "    x = np.concatenate([np.mgrid[-half:0], np.mgrid[1: half + 1]])[:, None]\n",
    "    x = np.tile(x, (1, size))\n",
    "    x = half + 1 - np.abs(x)\n",
    "    y = x.T\n",
    "\n",
    "    w = np.minimum(x, y)\n",
    "    w = (w / w.max()) ** sigma\n",
    "    w = np.minimum(w, 1)\n",
    "\n",
    "    w = (w - np.min(w) + eps) / (np.max(w) - np.min(w) + eps)\n",
    "\n",
    "    w = np.where(w > alpha, 1, w)\n",
    "    w = w / alpha\n",
    "    w = np.clip(w, 1e-3, 1)\n",
    "\n",
    "    w = np.round(w, 3)\n",
    "    return w.astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "faf9dd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_entire_mask(dataset, models, batch_size=32, tta=False):\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "    weighting = torch.from_numpy(get_tile_weighting(dataset.tile_size, sigma=1, alpha=1))\n",
    "    weighting_cuda = weighting.clone().cuda().unsqueeze(0)\n",
    "    weighting = weighting.cuda().half()\n",
    "\n",
    "    global_pred = torch.zeros(\n",
    "        (dataset.orig_size[0], dataset.orig_size[1]),\n",
    "        dtype=torch.half, device=\"cuda\"\n",
    "    )\n",
    "    global_counter = torch.zeros(\n",
    "        (dataset.orig_size[0], dataset.orig_size[1]),\n",
    "        dtype=torch.half, device=\"cuda\"\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img, pos in tqdm(loader):\n",
    "            img = img.to(\"cuda\")\n",
    "            _, _, h, w = img.shape\n",
    "            \n",
    "            model_preds = []\n",
    "            for model in models:\n",
    "                if model.num_classes == 1:\n",
    "                    pred = model(img).view(1, -1, h, w).sigmoid().detach()\n",
    "                else:\n",
    "                    pred = model(img)[:, 0].view(1, -1, h, w).sigmoid().detach()\n",
    "\n",
    "                if tta:\n",
    "                    for f in FLIPS:\n",
    "                        pred_flip = model(torch.flip(img, f))\n",
    "                        if model.num_classes == 2:\n",
    "                            pred_flip = pred_flip[:, :1]\n",
    "\n",
    "                        pred_flip = torch.flip(pred_flip, f).view(1, -1, h, w).sigmoid().detach()\n",
    "                        pred += pred_flip\n",
    "                    pred = torch.div(pred, len(FLIPS) + 1)\n",
    "\n",
    "                model_preds.append(pred)\n",
    "\n",
    "            pred = torch.cat(model_preds, 0).mean(0)\n",
    "\n",
    "            pred = torch.nn.functional.interpolate(\n",
    "                pred.unsqueeze(1), (dataset.tile_size, dataset.tile_size), mode='area'\n",
    "            ).squeeze(1)\n",
    "            \n",
    "            pred = (pred * weighting_cuda).half()\n",
    "\n",
    "            for tile_idx, (x0, x1, y0, y1) in enumerate(pos):\n",
    "                global_pred[x0: x1, y0: y1] += pred[tile_idx]\n",
    "                global_counter[x0: x1, y0: y1] += weighting\n",
    "\n",
    "    for i in range(len(global_pred)):\n",
    "        global_pred[i] = torch.div(global_pred[i], global_counter[i])\n",
    "\n",
    "    return global_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d2fead",
   "metadata": {},
   "source": [
    "<h6> Step 3 - Set configuration and paths </h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "717d277c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(r'C:/Users/soodn/Downloads/Naveksha/Kaggle HuBMAP')\n",
    "IMG_PATH = DATA_PATH/'Data/kidney-data/private test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c3186991",
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0.5\n",
    "\n",
    "USE_TTA = True # not DEBUG\n",
    "OVERLAP_FACTOR = 1.5\n",
    "\n",
    "CP_FOLDERS = [ Path(r'C:/Users/soodn/Downloads/Naveksha/Kaggle HuBMAP/Scripts/4. DeepLive/models/b1_2cfix/b1_2cfix'), Path(r'C:/Users/soodn/Downloads/Naveksha/Kaggle HuBMAP/Scripts/4. DeepLive/models//b1_last/b1_last')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "45b75f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH/'Data/kidney-data/sample_submission_pvt.csv')\n",
    "df_info = pd.read_csv(DATA_PATH/'Data/kidney-data/HuBMAP-20-dataset_information_pvt.csv')\n",
    "rles = pd.read_csv(DATA_PATH/'Data/kidney-data/private_test.csv')\n",
    "\n",
    "config = json.load(open(CP_FOLDERS[0]/'config.json', 'r'))\n",
    "config = Config(**config)\n",
    "config.overlap_factor = OVERLAP_FACTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "60df7694",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_test = DATA_PATH/'private test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ed753db2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -> Loading weights from C:\\Users\\soodn\\Downloads\\Naveksha\\Kaggle HuBMAP\\Scripts\\4. DeepLive\\models\\b1_2cfix\\b1_2cfix\\Unet_efficientnet-b1_0.pt\n",
      "\n",
      "\n",
      " -> Loading weights from C:\\Users\\soodn\\Downloads\\Naveksha\\Kaggle HuBMAP\\Scripts\\4. DeepLive\\models\\b1_2cfix\\b1_2cfix\\Unet_efficientnet-b1_1.pt\n",
      "\n",
      "\n",
      " -> Loading weights from C:\\Users\\soodn\\Downloads\\Naveksha\\Kaggle HuBMAP\\Scripts\\4. DeepLive\\models\\b1_2cfix\\b1_2cfix\\Unet_efficientnet-b1_2.pt\n",
      "\n",
      "\n",
      " -> Loading weights from C:\\Users\\soodn\\Downloads\\Naveksha\\Kaggle HuBMAP\\Scripts\\4. DeepLive\\models\\b1_2cfix\\b1_2cfix\\Unet_efficientnet-b1_3.pt\n",
      "\n",
      "\n",
      " -> Loading weights from C:\\Users\\soodn\\Downloads\\Naveksha\\Kaggle HuBMAP\\Scripts\\4. DeepLive\\models\\b1_2cfix\\b1_2cfix\\Unet_efficientnet-b1_4.pt\n",
      "\n",
      "\n",
      " -> Loading weights from C:\\Users\\soodn\\Downloads\\Naveksha\\Kaggle HuBMAP\\Scripts\\4. DeepLive\\models\\b1_last\\b1_last\\Unet_efficientnet-b1_0.pt\n",
      "\n",
      "\n",
      " -> Loading weights from C:\\Users\\soodn\\Downloads\\Naveksha\\Kaggle HuBMAP\\Scripts\\4. DeepLive\\models\\b1_last\\b1_last\\Unet_efficientnet-b1_1.pt\n",
      "\n",
      "\n",
      " -> Loading weights from C:\\Users\\soodn\\Downloads\\Naveksha\\Kaggle HuBMAP\\Scripts\\4. DeepLive\\models\\b1_last\\b1_last\\Unet_efficientnet-b1_2.pt\n",
      "\n",
      "\n",
      " -> Loading weights from C:\\Users\\soodn\\Downloads\\Naveksha\\Kaggle HuBMAP\\Scripts\\4. DeepLive\\models\\b1_last\\b1_last\\Unet_efficientnet-b1_3.pt\n",
      "\n",
      "\n",
      " -> Loading weights from C:\\Users\\soodn\\Downloads\\Naveksha\\Kaggle HuBMAP\\Scripts\\4. DeepLive\\models\\b1_last\\b1_last\\Unet_efficientnet-b1_4.pt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "\n",
    "for cp_folder in CP_FOLDERS:\n",
    "    models += load_models(cp_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7b1bda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t Image 00a67c839\n",
      "\n",
      " - Building dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soodn\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\rasterio\\__init__.py:219: NotGeoreferencedWarning: Dataset has no geotransform set. The identity matrix may be returned.\n",
      "  s = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n",
      "<ipython-input-7-51874a31b1a4>:12: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if isinstance(enc, np.float) and np.isnan(enc):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " - Predicting masks\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ab6a082b1c941c99b181bffe48f59f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " - Encoding\n",
      "\n",
      "\t Image 1eb18739d\n",
      "\n",
      " - Building dataset\n",
      "\n",
      " - Predicting masks\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d345ba4a781c4544b93b76719bec0c34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/46 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " - Encoding\n",
      "\n",
      "\t Image 5d8b53a68\n",
      "\n",
      " - Building dataset\n",
      "\n",
      " - Predicting masks\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89bb81437ee54e629eec1183c5509fdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " - Encoding\n",
      "\n",
      "\t Image 9e81e2693\n",
      "\n",
      " - Building dataset\n",
      "\n",
      " - Predicting masks\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f29ece956a2748aaa6633800badecb34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DEBUG = False\n",
    "for img in df['id'].unique():\n",
    "    if DEBUG:  # Check performances on a validation image\n",
    "        img = \"2f6ecfcdf\"  # check repro\n",
    "#         img = \"4ef6695ce\" # biggest img\n",
    "        IMG_PATH = DATA_PATH + \"train\"\n",
    "        models = [models[0], models[5]]\n",
    "                  \n",
    "    \n",
    "    print(f'\\n\\t Image {img}')\n",
    "    \n",
    "    print(f'\\n - Building dataset')\n",
    "    \n",
    "    rle_truth = rles[rles['id'] == img][\"expected\"]\n",
    "    \n",
    "    predict_dataset = InferenceEfficientDataset(\n",
    "        f\"{IMG_PATH}/{img}.tiff\",\n",
    "        rle=rle_truth,\n",
    "        overlap_factor=config.overlap_factor,\n",
    "        reduce_factor=config.reduce_factor,\n",
    "        tile_size=config.tile_size,\n",
    "        transforms=HE_preprocess(augment=False, visualize=False),\n",
    "    )\n",
    "    \n",
    "    print(f'\\n - Predicting masks')\n",
    "\n",
    "    global_pred = predict_entire_mask(\n",
    "        predict_dataset, models, batch_size=config.val_bs, tta=USE_TTA\n",
    "    )\n",
    "    \n",
    "    del predict_dataset\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    print('\\n - Encoding')\n",
    "    \n",
    "    global_pred_np = np.zeros(global_pred.size(), dtype=np.uint8)\n",
    "\n",
    "    for i in range(global_pred_np.shape[0]):\n",
    "        global_pred_np[i] = (global_pred[i] > THRESHOLD).cpu().numpy().astype(np.uint8)\n",
    "           \n",
    "    shape = df_info[df_info.image_file == img][['width_pixels', 'height_pixels']].values.astype(int)[0]\n",
    "    \n",
    "    rle = rle_encode_less_memory(global_pred_np)\n",
    "    df.loc[df.id == img, 'predicted'] = rle\n",
    "    \n",
    "    if DEBUG:\n",
    "        shape = df_info[df_info.image_file == img + \".tiff\"][['width_pixels', 'height_pixels']].values.astype(int)[0]\n",
    "        mask_truth = enc2mask(rle_truth, shape)\n",
    "        score = dice_scores_img(global_pred_np, mask_truth)\n",
    "        print(f\" -> Scored {score :.4f} with threshold {THRESHOLD:.2f}\")\n",
    "        break\n",
    "        \n",
    "    del global_pred, global_pred_np\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "df.to_csv('submission_kidney_pvt_deeplive.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "859bbe03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00a67c839</td>\n",
       "      <td>32616323 22 32646723 22 32677095 64 32677171 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1eb18739d</td>\n",
       "      <td>53886761 28 53907090 28 53927359 108 53947688 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5d8b53a68</td>\n",
       "      <td>66553749 6 66553775 30 66575902 6 66575926 1 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9e81e2693</td>\n",
       "      <td>202687491 14 202715133 14 202742773 20 2027704...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0749c6ccc</td>\n",
       "      <td>415945765 10 415976133 10 416006495 28 4160368...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5274ef79a</td>\n",
       "      <td>27721013 24 27743147 24 27765273 42 27787407 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>a14e495cf</td>\n",
       "      <td>111097909 82 111160597 82 111223275 110 111285...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bacb03928</td>\n",
       "      <td>48379381 6 48403349 6 48523189 2 48547157 2 48...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>e464d2f6c</td>\n",
       "      <td>120555115 26 120605675 26 120656229 42 1207067...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ff339c0b2</td>\n",
       "      <td>147895809 42 147944353 40 147992889 56 1480414...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                          predicted\n",
       "0  00a67c839  32616323 22 32646723 22 32677095 64 32677171 1...\n",
       "1  1eb18739d  53886761 28 53907090 28 53927359 108 53947688 ...\n",
       "2  5d8b53a68  66553749 6 66553775 30 66575902 6 66575926 1 6...\n",
       "3  9e81e2693  202687491 14 202715133 14 202742773 20 2027704...\n",
       "4  0749c6ccc  415945765 10 415976133 10 416006495 28 4160368...\n",
       "5  5274ef79a  27721013 24 27743147 24 27765273 42 27787407 4...\n",
       "6  a14e495cf  111097909 82 111160597 82 111223275 110 111285...\n",
       "7  bacb03928  48379381 6 48403349 6 48523189 2 48547157 2 48...\n",
       "8  e464d2f6c  120555115 26 120605675 26 120656229 42 1207067...\n",
       "9  ff339c0b2  147895809 42 147944353 40 147992889 56 1480414..."
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc95dca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
