{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "#default_exp learner\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learner\n",
    "\n",
    "> Implements functions necessary to build an  `EnsembleLearner` suitable for bioimgage segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import shutil, gc, joblib, json, zarr, numpy as np, pandas as pd\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from fastprogress import progress_bar\n",
    "from fastcore.basics import patch, GetAttr\n",
    "from fastcore.foundation import add_docs, L\n",
    "from fastai import optimizer\n",
    "from fastai.torch_core import TensorImage\n",
    "from fastai.learner import Learner\n",
    "from fastai.callback.tracker import SaveModelCallback\n",
    "from fastai.data.core import DataLoaders\n",
    "from fastai.data.transforms import get_image_files, get_files, Normalize\n",
    "from fastai.vision.augment import Brightness, Contrast, Saturation\n",
    "from fastai.losses import CrossEntropyLossFlat\n",
    "\n",
    "from deepflash2.metrics import Dice_f1, Iou\n",
    "from deepflash2.losses import WeightedSoftmaxCrossEntropy,load_kornia_loss\n",
    "from deepflash2.callbacks import ElasticDeformCallback\n",
    "from deepflash2.models import get_default_shapes, load_smp_model\n",
    "from deepflash2.data import TileDataset, RandomTileDataset, _read_img, _read_msk\n",
    "from deepflash2.utils import iou, plot_results, get_label_fn, calc_iterations, save_mask, save_unc\n",
    "import deepflash2.tta as tta\n",
    "from deepflash2.transforms import WeightTransform, calculate_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"Config class for settings.\"\n",
    "\n",
    "    # Project\n",
    "    proj_dir:str = 'deepflash2'\n",
    "\n",
    "    # GT Estimation Settings\n",
    "    staple_thres:float = 0.5\n",
    "    staple_fval:int= 1\n",
    "    mv_undec:int = 0\n",
    "\n",
    "    # Train General Settings\n",
    "    n:int = 4\n",
    "    max_splits:int=5\n",
    "    repo:str = 'matjesg/deepflash2'\n",
    "    arch:str = 'unext50_deepflash2'\n",
    "    pretrained:str = None\n",
    "    random_state:int = 42\n",
    "        \n",
    "    # Pytorch Segmentation Model Settings\n",
    "    encoder_name:str = 'efficientnet-b4'\n",
    "    encoder_weights:str = 'imagenet'\n",
    "\n",
    "    # Train Data Settings\n",
    "    c:int = 2\n",
    "    il:bool = False\n",
    "\n",
    "    # Train Settings\n",
    "    lr:float = 0.001\n",
    "    bs:int = 4\n",
    "    wd:float = 0.001\n",
    "    mpt:bool = False\n",
    "    optim:str = 'ranger'\n",
    "    loss:str = 'WeightedSoftmaxCrossEntropy'\n",
    "    n_iter:int = 1000\n",
    "\n",
    "    # Train Validation Settings\n",
    "    tta:bool = True\n",
    "\n",
    "    # Train Data Augmentation\n",
    "    saturation_max_lighting:float = 0.0\n",
    "    contrast_max_lighting:float = 0.0\n",
    "    brightness_max_lighting:float = 0.1\n",
    "    zoom_sigma:float = 0.0\n",
    "    flip:bool = True\n",
    "    rot:int = 360\n",
    "    deformation_grid:int = 150\n",
    "    deformation_magnitude:int = 10\n",
    "        \n",
    "    # Loss Settings Kornia\n",
    "    loss_alpha:float = 0.5 # Twerksky/Focal loss\n",
    "    loss_beta:float = 0.5 # Twerksy Loss\n",
    "    loss_gamma:float = 2.0 # Focal loss\n",
    "    \n",
    "    # Loss Mask Weights (WeightedSoftmaxCrossEntropy)\n",
    "    bwf:int = 25\n",
    "    bws:int = 10\n",
    "    fds:int = 10\n",
    "    fbr:float = 0.5\n",
    "\n",
    "    # Pred Settings\n",
    "    pred_tta:bool = True\n",
    "    extra_padding:int = 100\n",
    "\n",
    "    # OOD Settings\n",
    "    kernel:str = 'rbf'\n",
    "    nu:float = 0.01\n",
    "    gamma:float = 0.01\n",
    "    energy_ks:int = 20\n",
    "\n",
    "    # Folder Structure\n",
    "    gt_dir:str = 'GT_Estimation'\n",
    "    train_dir:str = 'Training'\n",
    "    pred_dir:str = 'Prediction'\n",
    "    ens_dir:str = 'ensemble'\n",
    "    val_dir:str = 'valid'\n",
    "\n",
    "    @property\n",
    "    def mw_kwargs(self):\n",
    "        mw_vars = ['bwf', 'bws', 'fds', 'fbr']\n",
    "        return dict(filter(lambda x: x[0] in mw_vars, self.__dict__.items()))\n",
    "\n",
    "    @property\n",
    "    def svm_kwargs(self):\n",
    "        svm_vars = ['kernel', 'nu', 'gamma']\n",
    "        return dict(filter(lambda x: x[0] in svm_vars, self.__dict__.items()))\n",
    "\n",
    "    def save(self, path):\n",
    "        'Save configuration to path'\n",
    "        path = Path(path)\n",
    "        with open(path.with_suffix('.json'), 'w') as config_file:\n",
    "            json.dump(asdict(self), config_file)\n",
    "        print(f'Saved current configuration to {path}.json')\n",
    "\n",
    "    def load(self, path):\n",
    "        'Load configuration from path'\n",
    "        path = Path(path)\n",
    "        try:\n",
    "            with open(path) as config_file: c = json.load(config_file)\n",
    "            if not Path(c['proj_dir']).is_dir(): c['proj_dir']='deepflash2'\n",
    "            for k,v in c.items(): setattr(self, k, v)\n",
    "            print(f'Successsfully loaded configuration from {path}')\n",
    "        except:\n",
    "            print('Error! Select valid config file (.json)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved current configuration to test_config.json\n",
      "Successsfully loaded configuration from test_config.json\n"
     ]
    }
   ],
   "source": [
    "t1 = Config(n=3)\n",
    "t1.save('test_config')\n",
    "t2 = Config()\n",
    "t2.load('test_config.json')\n",
    "test_eq(t1, t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "_optim_dict = {\n",
    "    'ranger' : optimizer.ranger,\n",
    "    'Adam' : optimizer.Adam,\n",
    "    'RAdam' : optimizer.RAdam,\n",
    "    'QHAdam' :optimizer.QHAdam,\n",
    "    'Larc' : optimizer.Larc,\n",
    "    'Lamb' : optimizer.Lamb,\n",
    "    'SGD' : optimizer.SGD,\n",
    "    'RMSProp' : optimizer.RMSProp,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patches for the `fastai` Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def apply_dropout(self:Learner):\n",
    "    \"If a module contains 'dropout', it will be switched to .train() mode.\"\n",
    "    for m in self.model.modules():\n",
    "        if isinstance(m, nn.Dropout):\n",
    "            m.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def energy_max(e, ks=20, dim=None):\n",
    "    e = torch.as_tensor(e).resize_((1,1,*e.shape))\n",
    "    e = F.avg_pool2d(e, ks)\n",
    "    return torch.max(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = np.random.randn(1024,1024)\n",
    "test_close(energy_max(e, ks=100),0, eps=1e-01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def predict_tiles(self:Learner, ds_idx=1, dl=None, path=None, mc_dropout=False, n_times=1, use_tta=False, \n",
    "                       tta_merge='mean', tta_tfms=None, uncertainty_estimates=True, energy_T=1):\n",
    "    \"Make predictions and reconstruct tiles, optional with dropout and/or tta applied.\"\n",
    "\n",
    "    if dl is None: dl = self.dls[ds_idx].new(shuffled=False, drop_last=False)\n",
    "    assert isinstance(dl.dataset, TileDataset), \"Provide dataloader containing a TileDataset\"\n",
    "    if use_tta: tfms = tta_tfms or [tta.HorizontalFlip(), tta.Rotate90(angles=[90,180,270])]\n",
    "    else: tfms=[]\n",
    "\n",
    "    self.model.eval()\n",
    "    if mc_dropout: self.apply_dropout()\n",
    "  \n",
    "    store = str(path) if path else zarr.storage.TempStore()\n",
    "    root = zarr.group(store=store, overwrite=True)\n",
    "    g_smx, g_seg, g_std, g_eng  = root.create_groups('smx', 'seg', 'std', 'energy')\n",
    "    \n",
    "    i = 0\n",
    "    last_file = None\n",
    "    for data in progress_bar(dl, leave=False):\n",
    "        if isinstance(data, TensorImage): images = data\n",
    "        else: images, _, _ = data\n",
    "        m_smx = tta.Merger()\n",
    "        m_energy = tta.Merger()\n",
    "        out_list_smx = []\n",
    "        for t in tta.Compose(tfms):\n",
    "            for _ in range(n_times):\n",
    "                aug_images = t.augment_image(images)\n",
    "                with torch.no_grad():\n",
    "                    out = self.model(aug_images)\n",
    "                out = t.deaugment_mask(out)\n",
    "                if dl.padding[0]!= images.shape[-1]-out.shape[-1]: \n",
    "                    padding = ((images.shape[-1]-out.shape[-1]-dl.padding[0])//2,)*4\n",
    "                    out = F.pad(out, padding)                \n",
    "                m_smx.append(F.softmax(out, dim=1))\n",
    "                if uncertainty_estimates:\n",
    "                    e = (energy_T*torch.logsumexp(out/energy_T, dim=1)) #negative energy score\n",
    "                    m_energy.append(e)\n",
    "        \n",
    "        ll = []\n",
    "        ll.append([x for x in m_smx.result().permute(0,2,3,1).cpu().numpy()])\n",
    "        if uncertainty_estimates:\n",
    "            ll.append([x for x in torch.mean(m_smx.result('std'), 1).cpu().numpy()])\n",
    "            ll.append([x for x in m_energy.result().cpu().numpy()])\n",
    "        for j, preds in enumerate(zip(*ll)):\n",
    "            if len(preds)==3: smx,std,eng = preds\n",
    "            else: smx = preds[0]\n",
    "            idx = i+j\n",
    "            f = dl.files[dl.image_indices[idx]]\n",
    "            outShape = dl.image_shapes[idx]\n",
    "            outSlice = dl.out_slices[idx]\n",
    "            inSlice = dl.in_slices[idx]\n",
    "            if last_file!=f: \n",
    "                z_smx = g_smx.empty(f.name, shape=(*outShape, dl.c), dtype='float32')\n",
    "                z_seg = g_seg.empty(f.name, shape=outShape, dtype='uint8')\n",
    "                z_std = g_std.empty(f.name, shape=outShape, dtype='float32')\n",
    "                z_eng = g_eng.empty(f.name, shape=outShape, dtype='float32')    \n",
    "                last_file = f\n",
    "            z_smx[outSlice] = smx[inSlice]\n",
    "            z_seg[outSlice] = np.argmax(smx, axis=-1)[inSlice]\n",
    "            if uncertainty_estimates:\n",
    "                z_std[outSlice] = std[inSlice]\n",
    "                z_eng[outSlice] = eng[inSlice]\n",
    "        i += dl.bs\n",
    "\n",
    "    return g_smx, g_seg, g_std, g_eng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class EnsembleLearner(GetAttr):\n",
    "    _default = 'config' \n",
    "    def __init__(self, image_dir='images', mask_dir=None, config=None, path=None, ensemble_dir=None, \n",
    "                 label_fn=None, metrics=None, cbs=None, ds_kwargs={}, dl_kwargs={}, model_kwargs={}, loss_kwargs={}, stats=None, files=None):\n",
    "\n",
    "        self.config = config or Config()\n",
    "        self.stats = stats \n",
    "        self.dl_kwargs = dl_kwargs\n",
    "        self.model_kwargs = model_kwargs\n",
    "        self.loss_kwargs = loss_kwargs\n",
    "        self.add_ds_kwargs = ds_kwargs\n",
    "        self.path = Path(path) if path is not None else Path('.')\n",
    "        self.metrics = metrics or [Iou(), Dice_f1()]\n",
    "        self.loss_fn = self.get_loss()\n",
    "        self.cbs = cbs or [SaveModelCallback(monitor='iou'), ElasticDeformCallback] #ShowGraphCallback\n",
    "        self.ensemble_dir = ensemble_dir or self.path/'ensemble'    \n",
    "        \n",
    "        self.files = L(files) or get_image_files(self.path/image_dir, recurse=False)\n",
    "        assert len(self.files)>0, f'Found {len(self.files)} images in \"{image_dir}\". Please check your images and image folder'\n",
    "        if any([mask_dir, label_fn]):\n",
    "            if label_fn: self.label_fn = label_fn\n",
    "            else: self.label_fn = get_label_fn(self.files[0], self.path/mask_dir)\n",
    "            #Check if corresponding masks exist\n",
    "            mask_check = [self.label_fn(x).exists() for x in self.files]\n",
    "            chk_str = f'Found {len(self.files)} images in \"{image_dir}\" and {sum(mask_check)} masks in \"{mask_dir}\".'\n",
    "            assert len(self.files)==sum(mask_check) and len(self.files)>0, f'Please check your images and masks (and folders). {chk_str}'\n",
    "            print(chk_str)\n",
    "                  \n",
    "        else:\n",
    "            self.label_fn = label_fn\n",
    "        self.n_splits=min(len(self.files), self.max_splits)\n",
    "          \n",
    "        # Transforms (Data Augmentation)\n",
    "        self.item_tfms=[Saturation(max_lighting=self.saturation_max_lighting, p=0.75, draw=None, batch=False), \n",
    "                        Contrast(max_lighting=self.contrast_max_lighting, p=0.75, draw=None, batch=False),\n",
    "                        Brightness(max_lighting=self.brightness_max_lighting, p=0.75, draw=None, batch=False)]\n",
    "        self.models = {}\n",
    "        self.recorder = {}\n",
    "        self._set_splits()\n",
    "        self.ds = RandomTileDataset(self.files, label_fn=self.label_fn, **self.mw_kwargs, **self.ds_kwargs)\n",
    "        self.in_channels = self.ds.get_data(max_n=1)[0].shape[-1]\n",
    "        self.df_val, self.df_ens, self.df_model, self.ood = None,None,None,None\n",
    "    \n",
    "    @property\n",
    "    def out_size(self):\n",
    "        return self.ds_kwargs['tile_shape'][0]-self.ds_kwargs['padding'][0]\n",
    "           \n",
    "    def _set_splits(self):\n",
    "        if self.n_splits>1:\n",
    "            kf = KFold(self.n_splits, shuffle=True, random_state=self.random_state)\n",
    "            self.splits = {key:(self.files[idx[0]], self.files[idx[1]]) for key, idx in zip(range(1,self.n_splits+1), kf.split(self.files))}    \n",
    "        else:\n",
    "            self.splits = {1: (self.files[0], self.files[0])}\n",
    "    \n",
    "    @property        \n",
    "    def ds_kwargs(self):\n",
    "        # Setting default shapes and padding\n",
    "        ds_kwargs = self.add_ds_kwargs.copy()\n",
    "        for key, value in get_default_shapes(self.arch).items():\n",
    "            ds_kwargs.setdefault(key, value)\n",
    "        # Settings from config\n",
    "        ds_kwargs['loss_weights'] = True if self.loss=='WeightedSoftmaxCrossEntropy' else False\n",
    "        ds_kwargs['zoom_sigma'] = self.zoom_sigma\n",
    "        ds_kwargs['flip'] = self.flip\n",
    "        ds_kwargs['deformation_grid']= (self.deformation_grid,)*2\n",
    "        ds_kwargs['deformation_magnitude'] = (self.deformation_magnitude,)*2\n",
    "        return ds_kwargs\n",
    "            \n",
    "    def get_loss(self):\n",
    "        if self.loss == 'WeightedSoftmaxCrossEntropy': return WeightedSoftmaxCrossEntropy(axis=1)\n",
    "        if self.loss == 'CrossEntropyLoss': return CrossEntropyLossFlat(axis=1)\n",
    "        else: \n",
    "            kwargs = {'alpha':self.loss_alpha, 'beta':self.loss_beta, 'gamma':self.loss_gamma}\n",
    "            return load_kornia_loss(self.loss, **kwargs)\n",
    "            \n",
    "    def get_model(self, pretrained):\n",
    "        if self.arch in [\"unet_deepflash2\",  \"unet_falk2019\", \"unet_ronnberger2015\", \"unet_custom\", \"unext50_deepflash2\"]:\n",
    "            model = torch.hub.load(self.repo, self.arch, pretrained=pretrained, n_classes=dls.c, in_channels=self.in_channels, **self.model_kwargs)\n",
    "        else:\n",
    "            kwargs = dict(encoder_name=self.encoder_name, encoder_weights=self.encoder_weights, \n",
    "                          in_channels=self.in_channels, classes=self.c, **self.model_kwargs)\n",
    "            model = load_smp_model(self.arch, **kwargs)\n",
    "        return model\n",
    "        \n",
    "    def save_model(self, file, model, pickle_protocol=2):\n",
    "        state = model.state_dict()\n",
    "        state = {'model': state, 'arch':self.arch, 'repo':self.repo, 'stats':self.stats, 'c':self.c}\n",
    "        torch.save(state, file, pickle_protocol=pickle_protocol, _use_new_zipfile_serialization=False)\n",
    "    \n",
    "    def load_model(self, file, with_meta=True, device=None, strict=True):\n",
    "        if isinstance(device, int): device = torch.device('cuda', device)\n",
    "        elif device is None: device = 'cpu'\n",
    "        state = torch.load(file, map_location=device)\n",
    "        hasopt = set(state)=={'model', 'arch', 'repo', 'stats', 'c'}\n",
    "        if hasopt:\n",
    "            model_state = state['model']\n",
    "            if with_meta:\n",
    "                self.config.arch = state['arch']\n",
    "                self.config.repo = state['repo']\n",
    "                self.config.c  = state['c']\n",
    "                self.stats = state['stats']\n",
    "        else:\n",
    "            model_state = state                \n",
    "        model = self.get_model(pretrained=None)\n",
    "        model.load_state_dict(model_state, strict=strict)\n",
    "        return model\n",
    "    \n",
    "    def get_batch_tfms(self):\n",
    "        tfms = [Normalize.from_stats(*self.stats)]\n",
    "        if isinstance(self.loss_fn, WeightedSoftmaxCrossEntropy):\n",
    "            tfms.append(WeightTransform(self.out_size, **self.mw_kwargs))\n",
    "        return tfms\n",
    "        \n",
    "    def fit(self, i, n_iter=None, lr_max=None, bs=None, **kwargs):\n",
    "        n_iter = n_iter or self.n_iter\n",
    "        lr_max = lr_max or self.lr\n",
    "        bs = bs or self.bs\n",
    "        self.stats = self.stats or self.ds.compute_stats()\n",
    "        name = self.ensemble_dir/f'{self.arch}_model-{i}.pth'\n",
    "        files_train, files_val = self.splits[i]\n",
    "        train_ds = RandomTileDataset(files_train, label_fn=self.label_fn, **self.mw_kwargs, **self.ds_kwargs)\n",
    "        valid_ds = TileDataset(files_val, label_fn=self.label_fn, **self.mw_kwargs,**self.ds_kwargs)\n",
    "        dls = DataLoaders.from_dsets(train_ds, valid_ds, bs=bs, after_item=self.item_tfms, after_batch=self.get_batch_tfms(), **self.dl_kwargs)\n",
    "        pre = None if self.pretrained=='new' else self.pretrained\n",
    "        model = self.get_model(pretrained=pre)\n",
    "        if torch.cuda.is_available(): dls.cuda(), model.cuda()\n",
    "        self.learn = Learner(dls, model, metrics=self.metrics, wd=self.wd, loss_func=self.loss_fn, opt_func=_optim_dict[self.optim], cbs=self.cbs)\n",
    "        self.learn.model_dir = self.ensemble_dir.parent/'.tmp'\n",
    "        if self.mpt: self.learn.to_fp16()\n",
    "        print(f'Starting training for {name.name}')\n",
    "        epochs = calc_iterations(n_iter=n_iter,ds_length=len(train_ds), bs=bs)\n",
    "        self.learn.fit_one_cycle(epochs, lr_max)\n",
    "\n",
    "        print(f'Saving model at {name}')\n",
    "        name.parent.mkdir(exist_ok=True, parents=True)\n",
    "        self.save_model(name, self.learn.model)\n",
    "        self.models[i]=name\n",
    "        self.recorder[i]=self.learn.recorder\n",
    "        #del model\n",
    "        #gc.collect()\n",
    "        #torch.cuda.empty_cache()\n",
    "            \n",
    "        \n",
    "    def fit_ensemble(self, n_iter, skip=False, **kwargs):\n",
    "        for i in range(1, self.n+1):\n",
    "            if skip and (i in self.models): continue\n",
    "            self.fit(i, n_iter,  **kwargs)\n",
    "       \n",
    "    def set_n(self, n):\n",
    "        for i in range(n, len(self.models)):\n",
    "            self.models.pop(i+1, None)            \n",
    "        self.n = n\n",
    "                 \n",
    "    def predict(self, files, model_no, bs=None, path=None, **kwargs):\n",
    "        bs = bs or self.bs\n",
    "        model_path = self.models[model_no]\n",
    "        model = self.load_model(model_path)\n",
    "        ds_kwargs = self.ds_kwargs\n",
    "        # Adding extra padding (overlap) for models that have the same input and output shape\n",
    "        if ds_kwargs['padding'][0]==0: ds_kwargs['padding'] = (self.extra_padding,)*2\n",
    "        ds = TileDataset(files, **ds_kwargs)\n",
    "        dls = DataLoaders.from_dsets(ds, batch_size=bs, after_batch=self.get_batch_tfms(), shuffle=False, drop_last=False, **self.dl_kwargs)\n",
    "        if torch.cuda.is_available(): dls.cuda(), model.cuda()\n",
    "        learn = Learner(dls, model, loss_func=self.loss_fn)\n",
    "        if self.mpt: learn.to_fp16()\n",
    "        if path: path = path/f'model_{model_no}'\n",
    "        return learn.predict_tiles(dl=dls.train, path=path, **kwargs)\n",
    "                               \n",
    "    def get_valid_results(self, model_no=None, export_dir=None, filetype='.png', **kwargs):\n",
    "        res_list = []\n",
    "        model_list = self.models if not model_no else [model_no]\n",
    "        if export_dir: \n",
    "            export_dir = Path(export_dir)\n",
    "            pred_path = export_dir/'masks'\n",
    "            pred_path.mkdir(parents=True, exist_ok=True)\n",
    "            if self.tta:\n",
    "                unc_path = export_dir/'uncertainties'\n",
    "                unc_path.mkdir(parents=True, exist_ok=True)\n",
    "        for i in model_list:\n",
    "            _, files_val = self.splits[i]\n",
    "            g_smx, g_seg, g_std, g_eng = self.predict(files_val, i, **kwargs)\n",
    "            chunk_store = g_smx.chunk_store.path\n",
    "            for j, f in enumerate(files_val):\n",
    "                msk = self.ds.get_data(f, mask=True)[0]\n",
    "                pred = g_seg[f.name][:]\n",
    "                m_iou = iou(msk, pred)\n",
    "                m_path = self.models[i].name\n",
    "                m_eng_max = energy_max(g_eng[f.name][:], ks=self.energy_ks)\n",
    "                df_tmp = pd.Series({'file' : f.name,\n",
    "                        'model' :  m_path,\n",
    "                        'model_no' : i,\n",
    "                        'img_path': f,\n",
    "                        'iou': m_iou,\n",
    "                        'energy_max': m_eng_max.numpy(),\n",
    "                        'msk_path': self.label_fn(f),\n",
    "                        'pred_path': f'{chunk_store}/{g_seg.path}/{f.name}',\n",
    "                        'smx_path': f'{chunk_store}/{g_smx.path}/{f.name}',\n",
    "                        'std_path': f'{chunk_store}/{g_std.path}/{f.name}'})\n",
    "                res_list.append(df_tmp)\n",
    "                if export_dir:   \n",
    "                    save_mask(pred, pred_path/f'{df_tmp.file}_{df_tmp.model}_mask', filetype)\n",
    "                    if self.tta:\n",
    "                        save_unc(g_std[f.name][:], unc_path/f'{df_tmp.file}_{df_tmp.model}_unc', filetype)\n",
    "        self.df_val = pd.DataFrame(res_list)\n",
    "        if export_dir: self.df_val.to_csv(export_dir/f'val_results.csv', index=False)\n",
    "        return self.df_val\n",
    "        \n",
    "    def show_valid_results(self, model_no=None, files=None, **kwargs):\n",
    "        if self.df_val is None: self.get_valid_results(**kwargs)\n",
    "        df = self.df_val\n",
    "        if files is not None: df = df.set_index('file', drop=False).loc[files]\n",
    "        if model_no is not None: df = df[df.model_no==model_no] \n",
    "        for _, r in df.iterrows():\n",
    "            img = self.ds.get_data(r.img_path)[0][:]\n",
    "            msk = self.ds.get_data(r.img_path, mask=True)[0]\n",
    "            pred = zarr.load(r.pred_path)\n",
    "            std = zarr.load(r.std_path)\n",
    "            _d_model = f'Model {r.model_no}'\n",
    "            if self.tta: plot_results(img, msk, pred, std, df=r, model=_d_model)  \n",
    "            else: plot_results(img, msk, pred, np.zeros_like(pred), df=r, model=_d_model)  \n",
    "          \n",
    "    def load_ensemble(self, path=None):\n",
    "        path = path or self.ensemble_dir\n",
    "        models = get_files(path, extensions='.pth', recurse=False)\n",
    "        assert len(models)>0, f'No models found in {path}'\n",
    "        self.models = {}\n",
    "        for m in models:\n",
    "            model_id = int(m.stem[-1])\n",
    "            self.models[model_id] = m\n",
    "        print(f'Found {len(self.models)} models in folder {path}')\n",
    "        print(self.models)\n",
    "            \n",
    "    def ensemble_results(self, files, path=None, export_dir=None, filetype='.png', use_tta=None, **kwargs):\n",
    "        use_tta = use_tta or self.pred_tta\n",
    "        if export_dir: \n",
    "            export_dir = Path(export_dir)\n",
    "            pred_path = export_dir/'masks'\n",
    "            pred_path.mkdir(parents=True, exist_ok=True)\n",
    "            if use_tta:\n",
    "                unc_path = export_dir/'uncertainties'\n",
    "                unc_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        store = str(path/'ensemble') if path else zarr.storage.TempStore()\n",
    "        root = zarr.group(store=store, overwrite=True)\n",
    "        chunk_store = root.chunk_store.path\n",
    "        g_smx, g_seg, g_std, g_eng  = root.create_groups('ens_smx', 'ens_seg', 'ens_std', 'ens_energy')\n",
    "        res_list = []\n",
    "        for f in files:\n",
    "            df_fil = self.df_models[self.df_models.file==f.name]\n",
    "            assert len(df_fil)==len(self.models), \"Predictions and models to not match.\"\n",
    "            m_smx, m_std, m_eng = tta.Merger(), tta.Merger(), tta.Merger()\n",
    "            for idx, r in df_fil.iterrows():\n",
    "                m_smx.append(zarr.load(r.smx_path))\n",
    "                m_std.append(zarr.load(r.std_path))\n",
    "                m_eng.append(zarr.load(r.eng_path))\n",
    "            smx = m_smx.result().numpy()\n",
    "            g_smx[f.name] = smx\n",
    "            g_seg[f.name] = np.argmax(smx, axis=-1)\n",
    "            g_std[f.name] = m_std.result().numpy()\n",
    "            eng = m_eng.result()\n",
    "            g_eng[f.name] = eng.numpy()\n",
    "            m_eng_max = energy_max(eng, ks=self.energy_ks).numpy()\n",
    "            df_tmp = pd.Series({'file' : f.name,\n",
    "                                'model' :  f'{self.arch}_ensemble',\n",
    "                                'energy_max': m_eng_max,\n",
    "                                'img_path': f,\n",
    "                                'pred_path': f'{chunk_store}/{g_seg.path}/{f.name}',\n",
    "                                'smx_path': f'{chunk_store}/{g_smx.path}/{f.name}',\n",
    "                                'std_path': f'{chunk_store}/{g_std.path}/{f.name}',\n",
    "                                'eng_path': f'{chunk_store}/{g_eng.path}/{f.name}'})\n",
    "            res_list.append(df_tmp)\n",
    "            if export_dir:   \n",
    "                save_mask(g_seg[f.name][:], pred_path/f'{df_tmp.file}_{df_tmp.model}_mask', filetype)\n",
    "                if use_tta:\n",
    "                    save_unc(g_std[f.name][:], unc_path/f'{df_tmp.file}_{df_tmp.model}_unc', filetype)\n",
    "        return pd.DataFrame(res_list)\n",
    "                            \n",
    "    def get_ensemble_results(self, new_files, export_dir=None, filetype='.png', **kwargs):   \n",
    "        res_list = []\n",
    "        for i in self.models:\n",
    "            g_smx, g_seg, g_std, g_eng = self.predict(new_files, i, **kwargs)\n",
    "            chunk_store = g_smx.chunk_store.path\n",
    "            for j, f in enumerate(new_files):\n",
    "                m_path = self.models[i].name\n",
    "                df_tmp = pd.Series({'file' : f.name,\n",
    "                                    'model_no': i, \n",
    "                                    'model' :  m_path,\n",
    "                                    'img_path': f,\n",
    "                                    'pred_path': f'{chunk_store}/{g_seg.path}/{f.name}',\n",
    "                                    'smx_path': f'{chunk_store}/{g_smx.path}/{f.name}',\n",
    "                                    'std_path': f'{chunk_store}/{g_std.path}/{f.name}',\n",
    "                                    'eng_path': f'{chunk_store}/{g_eng.path}/{f.name}'})\n",
    "                res_list.append(df_tmp)\n",
    "        self.df_models = pd.DataFrame(res_list)\n",
    "        self.df_ens  = self.ensemble_results(new_files, export_dir=export_dir, filetype=filetype, **kwargs)\n",
    "        return self.df_ens\n",
    "    \n",
    "    def show_ensemble_results(self, files=None, model_no=None, unc=True, unc_metric=None):\n",
    "        if self.df_ens is None: assert print(\"Please run `get_ensemble_results` first.\")\n",
    "        if model_no is None: df = self.df_ens\n",
    "        else: df = self.df_models[df_models.model_no==model_no]\n",
    "        if files is not None: df = df.set_index('file', drop=False).loc[files]\n",
    "        for _, r in df.iterrows():\n",
    "            img = _read_img(r.img_path)[:]\n",
    "            pred = zarr.load(r.pred_path)\n",
    "            std = zarr.load(r.std_path)\n",
    "            if unc: plot_results(img, pred, std, df=r, unc_metric=unc_metric) \n",
    "            else: plot_results(img, pred, df=r)  \n",
    "                \n",
    "    def lr_find(self, files=None, bs=None, **kwargs):\n",
    "        bs = bs or self.bs\n",
    "        files = files or self.files\n",
    "        train_ds = RandomTileDataset(files, label_fn=self.label_fn, **self.mw_kwargs, **self.ds_kwargs)\n",
    "        dls = DataLoaders.from_dsets(train_ds, train_ds, bs=bs, **self.dl_kwargs)\n",
    "        pre = None if self.pretrained=='new' else self.pretrained\n",
    "        model = self.get_model(pretrained=pre)\n",
    "        if torch.cuda.is_available(): dls.cuda(), model.cuda()\n",
    "        learn = Learner(dls, model, metrics=self.metrics, wd=self.wd, loss_func=self.loss_fn, opt_func=_optim_dict[self.optim])\n",
    "        if self.mpt: learn.to_fp16()\n",
    "        sug_lrs = learn.lr_find(**kwargs)\n",
    "        return sug_lrs, learn.recorder  \n",
    "    \n",
    "    def show_mask_weights(self, files, figsize=(12,12), **kwargs):\n",
    "        masks = [self.label_fn(Path(f)) for f in files]\n",
    "        for m in masks:\n",
    "            print(self.mw_kwargs)\n",
    "            print(f'Calculating weights. Please wait...')\n",
    "            msk = _read_msk(m)\n",
    "            _, w, _ = calculate_weights(msk, n_dims=self.c, **self.mw_kwargs)\n",
    "            fig, axes = plt.subplots(nrows=1, ncols=2, figsize=figsize, **kwargs)\n",
    "            axes[0].imshow(msk)\n",
    "            axes[0].set_axis_off()\n",
    "            axes[0].set_title(f'Mask {m.name}')\n",
    "            axes[1].imshow(w)\n",
    "            axes[1].set_axis_off()\n",
    "            axes[1].set_title('Weights')\n",
    "            plt.show()\n",
    "    \n",
    "    def ood_train(self, features=['energy_max'], **kwargs):\n",
    "        self.ood = Pipeline([('scaler', StandardScaler()), ('svm',svm.OneClassSVM(**kwargs))])\n",
    "        self.ood.fit(self.df_ens[features])     \n",
    "        \n",
    "    def ood_score(self, features=['energy_max']):\n",
    "        self.df_ens['ood_score'] = self.ood.score_samples(self.df_ens[features])\n",
    "    \n",
    "    def ood_save(self, path):\n",
    "        path = Path(path)\n",
    "        joblib.dump(self.ood, path.with_suffix('.pkl'))\n",
    "        print(f'Saved OOD model to {path}.pkl')\n",
    "    \n",
    "    def ood_load(self, path):\n",
    "        path = Path(path)\n",
    "        try:\n",
    "            self.ood = joblib.load(path)\n",
    "            print(f'Successsfully loaded OOD Model from {path}')\n",
    "        except: \n",
    "            print('Error! Select valid joblib file (.pkl)') \n",
    "    \n",
    "    def clear_tmp(self):\n",
    "        try: \n",
    "            shutil.rmtree('/tmp/*', ignore_errors=True)\n",
    "            shutil.rmtree(self.path/'.tmp')\n",
    "            print(f'Deleted temporary files from {self.path/\".tmp\"}')\n",
    "        except: print(f'No temporary files to delete at {self.path/\".tmp\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "add_docs(EnsembleLearner, \"Meta class to train and predict model ensembles with `n` models\",\n",
    "         save_model= \"Save `model` to `file` along with `arch`, `stats`, and `c` classes\",\n",
    "         load_model=\"Load `model` from `file` along with `arch`, `stats`, and `c` classes\",\n",
    "         fit=\"Fit model number `i`\",\n",
    "         fit_ensemble=\"Fit `i` models and `skip` existing\",\n",
    "         predict=\"Predict `files` with model at `model_path`\",\n",
    "         get_valid_results=\"Validate models on validation data and save results\",\n",
    "         show_valid_results=\"Plot results of all or `file` validation images\",\n",
    "         ensemble_results=\"Merge single model results\",\n",
    "         get_ensemble_results=\"Get models and ensemble results\", \n",
    "         show_ensemble_results=\"Show result of ensemble or `model_no`\",\n",
    "         load_ensemble=\"Get models saved at `path`\",\n",
    "         get_model=\"Get model architecture\",\n",
    "         get_loss=\"Get loss function from loss name (config)\",\n",
    "         get_batch_tfms=\"Get transform performed on batch level\",\n",
    "         set_n=\"Change to `n` models per ensemble\",\n",
    "         lr_find=\"Wrapper for learning rate finder\",\n",
    "         show_mask_weights='Plot fn for masks and weights',\n",
    "         ood_train=\"Train SVM for OOD Detection\",\n",
    "         ood_score=\"Get OOD score\",\n",
    "         ood_save='Save OOD model to path',\n",
    "         ood_load='Load OOD model from path',\n",
    "         clear_tmp=\"Clear directory with temporary files\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"EnsembleLearner\" class=\"doc_header\"><code>class</code> <code>EnsembleLearner</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>EnsembleLearner</code>(**`image_dir`**=*`'images'`*, **`mask_dir`**=*`None`*, **`config`**=*`None`*, **`path`**=*`None`*, **`ensemble_dir`**=*`None`*, **`label_fn`**=*`None`*, **`metrics`**=*`None`*, **`cbs`**=*`None`*, **`ds_kwargs`**=*`{}`*, **`dl_kwargs`**=*`{}`*, **`model_kwargs`**=*`{}`*, **`loss_kwargs`**=*`{}`*, **`stats`**=*`None`*, **`files`**=*`None`*) :: `GetAttr`\n",
       "\n",
       "Meta class to train and predict model ensembles with `n` models"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(EnsembleLearner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_learner.ipynb.\n",
      "Converted 01_models.ipynb.\n",
      "Converted 02_data.ipynb.\n",
      "Converted 02a_transforms.ipynb.\n",
      "Converted 03_metrics.ipynb.\n",
      "Converted 04_callbacks.ipynb.\n",
      "Converted 05_losses.ipynb.\n",
      "Converted 06_utils.ipynb.\n",
      "Converted 07_tta.ipynb.\n",
      "Converted 08_gui.ipynb.\n",
      "Converted 09_gt.ipynb.\n",
      "Converted add_information.ipynb.\n",
      "Converted deepflash2.ipynb.\n",
      "Converted gt_estimation.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted model_library.ipynb.\n",
      "Converted predict.ipynb.\n",
      "Converted train.ipynb.\n",
      "Converted tutorial.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "fastai"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
